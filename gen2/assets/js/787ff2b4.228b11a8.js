"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[2899],{28453:(e,n,a)=>{a.d(n,{R:()=>s,x:()=>o});var t=a(96540);const r={},i=t.createContext(r);function s(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),t.createElement(i.Provider,{value:n},e.children)}},99903:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>m,frontMatter:()=>s,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"client-sdk/python-sdk/streaming-example","title":"Example - Streaming","description":"Using aria.sdkgen2 and aria.streamreceiver for Real-Time Device Streaming","source":"@site/docs-ark/client-sdk/python-sdk/streaming-example.mdx","sourceDirName":"client-sdk/python-sdk","slug":"/client-sdk/python-sdk/streaming-example","permalink":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/streaming-example","draft":false,"unlisted":false,"editUrl":"https://www.internalfb.com/code/fbsource/arvr/projects/ariane/aria_research_kit/projectaria_tools/website/docs-ark/client-sdk/python-sdk/streaming-example.mdx","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Example - Streaming"},"sidebar":"arkSidebar","previous":{"title":"Example - Record and download VRS","permalink":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/recording-example"},"next":{"title":"Example - Text-to-speech","permalink":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/text-to-speech-example"}}');var r=a(74848),i=a(28453);const s={sidebar_position:4,title:"Example - Streaming"},o=void 0,c={},d=[{value:"Run streaming example",id:"run-streaming-example",level:2},{value:"Step-by-step walk through",id:"step-by-step-walk-through",level:2}];function l(e){const n={code:"code",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:"Using aria.sdk_gen2 and aria.stream_receiver for Real-Time Device Streaming"}),"\n",(0,r.jsx)(n.h2,{id:"run-streaming-example",children:"Run streaming example"}),"\n",(0,r.jsx)(n.p,{children:"How to Use This Script\nConnect your Aria device to your computer.\nRun the script from the command line:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"python ~/Downloads/projectaria_client_sdk_samples_gen2/device_streaming.py --record-to-vrs /path/to/output.vrs\n"})}),"\n",(0,r.jsx)(n.p,{children:"Omit --record-to-vrs if you do not want to save the stream.\nWatch the console for real-time data printouts from the callbacks."}),"\n",(0,r.jsx)(n.h2,{id:"step-by-step-walk-through",children:"Step-by-step walk through"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Import Required Modules"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"The script uses several modules:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"import argparse\nimport signal\nimport sys\nimport aria.sdk_gen2 as sdk_gen2\nimport aria.stream_receiver as receiver\nfrom projectaria_tools.core.mps import EyeGaze, hand_tracking, OpenLoopTrajectoryPose\nfrom projectaria_tools.core.sensor_data import (\n   AudioData, AudioDataRecord, FrontendOutput, ImageData, ImageDataRecord, MotionData,\n)\n"})}),"\n",(0,r.jsx)(n.p,{children:"aria.sdk_gen2 and aria.stream_receiver are the main SDKs for device streaming control and receive data streaming.\nprojectaria_tools.core modules provide data structures for interpreting streamed data."}),"\n",(0,r.jsxs)(n.ol,{start:"2",children:["\n",(0,r.jsx)(n.li,{children:"Parse Command-Line Arguments"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"The script accepts an optional argument to specify where to save the streamed data as a VRS file:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def parse_args() -> argparse.Namespace:\n   parser = argparse.ArgumentParser()\n   parser.add_argument(\n       "--record-to-vrs",\n       dest="record_to_vrs",\n       type=str,\n       default="",\n       required=False,\n       help="Output directory to save the received streaming into VRS",\n   )\n   return parser.parse_args()\n'})}),"\n",(0,r.jsxs)(n.p,{children:["Use --record-to-vrs ",(0,r.jsx)(n.code,{children:"<path>"})," to save the streaming data into VRS to inspect the live streaming data."]}),"\n",(0,r.jsx)(n.p,{children:"NOTE: Data drop could happen for poor streaming connections, the saved VRS from streaming data will reflect the data drop."}),"\n",(0,r.jsxs)(n.ol,{start:"3",children:["\n",(0,r.jsx)(n.li,{children:"Set Up and Connect to the Device"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"The device is initialized and connected:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# keep device_client as a global variable to allow connection to the device.\ndevice_client = sdk_gen2.DeviceClient()\n\n# establish connection to the device\nconfig = sdk_gen2.DeviceClientConfig()\ndevice_client.set_client_config(config)\ndevice = device_client.connect()\n\n# configure the streaming control\nstreaming_config = sdk_gen2.HttpStreamingConfig()\nstreaming_config.profile_name = "profile9"\nstreaming_config.streaming_interface = sdk_gen2.StreamingInterface.USB_NCM\ndevice.set_streaming_config(streaming_config)\n\n# start streaming\ndevice.start_streaming()\n'})}),"\n",(0,r.jsx)(n.p,{children:"The device is configured for streaming using profile9 and USB interface. We also provide WIFI and on-device hotspot for wireless streaming.\nFor more details on profiles and streaming interface, please refer to technical specs."}),"\n",(0,r.jsx)(n.p,{children:"NOTE: Device could reach thermal throttle and shutdown due to overheating, especially when streaming wirelessly."}),"\n",(0,r.jsxs)(n.ol,{start:"4",children:["\n",(0,r.jsx)(n.li,{children:"Define Data Callbacks"}),"\n"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"image_callback: callbacks for all the cameras from the device including RGB (x1), SLAM camera (x4) and ET camera (x2)"}),"\n",(0,r.jsx)(n.li,{children:"audio_callback: audio callback for all 8 channels"}),"\n",(0,r.jsx)(n.li,{children:"imu_callback: callback to receive both imu-left and imu-right imu data in 800Hz"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def image_callback(image_data: ImageData, image_record: ImageDataRecord):\n    print(\n        f"Received image data of size {image_data.to_numpy_array().shape} with timestamp {image_record.capture_timestamp_ns} ns"\n    )\n\n\ndef audio_callback(\n    audio_data: AudioData, audio_record: AudioDataRecord, num_channels: int\n):\n    print(\n        f"Received audio data with {len(audio_data.data)} samples and {len(audio_record.capture_timestamps_ns)} timestamps and num channels {num_channels}"\n    )\n\n\ndef imu_callback(imu_data: MotionData, sensor_label: str):\n    print(\n        f"Received {sensor_label} accel data {imu_data.accel_msec2} and gyro {imu_data.gyro_radsec}"\n    )\n\n\ndef eyegaze_callback(eyegaze_data: EyeGaze):\n    print(\n        f"Received EyeGaze data at timestamp {eyegaze_data.tracking_timestamp.total_seconds()} sec "\n        f"with yaw={eyegaze_data.yaw:.3f} rad, pitch={eyegaze_data.pitch:.3f} rad, "\n        f"depth={eyegaze_data.depth:.3f} m"\n    )\n\n\ndef handtracking_callback(handtracking_data: hand_tracking.HandTrackingResult):\n    print(\n        f"Received HandTracking data at timestamp {handtracking_data.tracking_timestamp.total_seconds()} sec"\n    )\n\n    # Check left hand data\n    if handtracking_data.left_hand is not None:\n        left_hand = handtracking_data.left_hand\n        print(f"  Left hand confidence: {left_hand.confidence:.3f}")\n        print(f"  Left wrist position: {left_hand.get_wrist_position_device()}")\n        print(f"  Left palm position: {left_hand.get_palm_position_device()}")\n        if left_hand.wrist_and_palm_normal_device is not None:\n            normals = left_hand.wrist_and_palm_normal_device\n            print(f"  Left wrist normal: {normals.wrist_normal_device}")\n            print(f"  Left palm normal: {normals.palm_normal_device}")\n    else:\n        print("  Left hand: No data")\n\n    # Check right hand data\n    if handtracking_data.right_hand is not None:\n        right_hand = handtracking_data.right_hand\n        print(f"  Right hand confidence: {right_hand.confidence:.3f}")\n        print(f"  Right wrist position: {right_hand.get_wrist_position_device()}")\n        print(f"  Right palm position: {right_hand.get_palm_position_device()}")\n        if right_hand.wrist_and_palm_normal_device is not None:\n            normals = right_hand.wrist_and_palm_normal_device\n            print(f"  Right wrist normal: {normals.wrist_normal_device}")\n            print(f"  Right palm normal: {normals.palm_normal_device}")\n    else:\n        print("  Right hand: No data")\n\n\ndef vio_callback(vio_data: FrontendOutput):\n    print(\n        f"Received VIO data at timestamp {vio_data.capture_timestamp_ns} with transform_odometry_bodyimu: {vio_data.transform_odometry_bodyimu.rotation().log()} and {vio_data.transform_odometry_bodyimu.translation()} ns"\n    )\n'})}),"\n",(0,r.jsxs)(n.ol,{start:"5",children:["\n",(0,r.jsx)(n.li,{children:"Set Up the Streaming Receiver"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"The receiver is configured to listen for data and register the callbacks:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def setup_streaming_receiver(device, record_to_vrs):\n   config = sdk_gen2.HttpServerConfig()\n   config.address = "0.0.0.0"\n   config.port = 6768\n   stream_receiver = receiver.StreamReceiver()\n   stream_receiver.set_server_config(config)\n   if record_to_vrs != "":\n       stream_receiver.record_to_vrs(record_to_vrs)\n   stream_receiver.register_slam_callback(image_callback)\n   stream_receiver.register_rgb_callback(image_callback)\n   stream_receiver.register_audio_callback(audio_callback)\n   stream_receiver.register_eye_gaze_callback(eyegaze_callback)\n   stream_receiver.register_hand_pose_callback(handtracking_callback)\n   stream_receiver.register_vio_callback(vio_callback)\n   stream_receiver.start_server()\n'})}),"\n",(0,r.jsx)(n.p,{children:"The server listens on all interfaces at port 6768. If record_to_vrs is set, the stream is saved to a VRS file. All relevant callbacks are registered."}),"\n",(0,r.jsx)(n.p,{children:"NOTE: Please ensure your port 6768 is open and VPN is disabled to allow streaming data to be received."}),"\n",(0,r.jsxs)(n.ol,{start:"6",children:["\n",(0,r.jsx)(n.li,{children:"Run the Script"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"The main block ties everything together:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'if __name__ == "__main__":\n   args = parse_args()\n   device = device_streaming()\n   setup_streaming_receiver(device, args.record_to_vrs)\n'})})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(l,{...e})}):l(e)}}}]);
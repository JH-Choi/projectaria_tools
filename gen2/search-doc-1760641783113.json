[{"title":"Aria Research Kit (ARK)","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark","content":"Aria Research Kit (ARK)","keywords":"","version":"Next"},{"title":"Advanced - Use Your Own Image Decoder","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/client-sdk/image-decoder","content":"Advanced - Use Your Own Image Decoder [advanced] Tutorial of using your own image decoder","keywords":"","version":"Next"},{"title":"Example - Authentication","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/auth-example","content":"","keywords":"","version":"Next"},{"title":"Run authentication example‚Äã","type":1,"pageTitle":"Example - Authentication","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/auth-example#run-authentication-example","content":" python ~/Downloads/projectaria_client_sdk_samples_gen2/device_auth.py   ","version":"Next","tagName":"h2"},{"title":"Step-by-step walk through‚Äã","type":1,"pageTitle":"Example - Authentication","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/auth-example#step-by-step-walk-through","content":" Import Required Modules  Start by importing the necessary modules:  import time import aria.sdk_gen2 as sdk_gen2   aria.sdk_gen2 is the SDK for interacting with Aria Gen2 devices.  Set Up the Device Client  Create an instance of the device client:  device_client = sdk_gen2.DeviceClient()   This object manages the connection and authentication process.  Configure the Device Client  Set up the configuration for the device client:  config = sdk_gen2.DeviceClientConfig() device_client.set_client_config(config)   You can specify device details (e.g., serial number) in DeviceClientConfig. If default config is specified to set_client_config(), it will connect to the first available device connect through USB  Initiate Device Authentication  Prompt the user to open the Aria Companion app and accept the pairing request:  print(&quot;Authenticating device. Please open the Aria app and accept the pairing request&quot;) device_client.authenticate() time.sleep(5)   The authenticate() method starts the authentication process. The script waits 5 seconds to allow the user to respond.  Connect to the Device  Attempt to establish a connection:  try: device = device_client.connect() print(f&quot;Device authentication successful to device {device.connection_id()}&quot;) except Exception: print(&quot;Failed to authenticate and connect to device&quot;) return   If authentication is successful, you‚Äôll see a success message with the device connection ID.  device = device_client.connect() will return a Device Class which can be used to control the device for more features such as recording, streaming, etc. ","version":"Next","tagName":"h2"},{"title":"Example - Connection","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/connection-example","content":"","keywords":"","version":"Next"},{"title":"Run connection example‚Äã","type":1,"pageTitle":"Example - Connection","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/connection-example#run-connection-example","content":" python ~/Downloads/projectaria_client_sdk_samples_gen2/device_connect.py --serial IM0XXXXXXXXXXX   Replace IM0XXXXXXXXXXX with your device‚Äôs actual serial number.  ","version":"Next","tagName":"h2"},{"title":"Step-by-step walk through‚Äã","type":1,"pageTitle":"Example - Connection","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/connection-example#step-by-step-walk-through","content":" Import Required Modules  Start by importing the necessary modules:  import argparse import aria.sdk_gen2 as sdk_gen2   aria.sdk_gen2 is the SDK for interacting with Aria devices.  Parse Command-Line Arguments  Set up argument parsing to allow the user to specify the device serial number:  def parse_args() -&gt; argparse.Namespace: parser = argparse.ArgumentParser() parser.add_argument( &quot;--serial&quot;, dest=&quot;serial&quot;, type=str, default=&quot;&quot;, required=False, help=&quot;Serial number of the device which will be connected. (e.g. 1M0YDB5H7B0020)&quot;, ) return parser.parse_args()   The --serial argument lets the user specify which device to connect to. If not provided, it defaults to an empty string, and the first available device will be used.  Set Up the Device Client and Configuration  Create and configure the device client:  def device_connect(serial): device_client = sdk_gen2.DeviceClient() config = sdk_gen2.DeviceClientConfig() config.device_serial = serial device_client.set_client_config(config)   DeviceClient manages the authentication and device connection.DeviceClientConfig is used to specify the target device by serial number or IP address.  Connect to the Device  Attempt to connect to the specified device:   try: device = device_client.connect() print(f&quot;Successfully connected to device {device.connection_id()}&quot;) except Exception: print(&quot;Failed to connect to device.&quot;)   If the connection is successful, a confirmation message is printed. ","version":"Next","tagName":"h2"},{"title":"Python SDK with examples","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/python-interface","content":"Python SDK with examples We provide a set of example codes to demonstrate how to use the SDK interface that includes Authenticate device with connected PCSetup connection to deviceSetup device recording and downloadSetup device streaming and receiver with custom callback functionsSend TTS to device The examples code need to be exported using the following command # Export examples codes python -m aria.extract_sdk_samples --output ~/Downloads/ You should be able to find the examples codes in the following directory projectaria_client_sdk_samples_gen2. ls ~/Downloads/projectaria_client_sdk_samples_gen2 device_auth.py device_connect.py device_record.py device_streaming.py In the next few pages, we provide detailed explanations for each example code.","keywords":"","version":"Next"},{"title":"Example - Record and download VRS","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/recording-example","content":"","keywords":"","version":"Next"},{"title":"Run recording example‚Äã","type":1,"pageTitle":"Example - Record and download VRS","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/recording-example#run-recording-example","content":" Usage Example:  python ~/Downloads/projectaria_client_sdk_samples_gen2/device_record.py   ","version":"Next","tagName":"h2"},{"title":"Step-by-step walk through‚Äã","type":1,"pageTitle":"Example - Record and download VRS","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/recording-example#step-by-step-walk-through","content":" Import Required Modules  Start by importing the necessary modules:  import argparse import time import aria.sdk_gen2 as sdk_gen2   aria.sdk_gen2 is the SDK for interacting with Aria devices.  Parse Command-Line Arguments  Set up argument parsing to allow the user to specify the recording duration and output directory:  def parse_args() -&gt; argparse.Namespace: parser = argparse.ArgumentParser() parser.add_argument( &quot;--output&quot;, dest=&quot;output_path&quot;, type=str, default=&quot;&quot;, required=False, help=&quot;Output directory to save the recording&quot;, ) parser.add_argument( &quot;--duration&quot;, dest=&quot;duration&quot;, type=int, default=10, required=False, help=&quot;Recording duration in seconds (default: 10)&quot;, ) return parser.parse_args()   --output specifies where the recording will be saved.--duration sets how long the recording will last (in seconds).  Set Up the Device Client and Configuration  Create and configure the device client:  device_client = sdk_gen2.DeviceClient() config = sdk_gen2.DeviceClientConfig() device_client.set_client_config(config)   DeviceClient manages the authentication and device connection. DeviceClientConfig is used to specify the target device by serial number or IP address.  Connect to the Device  Attempt to connect to the device:  device = device_client.connect() print(f&quot;Successfully connected to device {device.connection_id()}&quot;)   If successful, you‚Äôll see a confirmation message.  Set Up Recording Configuration  Configure the recording profile and type:  recording_config = sdk_gen2.RecordingConfig() recording_config.profile_name = &quot;profile9&quot; recording_config.recording_type = sdk_gen2.RecordingType.RECORDING_TYPE_PROTOTYPE recording_config.recording_name = &quot;example_recording&quot; device.set_recording_config(recording_config)   recording_name: User specified name of the recording.profile_name: Profile9 is the official supported profile to recording and streaming.RECORDING_TYPE_PROTOTYPE: This is the recording type that allows direct download from device to local PC for fast prototyping. For more details on RECORDING_TYPE, please refer to technical specs page.  Start and Stop Recording  Begin recording, wait for the specified duration, then stop:  uuid = device.start_recording() print(f&quot;Start recording for {duration} seconds with uuid: {uuid}&quot;) time.sleep(duration) device.stop_recording()   The recording runs for the duration specified by the user. The uuid is needed to download the specific recording.  List and Download Recordings  List all recordings and download the one just created using the uuid:  device.list_recordings() device.download_recording(uuid=uuid, output_path=output_path)   This will save the recording to the specified output directory. ","version":"Next","tagName":"h2"},{"title":"Example - Streaming","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/streaming-example","content":"","keywords":"","version":"Next"},{"title":"Run streaming example‚Äã","type":1,"pageTitle":"Example - Streaming","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/streaming-example#run-streaming-example","content":" How to Use This Script Connect your Aria device to your computer. Run the script from the command line:  python ~/Downloads/projectaria_client_sdk_samples_gen2/device_streaming.py --record-to-vrs /path/to/output.vrs   Omit --record-to-vrs if you do not want to save the stream. Watch the console for real-time data printouts from the callbacks.  ","version":"Next","tagName":"h2"},{"title":"Step-by-step walk through‚Äã","type":1,"pageTitle":"Example - Streaming","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/streaming-example#step-by-step-walk-through","content":" Import Required Modules  The script uses several modules:  import argparse import signal import sys import aria.sdk_gen2 as sdk_gen2 import aria.stream_receiver as receiver from projectaria_tools.core.mps import EyeGaze, hand_tracking, OpenLoopTrajectoryPose from projectaria_tools.core.sensor_data import ( AudioData, AudioDataRecord, FrontendOutput, ImageData, ImageDataRecord, MotionData, )   aria.sdk_gen2 and aria.stream_receiver are the main SDKs for device streaming control and receive data streaming. projectaria_tools.core modules provide data structures for interpreting streamed data.  Parse Command-Line Arguments  The script accepts an optional argument to specify where to save the streamed data as a VRS file:  def parse_args() -&gt; argparse.Namespace: parser = argparse.ArgumentParser() parser.add_argument( &quot;--record-to-vrs&quot;, dest=&quot;record_to_vrs&quot;, type=str, default=&quot;&quot;, required=False, help=&quot;Output directory to save the received streaming into VRS&quot;, ) return parser.parse_args()   Use --record-to-vrs &lt;path&gt; to save the streaming data into VRS to inspect the live streaming data.  NOTE: Data drop could happen for poor streaming connections, the saved VRS from streaming data will reflect the data drop.  Set Up and Connect to the Device  The device is initialized and connected:  # keep device_client as a global variable to allow connection to the device. device_client = sdk_gen2.DeviceClient() # establish connection to the device config = sdk_gen2.DeviceClientConfig() device_client.set_client_config(config) device = device_client.connect() # configure the streaming control streaming_config = sdk_gen2.HttpStreamingConfig() streaming_config.profile_name = &quot;profile9&quot; streaming_config.streaming_interface = sdk_gen2.StreamingInterface.USB_NCM device.set_streaming_config(streaming_config) # start streaming device.start_streaming()   The device is configured for streaming using profile9 and USB interface. We also provide WIFI and on-device hotspot for wireless streaming. For more details on profiles and streaming interface, please refer to technical specs.  NOTE: Device could reach thermal throttle and shutdown due to overheating, especially when streaming wirelessly.  Define Data Callbacks  image_callback: callbacks for all the cameras from the device including RGB (x1), SLAM camera (x4) and ET camera (x2)audio_callback: audio callback for all 8 channelsimu_callback: callback to receive both imu-left and imu-right imu data in 800Hz  def image_callback(image_data: ImageData, image_record: ImageDataRecord): print( f&quot;Received image data of size {image_data.to_numpy_array().shape} with timestamp {image_record.capture_timestamp_ns} ns&quot; ) def audio_callback( audio_data: AudioData, audio_record: AudioDataRecord, num_channels: int ): print( f&quot;Received audio data with {len(audio_data.data)} samples and {len(audio_record.capture_timestamps_ns)} timestamps and num channels {num_channels}&quot; ) def imu_callback(imu_data: MotionData, sensor_label: str): print( f&quot;Received {sensor_label} accel data {imu_data.accel_msec2} and gyro {imu_data.gyro_radsec}&quot; ) def eyegaze_callback(eyegaze_data: EyeGaze): print( f&quot;Received EyeGaze data at timestamp {eyegaze_data.tracking_timestamp.total_seconds()} sec &quot; f&quot;with yaw={eyegaze_data.yaw:.3f} rad, pitch={eyegaze_data.pitch:.3f} rad, &quot; f&quot;depth={eyegaze_data.depth:.3f} m&quot; ) def handtracking_callback(handtracking_data: hand_tracking.HandTrackingResult): print( f&quot;Received HandTracking data at timestamp {handtracking_data.tracking_timestamp.total_seconds()} sec&quot; ) # Check left hand data if handtracking_data.left_hand is not None: left_hand = handtracking_data.left_hand print(f&quot; Left hand confidence: {left_hand.confidence:.3f}&quot;) print(f&quot; Left wrist position: {left_hand.get_wrist_position_device()}&quot;) print(f&quot; Left palm position: {left_hand.get_palm_position_device()}&quot;) if left_hand.wrist_and_palm_normal_device is not None: normals = left_hand.wrist_and_palm_normal_device print(f&quot; Left wrist normal: {normals.wrist_normal_device}&quot;) print(f&quot; Left palm normal: {normals.palm_normal_device}&quot;) else: print(&quot; Left hand: No data&quot;) # Check right hand data if handtracking_data.right_hand is not None: right_hand = handtracking_data.right_hand print(f&quot; Right hand confidence: {right_hand.confidence:.3f}&quot;) print(f&quot; Right wrist position: {right_hand.get_wrist_position_device()}&quot;) print(f&quot; Right palm position: {right_hand.get_palm_position_device()}&quot;) if right_hand.wrist_and_palm_normal_device is not None: normals = right_hand.wrist_and_palm_normal_device print(f&quot; Right wrist normal: {normals.wrist_normal_device}&quot;) print(f&quot; Right palm normal: {normals.palm_normal_device}&quot;) else: print(&quot; Right hand: No data&quot;) def vio_callback(vio_data: FrontendOutput): print( f&quot;Received VIO data at timestamp {vio_data.capture_timestamp_ns} with transform_odometry_bodyimu: {vio_data.transform_odometry_bodyimu.rotation().log()} and {vio_data.transform_odometry_bodyimu.translation()} ns&quot; )   Set Up the Streaming Receiver  The receiver is configured to listen for data and register the callbacks:  def setup_streaming_receiver(device, record_to_vrs): config = sdk_gen2.HttpServerConfig() config.address = &quot;0.0.0.0&quot; config.port = 6768 stream_receiver = receiver.StreamReceiver() stream_receiver.set_server_config(config) if record_to_vrs != &quot;&quot;: stream_receiver.record_to_vrs(record_to_vrs) stream_receiver.register_slam_callback(image_callback) stream_receiver.register_rgb_callback(image_callback) stream_receiver.register_audio_callback(audio_callback) stream_receiver.register_eye_gaze_callback(eyegaze_callback) stream_receiver.register_hand_pose_callback(handtracking_callback) stream_receiver.register_vio_callback(vio_callback) stream_receiver.start_server()   The server listens on all interfaces at port 6768. If record_to_vrs is set, the stream is saved to a VRS file. All relevant callbacks are registered.  NOTE: Please ensure your port 6768 is open and VPN is disabled to allow streaming data to be received.  Run the Script  The main block ties everything together:  if __name__ == &quot;__main__&quot;: args = parse_args() device = device_streaming() setup_streaming_receiver(device, args.record_to_vrs)  ","version":"Next","tagName":"h2"},{"title":"Example - Text-to-speech","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/text-to-speech-example","content":"","keywords":"","version":"Next"},{"title":"Run TTS example‚Äã","type":1,"pageTitle":"Example - Text-to-speech","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/text-to-speech-example#run-tts-example","content":" python ~/Downloads/projectaria_client_sdk_samples_gen2/device_tts.py --text `hello world, my name is Aria Gen 2`   ","version":"Next","tagName":"h2"},{"title":"Step-by-step walk through‚Äã","type":1,"pageTitle":"Example - Text-to-speech","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/text-to-speech-example#step-by-step-walk-through","content":" Import Required Modules  Start by importing the necessary modules:  import argparse import aria.sdk_gen2 as sdk_gen2   aria.sdk_gen2 is the SDK for interacting with Aria devices.  Parse Command-Line Arguments  Set up argument parsing to allow the user to specify the TTS text:  def parse_args() -&gt; argparse.Namespace: parser = argparse.ArgumentParser() parser.add_argument( &quot;--text&quot;, dest=&quot;text&quot;, type=str, default=&quot;&quot;, required=True, help=&quot;TTS text to rendered by the device.&quot;, ) return parser.parse_args()   The --text argument lets the user specify the text to be rendered as speech by the device. This argument is required.  Set Up the Device Client and Configuration  Create and configure the device client:  device_client = sdk_gen2.DeviceClient() config = sdk_gen2.DeviceClientConfig() device_client.set_client_config(config)   DeviceClient manages the authentication and device connection.DeviceClientConfig is used to specify the target device by serial number or IP address.  Connect to the Device  Attempt to connect to the device:  device = device_client.connect() print(f&quot;Connected to device: {device.connection_id()}&quot;)   If successful, you‚Äôll see a confirmation message with the device connection ID.  Render Text-to-Speech (TTS)  Send the TTS command to the device:  print(f&quot;Rendering TTS: {args.text}&quot;) device.render_tts(text=args.text)   The device will render the specified text as speech. ","version":"Next","tagName":"h2"},{"title":"Build a ROS Example","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/client-sdk/ros","content":"Build a ROS Example Tutorial of ROS example","keywords":"","version":"Next"},{"title":"Recording control","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/client-sdk/recording","content":"Recording control To get started, you can use the following command to start/stop recording with the device # start recording aria-gen2 recording start --profile profile8 --recording-name example_recording # stop recording aria-gen2 recording stop To download the recording, you will need the uuid for the previous recording # list existing recordings and uuid aria-gen2 recording list # download recording aria-gen2 recording download -u &lt;uuid&gt; -o ~/Downloads/ # visualize recording aria_rerun_viewer --vrs ~/Downloads/&lt;recording_name&gt;.vrs Example visualization‚Äã For detailed instructions, you can refer to the Technical Specs page for more details.","keywords":"","version":"Next"},{"title":"Trouble Shooting","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/client-sdk/troubleshoot","content":"Trouble Shooting Trouble Shooting","keywords":"","version":"Next"},{"title":"Introduction to ClientSDK","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/client-sdk/start","content":"","keywords":"","version":"Next"},{"title":"Install via virtual environment‚Äã","type":1,"pageTitle":"Introduction to ClientSDK","url":"/projectaria_tools/gen2/ark/client-sdk/start#install-via-virtual-environment","content":" ","version":"Next","tagName":"h2"},{"title":"Step 1 : Install Python‚Äã","type":1,"pageTitle":"Introduction to ClientSDK","url":"/projectaria_tools/gen2/ark/client-sdk/start#step-1--install-python","content":" To use ClientSDK, you'll need Python 3.8 - 3.12.  Python 3 download pageTo check what what version of Python 3 you have use python3 --version  ","version":"Next","tagName":"h3"},{"title":"Step 2 : Create a virtual environment‚Äã","type":1,"pageTitle":"Introduction to ClientSDK","url":"/projectaria_tools/gen2/ark/client-sdk/start#step-2--create-a-virtual-environment","content":" Linux &amp; macOS rm -rf $HOME/clientsdk_python_env python3 -m venv $HOME/clientsdk_python_env source $HOME/clientsdk_python_env/bin/activate   ","version":"Next","tagName":"h3"},{"title":"Step 3 : Install the required python packages‚Äã","type":1,"pageTitle":"Introduction to ClientSDK","url":"/projectaria_tools/gen2/ark/client-sdk/start#step-3--install-the-required-python-packages","content":" python3 -m pip install --upgrade pip # install projectaria-tools pip install projectaria-tools==2.0.0   ","version":"Next","tagName":"h3"},{"title":"Step 4 : Install client sdk python package.‚Äã","type":1,"pageTitle":"Introduction to ClientSDK","url":"/projectaria_tools/gen2/ark/client-sdk/start#step-4--install-client-sdk-python-package","content":" pip install projectaria-client-sdk==2.0.0   ","version":"Next","tagName":"h3"},{"title":"Authenticate your device with your PC‚Äã","type":1,"pageTitle":"Introduction to ClientSDK","url":"/projectaria_tools/gen2/ark/client-sdk/start#authenticate-your-device-with-your-pc","content":" You need to authenticate your device with your PC to allow secure connection and control. To pair your device with the PC, please first pair the device with your mobile Companion App.  # connect device to the computer and check if the device appears aria-gen2 device list # Authenticate device to connect with the PC aria-gen2 auth pair   You will need to approve the authentication from your mobile Companion App to allow connection as shown here.     NOTE: You only need to do this once per device and PC.  Once authenticated, you can check your authentication status using the command below.  # check if authentication aria-gen2 auth check   ","version":"Next","tagName":"h2"},{"title":"CLI available controls‚Äã","type":1,"pageTitle":"Introduction to ClientSDK","url":"/projectaria_tools/gen2/ark/client-sdk/start#cli-available-controls","content":" You can use the packaged CLI to control the device to  Recording: control the device to start, stop, list and download recordings in VRS format.Streaming: control the device to start and stop streaming.Device: general device controls that includes obtain device information, connect to WIFI, sending text-to-speech, etcPlayer: Receives device streaming data and perform health monitoring of the streaming data regard latency and data drops.  For detailed instructions, you can refer to the Technical Specs page for more details. ","version":"Next","tagName":"h2"},{"title":"Streaming control","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/client-sdk/streaming","content":"Streaming control To get started, you can use the following command to start/stop streaming with the device # start streaming aria-gen2 streaming start # visualize streaming aria-streaming-viewer # stop streaming aria-gen2 streaming stop Example visualization‚Äã For detailed instructions, you can refer to the Technical Specs page for more details.","keywords":"","version":"Next"},{"title":"Control Recording","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/companion-app/recording","content":"Control Recording Record via the Aria Companion App.","keywords":"","version":"Next"},{"title":"Get Started","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/companion-app/start","content":"Get Started What is CA, and Download the Aria Companion App.","keywords":"","version":"Next"},{"title":"Accessories and Fitment","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/device/accessories_fitment","content":"","keywords":"","version":"Next"},{"title":"Aria Welcome Kit Contents‚Äã","type":1,"pageTitle":"Accessories and Fitment","url":"/projectaria_tools/gen2/ark/device/accessories_fitment#aria-welcome-kit-contents","content":" Case:Safely stores your Aria glasses and accessories for transport. Microfiber Lens Cloth:Use this cloth to gently clean and dry your Aria glasses. (Note: The glasses are not waterproof.) Lens cleaning wipes are also suitable. Quick Start Guide:Provides essential information about the device and Companion App. Full details are also available on our wiki. Accessories Guide:Explains all included accessories and their usage. Regulatory Insert:Contains compliance and safety information. Circle and Wedge Fitment Accessories:These help improve comfort and prevent slippage. For application instructions, refer to the Accessories Guide.    For more details, please consult the included guides or contact our Support team.  ","version":"Next","tagName":"h2"},{"title":"Fitment‚Äã","type":1,"pageTitle":"Accessories and Fitment","url":"/projectaria_tools/gen2/ark/device/accessories_fitment#fitment","content":" Aria Gen 2 boasts superior wearability, characterized by enhanced comfort and fit, while accommodating a wider range of face morphologies. To ensure you have an optimal physical and functional fit, we‚Äôve introduced eight size variations of the device‚Äîaccounting for a number of human factors including head breadth and nose bridge variation.     To determine your Aria size, please follow the instructions and fill your measurements in the form below:  ","version":"Next","tagName":"h2"},{"title":"Aria Gen 2 Self-Sizing Form‚Äã","type":1,"pageTitle":"Accessories and Fitment","url":"/projectaria_tools/gen2/ark/device/accessories_fitment#aria-gen-2-self-sizing-form","content":"","version":"Next","tagName":"h3"},{"title":"Request for Machine Perception Service","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/mps/request","content":"Request for Machine Perception Service Tutorial of requesting SLAM &amp; eye gaze &amp; hand tracking","keywords":"","version":"Next"},{"title":"Get Started","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/mps/start","content":"Get Started Get started - What is MPS &amp; MPS CLI installation &amp; data life cycle","keywords":"","version":"Next"},{"title":"Trouble Shooting","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/mps/troubleshoot","content":"Trouble Shooting Trouble Shooting","keywords":"","version":"Next"},{"title":"On-device Machine Perception","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/on_device_mp","content":"","keywords":"","version":"Next"},{"title":"Visual Inertial Odometry (VIO)‚Äã","type":1,"pageTitle":"On-device Machine Perception","url":"/projectaria_tools/gen2/ark/on_device_mp#visual-inertial-odometry-vio","content":" One of the key features of Aria Gen 2 is its ability to track the glasses in six degrees of freedom (6DOF) within a spatial frame of reference using Visual Inertial Odometry (VIO), by fusing the sensor data from four CV cameras and two IMUs. This allows for seamless navigation and mapping of the environment, opening up new possibilities for research in contextual AI and robotics. The VIO output is generated at 10Hz with the following output:  3-DOF position3-DOF linear velocity3-DOF orientation in quaternion form3-DOF angular velocityEstimated direction of gravity for the odometry frame  Additionally, Aria Gen2 also produces high-frequency VIO output (the fields of the output are the same regular VIO) at IMU rate (800Hz), by performing IMU pre-integration on top of the regular 10Hz VIO output. The high-frequency VIO output can be useful for applications where low-latency VIO poses are needed.  ","version":"Next","tagName":"h2"},{"title":"Eye Tracking‚Äã","type":1,"pageTitle":"On-device Machine Perception","url":"/projectaria_tools/gen2/ark/on_device_mp#eye-tracking","content":" Aria Gen 2 also boasts an advanced camera-based eye tracking system that tracks the wearer‚Äôs gaze. The advanced gaze signal enables a deeper understanding of the wearer‚Äôs visual attention and intentions, unlocking new possibilities for human-computer interaction. This system generates the following eye tracking outputs for each eye, up to 90Hz:  The origin and direction of the individual gaze rayThe 3-DOF position of the entrance pupilThe diameter of the pupilWhether the eye is blinking  Additionally, the system also produces the following signals for the combined gaze estimated from both eyes, including:  The original and direction of the combined gaze rayVergence depth of the combined gazeDistance between the left/right eye pupils, a.k.a, IPD (Inter Pupillary Distance)  ","version":"Next","tagName":"h2"},{"title":"Hand Tracking‚Äã","type":1,"pageTitle":"On-device Machine Perception","url":"/projectaria_tools/gen2/ark/on_device_mp#hand-tracking","content":" Aria Gen 2 also features a hand detection and tracking solution that tracks the wearer‚Äôs hand in 3D space. This produces articulated hand-joint poses in the device frame of reference, facilitating accurate hand annotations for datasets and enabling applications such as dexterous robot hand manipulation that require high precision. The hand tracking pipeline generates the following outputs at 30Hz for each hand (left and right):  3-DOF position of the wrist3-DOF rotation of the wrist3-DOF positions of the 21 finger joint landmarks ","version":"Next","tagName":"h2"},{"title":"Support and Troubleshooting","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/support","content":"Support and Troubleshooting Support and Troubleshooting","keywords":"","version":"Next"},{"title":"Aria Gen 2 Glasses Manual","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/device/manual","content":"","keywords":"","version":"Next"},{"title":"Sensors‚Äã","type":1,"pageTitle":"Aria Gen 2 Glasses Manual","url":"/projectaria_tools/gen2/ark/device/manual#sensors","content":" Aria Gen2 introduces a state-of-the-art sensor suite that expands the range of research applications:     CV Cameras: Four computer vision (CV) cameras provide a wide field of view and high-dynamic-range (HDR) imaging, enabling robust scene understanding and perception even in challenging lighting conditions.RGB Camera: RGB point-of-view (POV) camera for high-resolution egocentric vision and ML applicationsEye Tracking system: Enable precise gaze estimation and user intent modeling.Spatial Microphones: Capture high-fidelity audio for spatial awareness and voice interaction.Contact Microphone (Nosepad): Isolates the wearer‚Äôs voice from bystanders, improving voice command reliability in noisy environments.IMU, Barometer, Magnetometer, GNSS: Support accurate localization, motion tracking, and environmental sensing.Ambient Light Sensor (ALS) with UV Channel: Distinguishes between indoor and outdoor lighting, opening new avenues for context-aware research.PPG Sensor (Nosepad): Measures heart rate, supporting physiological and affective computing studies.Multi-device time alignment using SubGHz radio based custom protocols allow for multiple Aria Gen 2 device to align their timebases to 10‚Äôs of microseconds accuracy.  These new sensors, especially the ALS with UV channel, contact-mics and the physiological sensors in the nosepad, unlock research opportunities in context-aware computing, health monitoring, and robust multimodal perception.  For details, see the Hardware Specifications.    ","version":"Next","tagName":"h2"},{"title":"Hardwares‚Äã","type":1,"pageTitle":"Aria Gen 2 Glasses Manual","url":"/projectaria_tools/gen2/ark/device/manual#hardwares","content":" Aria Gen 2 glasses are equipped with a suite of hardware controls, visual indicators, and audio feedback to help you operate and understand the device‚Äôs status.       ","version":"Next","tagName":"h2"},{"title":"Controls and Hardware Features‚Äã","type":1,"pageTitle":"Aria Gen 2 Glasses Manual","url":"/projectaria_tools/gen2/ark/device/manual#controls-and-hardware-features","content":" The glasses include several built-in controls:  Privacy Switch: Controls whether the device can record data. When set to the rear and the red underneath is visible, recording is disabled and any ongoing recordings are deleted.Action Button: Starts and ends recording sessions. This button can be customized to perform other actions (e.g., start a livestream).Power Button: Turns the device on and off.USB Port: Uses a standard USB-C cable.Volume Control: Adjusts the speaker volume.    ","version":"Next","tagName":"h3"},{"title":"Visual and Audio Indicators‚Äã","type":1,"pageTitle":"Aria Gen 2 Glasses Manual","url":"/projectaria_tools/gen2/ark/device/manual#visual-and-audio-indicators","content":" Aria Gen 2 uses LEDs and speakers to communicate device status to both the wearer and bystanders.  LED Indicators‚Äã  There are three main LEDs:  Inner LED: Visible only to the wearer, primarily communicates device state and events.Bystander LED: Visible to bystanders, indicates when the device is recording or streaming data from sensors.Power LED: Located above the power button on the inner right arm, indicates battery and charging status (not visible while wearing).  Audio Feedback‚Äã  Earcons: Non-verbal sounds (e.g., camera shutter, error beeps) indicate device states.TTS (Text-to-Speech): Spoken messages (e.g., ‚ÄúGlasses shutting down‚Äù, ‚ÄúBattery ten percent: charge soon‚Äù) provide status updates.    ","version":"Next","tagName":"h3"},{"title":"Device States and Indicators‚Äã","type":1,"pageTitle":"Aria Gen 2 Glasses Manual","url":"/projectaria_tools/gen2/ark/device/manual#device-states-and-indicators","content":" The following table summarizes how Aria Gen 2 communicates its various states:  Inward LED\tPower LED\tOutward LED\tDevice Status ExplanationOff\tOff\tOff\tDevice is off or in standby Off\tSolid blue\tOff\tBattery charging or fully charged (system running) Off\tSolid orange\tOff\tFastboot mode Off\tBlinking red\tOff\tLow battery if powering on or charging Off\tSolid blue (1s)\tOff\tStatus check (short-press power button) Off\tPulsing blue\tOff\tSystem shutting down (with earcon) Pulsing green\tOff\tOff\tUploading Solid green (3s)\tSolid blue (3s)\tOff\tBoot complete (both LEDs solid for 3s, then off if not charging) Blinking orange (4x)\tOff\tOff\tWarning: battery low (&lt;&lt;&lt;20%), thermal throttling, low storage, or other warning Blinking red (4x)\tOff\tOff\tError: battery critical (‚â§‚â§‚â§5%) or other error Solid white (1s)\tOff\tOff\tDevice On Notification (DON): user puts on glasses, earcon plays Pulsing white\tOff\tOff\tSystem warm booting (waking from sleep) Blinking white, then solid white\tOff\tBlinking white (2x), then solid white\tRecording/Streaming active: inward LED blinks then solid; outward LED blinks then solid Pulsing white\tPulsing blue\tOff\tOS booting up    ","version":"Next","tagName":"h3"},{"title":"Power and Charging‚Äã","type":1,"pageTitle":"Aria Gen 2 Glasses Manual","url":"/projectaria_tools/gen2/ark/device/manual#power-and-charging","content":" Only use a USB-C cable to charge your glasses. When first plugged in, a red flashing light inside the right arm indicates the battery is &lt;&lt;&lt;5% but charging.  To turn the device on, press and hold the power button for 3 seconds. When booting is complete, the power LED will turn solid blue for 3 seconds before fading. You can also turn on your glasses by plugging them into a power source; wait for the blue LED before disconnecting the charger.  To turn the device off, hold the power button for 3 seconds and release it. The blue LED will start blinking and then turn off after a short delay, and you will hear an earcon confirming shutdown.    ","version":"Next","tagName":"h3"},{"title":"Resetting the Device‚Äã","type":1,"pageTitle":"Aria Gen 2 Glasses Manual","url":"/projectaria_tools/gen2/ark/device/manual#resetting-the-device","content":" Force Reset‚Äã  A force reset immediately reboots your device, preserving all data. Use this if the device is unresponsive.  Toggle the privacy switch forward (orange not visible).Hold both the action and power buttons for 3 seconds.Wait for the device to reboot.  Factory Reset‚Äã  A factory reset erases all local data, including recordings.  Toggle the privacy switch to the rear (orange visible).Tap (do not hold) the power and action buttons simultaneously.The device will reboot and the status LED will flash blue.The device is now ready to be paired.  Warning: Factory reset will delete all local data. ","version":"Next","tagName":"h3"},{"title":"Configuration","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/vrs_health_check/configuration_and_thresholds","content":"","keywords":"","version":"Next"},{"title":"Listing Available Configurations‚Äã","type":1,"pageTitle":"Configuration","url":"/projectaria_tools/gen2/ark/vrs_health_check/configuration_and_thresholds#listing-available-configurations","content":" To see all available threshold configurations:  run_vrs_health_check --list-configurations   Example output:  Available VrsHealthCheck configurations: AriaGen2_Default, AriaGen2_Location, AriaGen1_Default, AriaGen1_Location   ","version":"Next","tagName":"h2"},{"title":"Viewing Configuration Details‚Äã","type":1,"pageTitle":"Configuration","url":"/projectaria_tools/gen2/ark/vrs_health_check/configuration_and_thresholds#viewing-configuration-details","content":" To see the detailed checks and thresholds for a specific configuration:  run_vrs_health_check --show-configuration-json AriaGen2_Default   This displays the complete JSON structure with all threshold values for that configuration.  ","version":"Next","tagName":"h2"},{"title":"Selecting Configuration for Output‚Äã","type":1,"pageTitle":"Configuration","url":"/projectaria_tools/gen2/ark/vrs_health_check/configuration_and_thresholds#selecting-configuration-for-output","content":" To control which configuration's results are reported as the final status, use the --choose-configuration flag:  run_vrs_health_check --path recording.vrs \\ --output-json results.json \\ --choose-configuration AriaGen2_Location   ‚ö†Ô∏è Important Note: The --choose-configuration flag does not limit which configuration's checks are performed. All available configurations are evaluated during every run and the results are reported in the output.json file. This flag only controls:  Final Exit Code: Which configuration's result determines the tool's exit status (PASS/WARN/FAIL)Terminal Output: Which configuration's detailed results are displayed in the console  This design ensures you get comprehensive analysis data for all configurations while allowing you to focus on the specific threshold set most relevant to your use case.  Understanding Checks  Checks are quality validation rules applied to VRS data streams. Each check evaluates a specific metric against predefined thresholds. There are two fundamental types of checks: Numeric Checks and Boolean Checks.  ","version":"Next","tagName":"h2"},{"title":"1. Numeric Checks‚Äã","type":1,"pageTitle":"Configuration","url":"/projectaria_tools/gen2/ark/vrs_health_check/configuration_and_thresholds#1-numeric-checks","content":" Numeric checks compare measured values (like frame rates, error counts, time intervals) against numerical thresholds.  Supported Check Types:  LE (Less than or Equal): Pass if measured_value &lt;= thresholdLT (Less than): Pass if measured_value &lt; thresholdGE (Greater than or Equal): Pass if measured_value &gt;= thresholdGT (Greater than): Pass if measured_value &gt; threshold  Structure:  { &quot;check_type&quot;: &quot;LE|LT|GE|GT&quot;, &quot;fail_threshold&quot;: &lt;number&gt;, // Optional: value that triggers FAIL &quot;warn_threshold&quot;: &lt;number&gt; // Optional: value that triggers WARN }   Evaluation Logic:  PASS: Value meets all thresholdsWARN: Value exceeds warn_threshold but not fail_thresholdFAIL: Value exceeds fail_threshold  ","version":"Next","tagName":"h2"},{"title":"2. Boolean Checks‚Äã","type":1,"pageTitle":"Configuration","url":"/projectaria_tools/gen2/ark/vrs_health_check/configuration_and_thresholds#2-boolean-checks","content":" Boolean checks verify true/false conditions like calibration validity or file integrity.  Supported Check Types:  EQ_TRUE: Pass if value is trueEQ_FALSE: Pass if value is false  Structure:  { &quot;check_type&quot;: &quot;EQ_TRUE|EQ_FALSE&quot;, &quot;fail_if_mismatch&quot;: true|false // Whether mismatch causes FAIL or WARN }   Evaluation Logic:  PASS: Value matches expected booleanFAIL/WARN: Value doesn't match (severity depends on fail_if_mismatch)   ","version":"Next","tagName":"h2"},{"title":"Open Science Initiative (OSI)","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools","content":"Open Science Initiative (OSI)","keywords":"","version":"Next"},{"title":"Aria Gen 2 Pilot Dataset","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/content","content":"","keywords":"","version":"Next"},{"title":"Dataset Content‚Äã","type":1,"pageTitle":"Aria Gen 2 Pilot Dataset","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/content#dataset-content","content":" The Aria Gen 2 pilot dataset comprises four primary data content types:  raw sensor streams acquired directly from Aria Gen 2 devices and recorded by Profile 8real-time machine perception outputs generated on-device via embedded algorithms during data collectionoffline machine perception results produced by Machine Perception Services (MPS) during post-processing; andoutputs from additional offline perception algorithms. See below for details.  Content (1) and (2) are obtained natively from the device, whereas (3) and (4) are derived through post-hoc processing.  ","version":"Next","tagName":"h2"},{"title":"Additional Perception Algorithms‚Äã","type":1,"pageTitle":"Aria Gen 2 Pilot Dataset","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/content#additional-perception-algorithms","content":" Algorithm\tDescription\tOutputDirectional Automatic Speech Recognition (ASR)\tDistinguishes between wearer and others, generating timestamped transcripts for all sequences. Enables analysis of conversational dynamics and social context.\tTimestamped transcripts of speech. Heart Rate Estimation\tUses PPG sensors to estimate continuous heart rate, reflecting physical activity and physiological state. Coverage for over 95% of recording duration.\tTimestamped heart rate in beats per minute. Hand-Object Interaction Recognition\tSegments left/right hands and interacted objects, enabling analysis of manipulation patterns and object usage.\tSegmentation masks for hands and objects per RGB image. 3D Object Detection (Egocentric Voxel Lifting)\tDetects 2D and 3D bounding boxes for objects in indoor scenes using multi-camera data. Supports spatial understanding and scene reconstruction.\t2D and 3D bounding boxes with class prediction. Depth Estimation (Foundation Stereo)\tGenerates depth maps from overlapping CV cameras, enabling research in 3D scene understanding and object localization.\tDepth images, rectified CV images, and corresponding camera intrinsics/extrinsics. ","version":"Next","tagName":"h3"},{"title":"Customization - Override Check Thresholds","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/vrs_health_check/customization","content":"","keywords":"","version":"Next"},{"title":"Method 1: JSON Override File‚Äã","type":1,"pageTitle":"Customization - Override Check Thresholds","url":"/projectaria_tools/gen2/ark/vrs_health_check/customization#method-1-json-override-file","content":" Create a JSON file with your custom thresholds and use --override-check-file:  Example override file (custom_thresholds.json):  { &quot;configuration_name&quot;: &quot;UserOverride&quot;, &quot;configuration_to_override&quot;: &quot;AriaGen2_Default&quot;, &quot;checks_and_thresholds&quot;: { &quot;Camera Data (SLAM)&quot;: { &quot;ratio_dropped_over_expected&quot;: { &quot;fail_threshold&quot;: 0.05, &quot;warn_threshold&quot;: 0.02 } }, &quot;IMU Data (SLAM)&quot;: { &quot;non_monotonic&quot;: { &quot;ignore&quot;: true } } } }   Usage:  run_vrs_health_check --path recording.vrs \\ --output-json results.json \\ --override-check-file custom_thresholds.json \\ --choose-configuration UserOverride   ","version":"Next","tagName":"h2"},{"title":"Method 2: Command Line Overrides‚Äã","type":1,"pageTitle":"Customization - Override Check Thresholds","url":"/projectaria_tools/gen2/ark/vrs_health_check/customization#method-2-command-line-overrides","content":" For quick adjustments, use --override-checks with comma-separated overrides:  Syntax:  --override-checks &quot;stream_name.check_name.threshold_type=value,...&quot;   Examples:  # Override camera drop threshold --override-checks &quot;Camera Data (SLAM).ratio_dropped_over_expected.fail_threshold=0.05&quot; # Multiple overrides --override-checks &quot;Camera Data (SLAM).ratio_dropped_over_expected.fail_threshold=0.05,IMU Data (SLAM).non_monotonic.ignore=true&quot; # Specify base configuration to override --configuration-to-override AriaGen2_Default --override-checks &quot;...&quot;   Usage:  run_vrs_health_check --path recording.vrs \\ --output-json result.json \\ --configuration-to-override AriaGen2_Default \\ --override-checks &quot;Pose Data Class (hand).non_monotonic.ignore=true&quot; \\ --choose-configuration UserOverride   ","version":"Next","tagName":"h2"},{"title":"Disabling Checks‚Äã","type":1,"pageTitle":"Customization - Override Check Thresholds","url":"/projectaria_tools/gen2/ark/vrs_health_check/customization#disabling-checks","content":" To completely disable a check, set ignore=true:  JSON method:  &quot;IMU Data (SLAM)&quot;: { &quot;non_monotonic&quot;: { &quot;ignore&quot;: true } }   Command line method:  --override-checks &quot;IMU Data (SLAM).non_monotonic.ignore=true&quot;   Best Practices for Overriding  Use JSON files for complex, reusable configuration changesUse command line overrides for quick testing or one-off adjustmentsDocument your custom thresholds and the reasoning behind themTest your overrides with known good and bad recordings to validate effectivenessStart with existing configurations as a base rather than creating from scratch ","version":"Next","tagName":"h3"},{"title":"Download","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/download","content":"Download install and download","keywords":"","version":"Next"},{"title":"Loading and Visualizing","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/loader-viz","content":"Loading and Visualizing Tutorial of loading and visualizing .","keywords":"","version":"Next"},{"title":"Installation and Quick Start","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/vrs_health_check/installation","content":"","keywords":"","version":"Next"},{"title":"Installing VRS Health Check‚Äã","type":1,"pageTitle":"Installation and Quick Start","url":"/projectaria_tools/gen2/ark/vrs_health_check/installation#installing-vrs-health-check","content":" Vrs Health Check is available as a Python package on PyPI. You can install it using pip. And we strongly recommend installing it under a python virtual environment:  Linux &amp; macOSWindows rm -rf $HOME/projectaria_vhc_env python3 -m venv $HOME/projectaria_vhc_env source $HOME/projectaria_vhc_env/bin/activate python3 -m pip install projectaria-tools-vrs-health-check   ","version":"Next","tagName":"h2"},{"title":"Quick Start - Running the tool‚Äã","type":1,"pageTitle":"Installation and Quick Start","url":"/projectaria_tools/gen2/ark/vrs_health_check/installation#quick-start---running-the-tool","content":" run_vrs_health_check -- --path &lt;VRS_FILE&gt; --output-json &lt;RESULTS_FILE&gt; [OPTIONS]   Note that if the above command emits some error, please refer to the following troubleshooting section.  ","version":"Next","tagName":"h2"},{"title":"Available Options:‚Äã","type":1,"pageTitle":"Installation and Quick Start","url":"/projectaria_tools/gen2/ark/vrs_health_check/installation#available-options","content":" üìÅ Basic Options‚Äã  Option\tDescription--path\tPath to VRS file or directory containing VRS files --output-json\tPath to save detailed JSON results  üñ•Ô∏è Terminal Output Control‚Äã  Option\tDescription--print-stats\tPrint detailed statistics to console --disable-logging\tSuppress diagnostic log messages (quiet mode)  ‚öôÔ∏è Configuration Management‚Äã  Option\tDescription--list-configurations\tList all available threshold configurations --show-configuration-json &lt;CONFIG&gt;\tShow details of a specific configuration --choose-configuration &lt;CONFIG&gt;\tSelect a configuration to output its results  üîß Custom Threshold Overrides‚Äã  Option\tDescription--override-check-file &lt;FILE&gt;\tJSON file with custom threshold overrides --override-checks &lt;OVERRIDES&gt;\tComma-separated threshold overrides --configuration-to-override &lt;CONFIG&gt;\tBase configuration for overrides  ","version":"Next","tagName":"h2"},{"title":"Expected Outputs‚Äã","type":1,"pageTitle":"Installation and Quick Start","url":"/projectaria_tools/gen2/ark/vrs_health_check/installation#expected-outputs","content":" The VRS Health Check tool provides two primary outputs:  ","version":"Next","tagName":"h2"},{"title":"1. Exit Code‚Äã","type":1,"pageTitle":"Installation and Quick Start","url":"/projectaria_tools/gen2/ark/vrs_health_check/installation#1-exit-code","content":" The tool returns an exit code indicating the overall health check result:  Exit Code\tStatus\tDescription0\tPASS\tHealth check passed successfully 1\tWARN\tHealth check passed with warnings 2\tFAIL\tHealth check failed Others 64\tEX_USAGE\tCommand line usage error 66\tEX_NOINPUT\tCannot open input file or setup failure  ","version":"Next","tagName":"h3"},{"title":"2. JSON Results File‚Äã","type":1,"pageTitle":"Installation and Quick Start","url":"/projectaria_tools/gen2/ark/vrs_health_check/installation#2-json-results-file","content":" The --output-json parameter creates a detailed JSON file containing:  Overall status: per-configuration check result : pass / warn / fail.Failed check list: List of checks that result in fail.Warn check list: List of checks that result in warn.Performed checks with details: Complete list of all performed checks, with details including value, threshold, and result.Unperformed checks: List of VRS stream statistics that are computed, but not used for threshold checking.See the following for an example output json file:  { &quot;AriaGen2_Default&quot;: { &quot;final_result&quot;: &quot;fail&quot;, &quot;failed_checks&quot;: [ &quot;RGB Camera Class #1.time_error&quot; ], &quot;warn_checks&quot;: [], &quot;performed_checks_with_details&quot;: { &quot;Ambient Light Sensor (ALS) Data Class #1&quot;: { &quot;bad&quot;: { &quot;check_type&quot;: &quot;LE&quot;, &quot;fail_threshold&quot;: 0.0, &quot;result&quot;: &quot;pass&quot;, &quot;value&quot;: 0.0 }, &quot;end_offset_from_file_us&quot;: { &quot;check_type&quot;: &quot;LE&quot;, &quot;fail_threshold&quot;: 3000000.0, &quot;result&quot;: &quot;pass&quot;, &quot;value&quot;: 14221.0, &quot;warn_threshold&quot;: 1500000.0 }, &quot;non_monotonic&quot;: { &quot;check_type&quot;: &quot;LE&quot;, &quot;fail_threshold&quot;: 0.0, &quot;result&quot;: &quot;pass&quot;, &quot;value&quot;: 0.0 }, ... }, &quot;Barometer Data Class #1&quot;: { &quot;bad&quot;: { &quot;check_type&quot;: &quot;LE&quot;, &quot;fail_threshold&quot;: 0.0, &quot;result&quot;: &quot;pass&quot;, &quot;value&quot;: 0.0 }, ... }, ... }, &quot;unperformed_checks&quot;: { &quot;Ambient Light Sensor (ALS) Data Class #1&quot;: { &quot;dropped&quot;: 0, &quot;expected&quot;: 368, &quot;largest_deviation_from_period_us&quot;: 1577, &quot;processed&quot;: 372, &quot;time_error&quot;: 0 }, ... } }, &quot;AriaGen2_Location&quot;: { &quot;final_result&quot;: &quot;fail&quot;, &quot;failed_checks&quot;: [ &quot;RGB Camera Class #1.time_error&quot; ], &quot;warn_checks&quot;: [], &quot;performed_checks_with_details&quot;: { ... }, &quot;unperformed_checks&quot;: { ... } } }   ","version":"Next","tagName":"h3"},{"title":"Troubleshoot‚Äã","type":1,"pageTitle":"Installation and Quick Start","url":"/projectaria_tools/gen2/ark/vrs_health_check/installation#troubleshoot","content":" ","version":"Next","tagName":"h2"},{"title":"run_vrs_health_check exists but running the command emits some error‚Äã","type":1,"pageTitle":"Installation and Quick Start","url":"/projectaria_tools/gen2/ark/vrs_health_check/installation#run_vrs_health_check-exists-but-running-the-command-emits-some-error","content":" This is likely due to that you have previously installed projectaria-tools (version &lt; 2.0) python package globally, instead installing in a virtual Python environment. This leads to that the Python tool with the same name projectaria_tools becoming a globally visible binary tool.  This can be validated by running:  which run_vrs_health_check   if you see it pointing to anywhere other than ~/$YOUR_NEW_VHC_PYTHON_VENV/bin/run_vrs_health_check, then this is the culprit.  To resolve, simply upgrade projectaria-tools to &gt;= 2.0.0, it will resolve this naming conflict. ","version":"Next","tagName":"h3"},{"title":"Advanced Installation From Source Code","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/projectariatools/advanced-installation","content":"","keywords":"","version":"Next"},{"title":"Pre-requisites: install dependencies‚Äã","type":1,"pageTitle":"Advanced Installation From Source Code","url":"/projectaria_tools/gen2/research-tools/projectariatools/advanced-installation#prerequisites","content":" Before proceed to any of the advanced installation steps, please follow the command in the installer widget on Quick Start page to pull the correct branch from Github.  Then install the following third party dependency lib:  UbuntuFedoraMacOS # Deactivate CONDA / MAMBA virtual environment you may be in! This is IMPORTANT! conda deactivate # Install build essentials sudo apt install build-essential git cmake # Install VRS/Pangolin dependencies sudo apt install libgtest-dev libgmock-dev libgoogle-glog-dev libfmt-dev \\ liblz4-dev libzstd-dev libxxhash-dev libboost-all-dev libpng-dev \\ libjpeg-turbo8-dev libturbojpeg0-dev libglew-dev libgl1-mesa-dev libeigen3-dev   Install FFmpeg: Aria Gen2 image data is encoded with H.265 codec.projectaria_tools relies on the decoding functions in the vrs library to decode H.265 image data. Here, we will build FFmpeg from source code with the exact same script from the vrs library:  # Set this to the root directory where you pulled the codebase SRC_DIR=${HOME}/Documents/projectaria_sandbox/projectaria_tools/ cd ${SRC_DIR} # Remove previously installed ffmpeg rm -rf ~/vrs_third_party_libs/ffmpeg/ # ffmpeg will be installed under ~/vrs_third_party_libs/ffmpeg/ ./build_third_party_libs/build_ffmpeg_linuxunix.sh   ","version":"Next","tagName":"h2"},{"title":"Build projectaria_tools lib using CMake, without visualization‚Äã","type":1,"pageTitle":"Advanced Installation From Source Code","url":"/projectaria_tools/gen2/research-tools/projectariatools/advanced-installation#cmake_no_viz","content":" cd ${SRC_DIR} cmake -B ./build/ -S ./ cd build/ make -j8   ","version":"Next","tagName":"h2"},{"title":"Build projectaria_tools lib using CMake, with visualization‚Äã","type":1,"pageTitle":"Advanced Installation From Source Code","url":"/projectaria_tools/gen2/research-tools/projectariatools/advanced-installation#cmake_with_viz","content":" The C++ visualization binaries in projectaria_tools are built using the Pangolin library. Therefore, you need to install Pangolin as a system library first.  cd /tmp git clone --recursive https://github.com/stevenlovegrove/Pangolin.git mkdir -p Pangolin_Build &amp;&amp; cd Pangolin_Build cmake -DCMAKE_BUILD_TYPE=Release -DBUILD_TOOLS=OFF -DBUILD_PANGOLIN_PYTHON=OFF \\ -DBUILD_EXAMPLES=OFF ../Pangolin/ make -j8 sudo make install   Then you can build projectaria_tools in CMake:  cd ${SRC_DIR} cmake -B ./build/ -S ./ -DPROJECTARIA_TOOLS_BUILD_TOOLS=ON cd build/ make -j8   ","version":"Next","tagName":"h2"},{"title":"Build projectaria_tools Python package from source code‚Äã","type":1,"pageTitle":"Advanced Installation From Source Code","url":"/projectaria_tools/gen2/research-tools/projectariatools/advanced-installation#python_from_source","content":" First, make sure you have installed the dependencies libraries as pre-requisites. You can build a local version of projectaria-tools Python package by doing the following. We strongly recommend you build this within a Python virtual environment.  # Go to your check-out source code directory cd ${SRC_DIR} python3 -m pip install --upgrade pip python3 -m pip install .   ","version":"Next","tagName":"h2"},{"title":"Build projectaria_tools Python package from source code with type hinting‚Äã","type":1,"pageTitle":"Advanced Installation From Source Code","url":"/projectaria_tools/gen2/research-tools/projectariatools/advanced-installation#python_from_source_type_hint","content":" Generate Python type hinting with generate_stubs.py script once projectaria_tools package is installedInstall type hinting package for projectaria_tools  # Install pybind11-stubgen to generate the stub files python3 -m pip install pybind11-stubgen==1.1 # Generate stubs cd $HOME/Documents/projectaria_sandbox/projectaria_tools python3 generate_stubs.py cp -r projectaria_tools-stubs/projectaria_tools . # Install projectaria-tools python package from source code. python3 -m pip install .  ","version":"Next","tagName":"h2"},{"title":"Build projectaria_tools in isolation with Pixi","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/projectariatools/advanced-installation-pixi","content":"","keywords":"","version":"Next"},{"title":"Step 1: Install Pixi‚Äã","type":1,"pageTitle":"Build projectaria_tools in isolation with Pixi","url":"/projectaria_tools/gen2/research-tools/projectariatools/advanced-installation-pixi#step-1-install-pixi","content":" # Install Pixi curl -fsSL https://pixi.sh/install.sh | bash   ","version":"Next","tagName":"h3"},{"title":"Step 2: Compile and test the code‚Äã","type":1,"pageTitle":"Build projectaria_tools in isolation with Pixi","url":"/projectaria_tools/gen2/research-tools/projectariatools/advanced-installation-pixi#step-2-compile-and-test-the-code","content":" C++Python # When this command line first run, it will collect required dependencies, and then trigger the build and unit test pixi run run_c   ","version":"Next","tagName":"h3"},{"title":"Step 3: Access the compiled artifacts in C++‚Äã","type":1,"pageTitle":"Build projectaria_tools in isolation with Pixi","url":"/projectaria_tools/gen2/research-tools/projectariatools/advanced-installation-pixi#step-3-access-the-compiled-artifacts-in-c","content":" # Activate the environment pixi shell # You can now run and use things from the projects. # For example, with Python, you can import the projectaria_tools package and use it. python import projectaria_tools dir(projectaria_tools) # Will print &gt;&gt;&gt;['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'core']  ","version":"Next","tagName":"h3"},{"title":"Aria Gen 2 Pilot Dataset Format","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/format","content":"","keywords":"","version":"Next"},{"title":"File format‚Äã","type":1,"pageTitle":"Aria Gen 2 Pilot Dataset Format","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/format#file-format","content":" closed_loop_trajectory.csv, open_loop_trajectory.csv, semidense_observations.csv.gz, semidense_points.csv.gz, online_calibration.jsonl and hand_tracking_results.csv follow MPS data format.  ","version":"Next","tagName":"h2"},{"title":"diarization_results.csv‚Äã","type":1,"pageTitle":"Aria Gen 2 Pilot Dataset Format","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/format#diarization_resultscsv","content":" Column\tType\tDescriptionstart_timestamp_ns\tint\tTimestamp, in nanoseconds, in device time domain. end_timestamp_ns\tint\tTimestamp, in nanoseconds, in device time domain. speaker\tstring\tUnique identifier of the speaker content\tstring\tThe ASR results in text.  ","version":"Next","tagName":"h3"},{"title":"2d_bounding_box.csv‚Äã","type":1,"pageTitle":"Aria Gen 2 Pilot Dataset Format","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/format#2d_bounding_boxcsv","content":" Column\tType\tDescriptionstream_id\tstring\tcamera stream id associated with the bounding box image object_uid\tuint64_t\tid of the object instance timestamp[ns]\tint64_t\ttimestamp of the image in nanoseconds x_min[pixel]\tint\tminimum dimension in the x axis x_max[pixel]\tint\tmaximum dimension in the x axis y_min[pixel]\tint\tminimum dimension in the y axis y_max[pixel]\tint\tmaximum dimension in the y axis visibility_ratio[%]\tdouble\tpercentage of the object that is visible (0: not visible, 1: fully visible). NOTE: for EVL this is estimated from semidense points and is NOT accurate and fills -1.  ","version":"Next","tagName":"h3"},{"title":"3d_bounding_box.csv‚Äã","type":1,"pageTitle":"Aria Gen 2 Pilot Dataset Format","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/format#3d_bounding_boxcsv","content":" Column\tType\tDescriptionobject_uid\tuint64_t\tid of the instance timestamp[ns]\tint64_t\ttimestamp of the image in nanoseconds. -1 means the instance is static p_local_obj_xmin[m]\tdouble\tminimum dimension in the x axis (in meters) of the bounding box p_local_obj_xmax[m]\tdouble\tmaximum dimension in the x axis (in meters) of the bounding box p_local_obj_ymin[m]\tdouble\tminimum dimension in the y axis (in meters) of the bounding box p_local_obj_ymax[m]\tdouble\tmaximum dimension in the y axis (in meters) of the bounding box p_local_obj_zmin[m]\tdouble\tminimum dimension in the z axis (in meters) of the bounding box p_local_obj_zmax[m]\tdouble\tmaximum dimension in the z axis (in meters) of the bounding box  ","version":"Next","tagName":"h3"},{"title":"instances.json‚Äã","type":1,"pageTitle":"Aria Gen 2 Pilot Dataset Format","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/format#instancesjson","content":" { object_id:{ &quot;canonical_pose&quot;:{&quot;front_vector&quot;:unit_vector,&quot;up_vector&quot;:unit_vector}, &quot;category&quot;:xxx, &quot;category_uid&quot;:xxx, &quot;instance_id&quot;:xxx, &quot;instance_name&quot;:text description, &quot;instance_type&quot;:&quot;object&quot;, &quot;motion_type&quot;:&quot;static&quot;, &quot;prototype_name&quot;:text description,\u000b&quot;rigidity&quot;: &quot;rigid&quot;, &quot;rotational_symmetry&quot;:{&quot;is_annotated&quot;:false} }, ... # example &quot;12&quot;:{ &quot;canonical_pose&quot;:{&quot;front_vector&quot;:[0,0,1],&quot;up_vector&quot;:[0,1,0]},&quot; category&quot;:&quot;Screen&quot;, &quot;category_uid&quot;:10, &quot;instance_id&quot;:12, &quot;instance_name&quot;:&quot;screen, television, laptop screen (not keyboard), tablet screen, computer monitor, display, not mobile phone screen&quot;, &quot;instance_type&quot;:&quot;object&quot;, &quot;motion_type&quot;:&quot;static&quot;, &quot;prototype_name&quot;:&quot;screen, television, laptop screen (not keyboard), tablet screen, computer monitor, display, not mobile phone screen&quot;, &quot;rigidity&quot;:&quot;rigid&quot;, &quot;rotational_symmetry&quot;:{&quot;is_annotated&quot;:false} }, ... }   ","version":"Next","tagName":"h3"},{"title":"scene\\_objects.csv‚Äã","type":1,"pageTitle":"Aria Gen 2 Pilot Dataset Format","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/format#scene_objectscsv","content":" Column\tType\tDescriptionobject_uid\tuint64_t\tid of the object instance timestamp[ns]\tint64_t\ttimestamp of the image in nanoseconds. -1 means the instance is static t_wo_x[m]\tdouble\tx translation from object frame to world (scene) frame (in meters) t_wo_y[m]\tdouble\ty translation from object frame to world (scene) frame (in meters) t_wo_z[m]\tdouble\tz translation from object frame to world (scene) frame (in meters) q_wo_w\tdouble\tw component of quaternion from object frame to world (scene) frame q_wo_x\tdouble\tx component of quaternion from object frame to world (scene) frame q_wo_y\tdouble\ty component of quaternion from object frame to world (scene) frame q_wo_z\tdouble\tz component of quaternion from object frame to world (scene) frame  ","version":"Next","tagName":"h3"},{"title":"heart_rate_results.csv‚Äã","type":1,"pageTitle":"Aria Gen 2 Pilot Dataset Format","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/format#heart_rate_resultscsv","content":" Column\tType\tDescriptiontimestamp_ns\tint\tTimestamp, in nanoseconds, in device time domain. heart_rate_bpm\tint\tThe estimated heart rate (beats per minutes) at the specific timestamp  ","version":"Next","tagName":"h3"},{"title":"Depth Folder‚Äã","type":1,"pageTitle":"Aria Gen 2 Pilot Dataset Format","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/format#depth-folder","content":" Depth output consists of the following files in a single directory named ‚Äúdepth‚Äù:  Rectified depth maps as 512 x 512, 16-bit grayscale PNG images. The pixel contents are integers expressing the depth along the pixel‚Äôs ray direction, in units of mm. This is the same format used in ASE. depth_{:08d}.png Matching rectified front-left SLAM camera images as 8-bit grayscale PNGs. image_{:08d}.png A JSON file containing camera transforms and intrinsics for the rectified pinhole camera, for each frame. pinhole_camera_parameters.json  Example JSON:  [ { &quot;T_world_camera&quot;: { &quot;QuaternionXYZW&quot;: [ -0.56967133283615112, 0.35075613856315613, 0.60195386409759521, 0.4360002875328064 ], &quot;Translation&quot;: [ -0.0005431128665804863, 0.0053895660676062107, -0.0027622696943581104 ] }, &quot;camera&quot;: { &quot;ModelName&quot;: &quot;Linear:fu,fv,u0,v0&quot;, &quot;Parameters&quot;: [ 306.38043212890625, 306.38043212890625, 254.6942138671875, 257.29779052734375 ] }, &quot;index&quot;: 0, &quot;frameTimestampNs&quot;: 34234234234, } ]   ","version":"Next","tagName":"h3"},{"title":"hand_object_interaction_results.json‚Äã","type":1,"pageTitle":"Aria Gen 2 Pilot Dataset Format","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/format#hand_object_interaction_resultsjson","content":" Standard COCO json format, with category_id 1 (left hand) 2 (right hand) 3 (hand interacting object). Example Json file:  [ { &quot;segmentation&quot;: { &quot;size&quot;: [ 1512, 2016 ], &quot;counts&quot;: &quot;TUk`13U_10L8L0K7N0UJOYlN9eS1G[lN9aS11okN=mS1CSlN=kS1GokNa0a1kNRl0d0]ROa0a1kNnk0]1aPOhNSN`1Q5dNYl0T1cPOhNSN`1Q5dNYl0T1cPOhNSN`1Q5dNQl0T2mnNT3V1QMko0KonNT3V1QMio0Z7jnNRIVQ1n6jnNRIVQ1`:00N200L400M3000000N200N200N2000000N200000002N00000N4N0N200N4N0000002N00000N4N0000002N004L004L0000002N002N00000000000020N02N002N0020N02N0020N03M002N002N00f0ZO004L002N00000N4N000000000000000000000000000000000002L2000000000000000002N000N20000000000000000000000000000002N00000000000000000000N2000N20N20000000N2000000000N202N00000000000000000000000000000000000000000000000N20000000000000000N202N00000000000N20000000N200000000000000N200N20000000000000N02000000000000000000000N202N000N202L202L202L200N20000N200N200N202I504hNT102iNU103K202H606XOb004mJYmNTMPS1l2PmNTMRS1f2VmNRMnR1n2RmNRMPS1c2amNaLSS1_3mlNaLdU1^Ok50?VO;06^O&lt;0de]n0&quot; }, &quot;bbox&quot;: [ 1050.1774193548385, 738.1073369565217, 325.16129032258095, 535.0801630434783 ], &quot;score&quot;: 1.0, &quot;image_id&quot;: 2620886, # timestamp_ns = image_id * 1e6 &quot;category_id&quot;: 3 }, ... ]```  ","version":"Next","tagName":"h3"},{"title":"Tutorial 1: VrsDataProvider Basics","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/dataprovider","content":"","keywords":"","version":"Next"},{"title":"Introduction‚Äã","type":1,"pageTitle":"Tutorial 1: VrsDataProvider Basics","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/dataprovider#introduction","content":" Aria Gen2 glasses are Meta's dedicated research tool in an always-on glasses form factor. The data recorded by Aria Gen2 glasses are stored in VRS files, where each VRS file captures time-synchronized data streams from various sensors, such cameras, IMUs, audio, and more. The VrsDataProvider interface provides a unified way to access multimodal sensor data from these VRS files.  What you'll learn:  How to create a VrsDataProvider from a VRS file.Discover available sensor data streams, and check their configurationsRetrieve data using either sequential (index-based) or temporal (timestamp-based) access APIs.Learn about timing domains and time query options  Prerequisites: Basic Python knowledge and a general understanding of multimodal sensor data.  ","version":"Next","tagName":"h2"},{"title":"Basic File Loading‚Äã","type":1,"pageTitle":"Tutorial 1: VrsDataProvider Basics","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/dataprovider#basic-file-loading","content":" The create_vrs_data_provider($FILE_PATH) factory function will create a vrs_data_provider object. This object is your entry point for accessing VRS data.  from projectaria_tools.core import data_provider # Load local VRS file vrs_file_path = &quot;path/to/your/recording.vrs&quot; vrs_data_provider = data_provider.create_vrs_data_provider(vrs_file_path)   ","version":"Next","tagName":"h2"},{"title":"Stream Discovery and Navigation‚Äã","type":1,"pageTitle":"Tutorial 1: VrsDataProvider Basics","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/dataprovider#stream-discovery-and-navigation","content":" ","version":"Next","tagName":"h2"},{"title":"Understanding Stream IDs‚Äã","type":1,"pageTitle":"Tutorial 1: VrsDataProvider Basics","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/dataprovider#understanding-stream-ids","content":" A VRS file contains multiple streams, each storing data from a specific sensor or on-device algorithm result.  Each VRS stream is identified by a unique StreamId (e.g. 1201-1), consisting RecordableTypeId (sensor type, e.g. 1201, standing for &quot;SLAM camera&quot;), and an instance_id (for multiple sensors of the same type, e.g. -1, standing for &quot;instance #1 of this sensor type&quot;). Below are some common RecordableTypeId in Aria recordings. Full definitions of all Recordable Types are given in this wiki page (TODO: Add website page), or refer to the StreamId.h file in the VRS repo.  RecordableTypeId\tDescription214\tRGB camera stream 1201\tSLAM camera stream 211\tEyeTracking camera stream 1202\tIMU sensor stream 231\tAudio sensor stream 373\tEyeGaze data stream from on-device EyeTracking algorithm.  ","version":"Next","tagName":"h3"},{"title":"Query StreamId By Label‚Äã","type":1,"pageTitle":"Tutorial 1: VrsDataProvider Basics","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/dataprovider#query-streamid-by-label","content":" # Get all available streams all_streams = vrs_data_provider.get_all_streams() print(f&quot;Found {len(all_streams)} streams in the VRS file:&quot;) # Print out each stream id, and their corresponding sensor label for stream_id in all_streams: label = vrs_data_provider.get_label_from_stream_id(stream_id) print(f&quot; --- Data stream {stream_id}'s label is: {label}&quot;)   # Find a specific stream's StreamId by sensor label print(&quot;Seeking RGB data stream...&quot;) rgb_stream_id = vrs_data_provider.get_stream_id_from_label(&quot;camera-rgb&quot;) if rgb_stream_id is not None: print(f&quot;Found camera-rgb stream in VRS file: {rgb_stream_id}&quot;) else: print(&quot;Cannot find camera-rgb stream in VRS file.&quot;)   ","version":"Next","tagName":"h3"},{"title":"Sensor Data Query APIs‚Äã","type":1,"pageTitle":"Tutorial 1: VrsDataProvider Basics","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/dataprovider#sensor-data-query-apis","content":" Query by index‚Äã  The query-by-index API allows you to retrieve the k-th data sample from a specific stream using the following syntax:  get_&lt;SENSOR&gt;_data_by_index(stream_id, index)   where &lt;SENSOR&gt; can be replaced by any sensor data type available in Aria VRS. See here for a full list of supported sensor types  This API is commonly used for sequential processing‚Äîsuch as iterating through all frames of a single stream‚Äîor when you know the exact frame number you want to query within a specific stream.  Important Note:The indices in each stream are independent and not correlated across different sensor streams. For example, the i-th RGB image does not necessarily correspond to the i-th SLAM image. This is because different sensors may operate at different frequencies or have missing frames, so their data streams are not synchronized by index.  # Visualize with Rerun import rerun as rr rr.init(&quot;rerun_viz_query_by_index&quot;) rr.notebook_show() # Get number of samples in stream num_samples = vrs_data_provider.get_num_data(rgb_stream_id) print(f&quot;RGB stream has a total of {num_samples} frames\\n&quot;) # Access frames sequentially, and plot the first few frames first_few = min(10, num_samples) print(f&quot;Printing the capture timestamps from the first {first_few} frames&quot;) for i in range(first_few): # First 10 frames image_data, image_record = vrs_data_provider.get_image_data_by_index( rgb_stream_id, i ) # Access image properties timestamp_ns = image_record.capture_timestamp_ns print(f&quot;Frame {i}: timestamp = {timestamp_ns}&quot;) # Process image data if image_data.is_valid(): rr.set_time_nanos(&quot;device_time&quot;, timestamp_ns) rr.log(&quot;camera_rgb&quot;, rr.Image(image_data.to_numpy_array()))   Query by Timestamp: TimeDomain and TimeQueryOptions‚Äã  A key feature of Aria devices is the ability to capture time-synchronized, multi-modal sensor data. To help you access this data with precise temporal control, projectaria_tools provides a comprehensive suite of time-based APIs.  The most commonly used is the timestamp-based query:  get_&lt;SENSOR&gt;_by_time_ns(stream_id, time_ns, time_domain=None, time_query_options=None)   where &lt;SENSOR&gt; can be replaced by any sensor data type available in Aria VRS. See here for a full list of supported sensor types  This API is often used to synchronize data across multiple sensor streams, fetch sensor data at specific timestamps, or perform temporal analysis.  TimeDomain and TimeQueryOptions‚Äã  When querying sensor data by timestamp, two important concepts are:  TimeDomain: Specifies the time reference for your query.TimeQueryOptions: Controls how the API selects data relative to your requested timestamp.  Below are all available options for each:  TimeDomain Options‚Äã  Name\tDescription\tTypical Use CaseRECORD_TIME\tTimestamp stored directly in the VRS index. Fast access, but time domain may vary.\tQuick access, not recommended for sync. DEVICE_TIME\tAccurate device capture time. All sensors on the same Aria device share this domain.\tRecommended for single-device data. HOST_TIME\tArrival time in the host computer's domain. May not be accurate.\tDebugging, host-side analysis. TIME_CODE\t[Aria-Gen1 only] TimeSync server's domain using external time-code devices, accurate across devices in multi-device capture.\tMulti-device synchronization. TIC_SYNC\t[Aria-Gen1 only] TimeSync server's domain using tic-sync, accurate for multi-device capture.\tMulti-device synchronization. SubGhz\t[Aria-Gen2 only] TimeSync server's domain using SubGhz signals, accurate for multi-device capture.\tMulti-device synchronization. Utc\tUTC time domain, only seconds-level accuracy.\tCoarse, global time reference.  TimeQueryOptions‚Äã  Name\tDescriptionBefore\tReturns the last valid data with timestamp &lt;= t_query. After\tReturns the first valid data with timestamp &gt;= t_query. Closest\tReturns the data sample closest to t_query. If two are equally close, returns the one before the query.  For detailed usage and best practices‚Äîespecially for time-sync across multiple devices‚Äîsee Tutorial_6.  from projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions rr.init(&quot;rerun_viz_query_by_timestamp&quot;) rr.notebook_show() # Get time bounds for RGB images first_timestamp_ns = vrs_data_provider.get_first_time_ns(rgb_stream_id, TimeDomain.DEVICE_TIME) last_timestamp_ns = vrs_data_provider.get_last_time_ns(rgb_stream_id, TimeDomain.DEVICE_TIME) # Query specific timestamp target_time_ns = first_timestamp_ns + int(1e9) # 1 second later image_data, image_record = vrs_data_provider.get_image_data_by_time_ns( rgb_stream_id, target_time_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST ) actual_time_ns = image_record.capture_timestamp_ns print(f&quot;Requested RGB data that is closest to: {target_time_ns} ns, Got closest sample at: {actual_time_ns} ns&quot;) # Plot RGB and SLAM images at approx 1 hz camera_label_list = [&quot;camera-rgb&quot;, &quot;slam-front-left&quot;, &quot;slam-front-right&quot;, &quot;slam-side-left&quot;, &quot;slam-side-right&quot;] camera_stream_ids = [vrs_data_provider.get_stream_id_from_label(camera_label) for camera_label in camera_label_list] query_timestamp_ns = first_timestamp_ns for _ in range(10): for label, stream_id in zip(camera_label_list, camera_stream_ids): # Query each camera's data according to query timestamp image_data, image_record = vrs_data_provider.get_image_data_by_time_ns( stream_id, query_timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST) # note that the actual timestamp of the image data is stored within image_record. It can be different from query_time. capture_time_ns = image_record.capture_timestamp_ns # Plot to Rerun if image_data.is_valid(): rr.set_time_nanos(&quot;device_time&quot;, capture_time_ns) rr.log(label, rr.Image(image_data.to_numpy_array())) query_timestamp_ns = query_timestamp_ns + int(1e9) # 1 second  ","version":"Next","tagName":"h2"},{"title":"Tutorial 2: Device calibration","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/calibration","content":"","keywords":"","version":"Next"},{"title":"Introduction‚Äã","type":1,"pageTitle":"Tutorial 2: Device calibration","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/calibration#introduction","content":" Most sensors in Aria glasses are calibrated both extrinsically and intrinsically, allowing you to rectify sensor measurements to real-world quantities. Calibration is performed per device, and the information is stored in the VRS file. This tutorial demonstrates how to work with device calibration in Project Aria using projectaria_tools.  ","version":"Next","tagName":"h2"},{"title":"What You'll Learn‚Äã","type":1,"pageTitle":"Tutorial 2: Device calibration","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/calibration#what-youll-learn","content":" How to obtain each sensor's calibration dataCamera calibration: projection and unprojection, and how to post-process images according to calibration (distort).IMU calibration: measurement rectification.Multi-sensor coordination and sensor poses, and the concept of the &quot;Device&quot; frame.  Pre-requisite:Familiarity with VRS basics from Tutorial_1_vrs_Data_provider_basics.ipynb.  ","version":"Next","tagName":"h3"},{"title":"Obtaining Device Calibration Content‚Äã","type":1,"pageTitle":"Tutorial 2: Device calibration","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/calibration#obtaining-device-calibration-content","content":" Each VRS file's device calibration can be accessed as a DeviceCalibration instance via the VrsDataProvider API.  from projectaria_tools.core import data_provider # Load local VRS file vrs_file_path = &quot;path/to/your/recording.vrs&quot; vrs_data_provider = data_provider.create_vrs_data_provider(vrs_file_path) # Obtain device calibration device_calib = vrs_data_provider.get_device_calibration() if device_calib is None: raise RuntimeError( &quot;device calibration does not exist! Please use a VRS that contains valid device calibration for this tutorial. &quot; ) # You can obtain device version (Aria Gen1 vs Gen2), or device subtype (DVT with small/large frame width + short/long temple arms, etc) information from calibration if device_calib is not None: device_version = device_calib.get_device_version() device_subtype = device_calib.get_device_subtype() print(f&quot;Aria Device Version: {device_version}&quot;) print(f&quot;Device Subtype: {device_subtype}&quot;)   ","version":"Next","tagName":"h3"},{"title":"Accessing Individual Sensor Calibration‚Äã","type":1,"pageTitle":"Tutorial 2: Device calibration","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/calibration#accessing-individual-sensor-calibration","content":" DeviceCalibration provides APIs to query the intrinsics and extrinsics of each calibrated sensor.  1. Camera calibration content‚Äã  # Get sensor labels within device calibration all_labels = device_calib.get_all_labels() print(f&quot;All sensors within device calibration: {all_labels}&quot;) print(f&quot;Cameras: {device_calib.get_camera_labels()}&quot;) # Query a specific camera's calibration rgb_camera_label = &quot;camera-rgb&quot; camera_calib = device_calib.get_camera_calib(rgb_camera_label) if camera_calib is None: raise RuntimeError( &quot;camera-rgb calibration does not exist! Please use a VRS that contains valid RGB camera calibration for this tutorial. &quot; ) print(f&quot;-------------- camera calibration for {rgb_camera_label} ----------------&quot;) print(f&quot;Image Size: {camera_calib.get_image_size()}&quot;) print(f&quot;Camera Model Type: {camera_calib.get_model_name()}&quot;) print( f&quot;Camera Intrinsics Params: {camera_calib.get_projection_params()}, \\n&quot; f&quot;where focal is {camera_calib.get_focal_lengths()}, &quot; f&quot;and principal point is {camera_calib.get_principal_point()}\\n&quot; ) # Get extrinsics (device to camera transformation) T_device_camera = camera_calib.get_transform_device_camera() print(f&quot;Camera Extrinsics T_Device_Camera:\\n{T_device_camera.to_matrix()}&quot;)   2. IMU calibration content‚Äã  imu_label = &quot;imu-right&quot; imu_calib = device_calib.get_imu_calib(imu_label) if imu_calib is None: raise RuntimeError( &quot;imu-right calibration does not exist! Please use a VRS that contains valid IMU calibration for this tutorial. &quot; ) print(f&quot;-------------- IMU calibration for {imu_label} ----------------&quot;) # Get IMU intrinsics parameters accel_bias = imu_calib.get_accel_model().get_bias() accel_rectification_matrix = imu_calib.get_accel_model().get_rectification() gyro_bias = imu_calib.get_gyro_model().get_bias() gyro_rectification_matrix = imu_calib.get_gyro_model().get_rectification() print(f&quot;Accelerometer Intrinsics:&quot;) print(f&quot; Bias: {accel_bias}&quot;) print(f&quot; Rectification Matrix:\\n{accel_rectification_matrix}&quot;) print(f&quot;Gyroscope Intrinsics:&quot;) print(f&quot; Bias: {gyro_bias}&quot;) print(f&quot; Rectification Matrix:\\n{gyro_rectification_matrix}&quot;) # Get extrinsics (device to IMU transformation) T_device_imu = imu_calib.get_transform_device_imu() print(f&quot;IMU Extrinsics T_Device_IMU:\\n{T_device_imu.to_matrix()}&quot;) print(f&quot; \\n ------IMPORTANT----- \\n &quot; f&quot;Please use .raw_to_rectified_[accel,gyro]() and .rectified_to_raw_[accel,gyro]() APIs to apply IMU calibration! \\n&quot;)   3. Magnetometer, barometer, and microphone calibration content‚Äã  print(f&quot;Magnetometers: {device_calib.get_magnetometer_labels()}&quot;) print(f&quot;Barometers: {device_calib.get_barometer_labels()}&quot;) print(f&quot;Microphones: {device_calib.get_microphone_labels()}&quot;) # ---------------- # Magnetometer calibration # ---------------- magnetometer_label = &quot;mag0&quot; magnetometer_calib = device_calib.get_magnetometer_calib(magnetometer_label) if magnetometer_calib is None: raise RuntimeError( f&quot;{magnetometer_label} calibration does not exist! Please use a VRS that contains valid magnetometer calibration for this tutorial.&quot; ) # Get magnetometer intrinsics parameters mag_bias = magnetometer_calib.get_model().get_bias() mag_rectification_matrix = magnetometer_calib.get_model().get_rectification() print(f&quot;Magnetometer calibration for {magnetometer_label} only have intrinsics:&quot;) print(f&quot; Bias: {mag_bias}&quot;) print(f&quot; Rectification Matrix:\\n{mag_rectification_matrix}&quot;) # ---------------- # Barometer calibration # ---------------- baro_label = &quot;baro0&quot; baro_calib = device_calib.get_barometer_calib(baro_label) if baro_calib is None: raise RuntimeError( f&quot;{baro_label} calibration does not exist! Please use a VRS that contains valid barometer calibration for this tutorial.&quot; ) print(f&quot;Barometer calibration for {baro_label} only have intrinsics:&quot;) print(f&quot; Slope: {baro_calib.get_slope()}&quot;) print(f&quot; Offset in Pascal:\\n{baro_calib.get_offset_pa()}&quot;) # ---------------- # Microphone calibration, containing both mic and speaker calibrations. # ---------------- microphone_labels = device_calib.get_microphone_labels() speaker_labels = device_calib.get_speaker_labels() audio_sensor_labels = device_calib.get_audio_labels() print(f&quot;Both mic and speakers are calibrated. \\n&quot; f&quot;List of mics that are calibrated: {microphone_labels} \\n&quot; f&quot;List of speakers that are calibrated: {speaker_labels}&quot;) for audio_label in audio_sensor_labels: audio_calib = device_calib.get_microphone_calib(audio_label) if audio_calib is None: print(f&quot;Audio sensor calibration for {audio_label} is not available.&quot;) continue print(f&quot;Audio sensor calibration for {audio_label} only has intrinsics:&quot;) print(f&quot; sensitivity delta: {audio_calib.get_d_sensitivity_1k_dbv()}&quot;) print(f&quot; \\n ------IMPORTANT----- \\n &quot; f&quot;Please use .raw_to_rectified() and .rectified_to_raw() APIs to apply magnetometer, barometer, and microphone calibration!\\n&quot;)   ","version":"Next","tagName":"h3"},{"title":"Camera Intrinsics: Project and Unproject‚Äã","type":1,"pageTitle":"Tutorial 2: Device calibration","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/calibration#camera-intrinsics-project-and-unproject","content":" A camera intrinsic model maps between a 3D point in the camera coordinate system and its corresponding 2D pixel on the sensor. This supports:  Projection: 3D point ‚Üí 2D pixelUnprojection: 2D pixel ‚Üí 3D ray  import numpy as np # Project 3D point to pixel point_3d = np.array([0.1, 0.05, 1.0]) # Point in camera frame (meters) pixel = camera_calib.project(point_3d) if pixel is not None: print(f&quot;3D point {point_3d} projected to -&gt; pixel {pixel}&quot;) else: print(f&quot;3D point {point_3d} projected out of camera sensor plane&quot;) # Unproject pixel to 3D ray. test_pixel = np.array([400, 300]) ray_3d = camera_calib.unproject(test_pixel) print(f&quot;Pixel {test_pixel} unprojected to -&gt; 3D ray {ray_3d}&quot;)   ","version":"Next","tagName":"h3"},{"title":"Camera Intrinsics: undistortion‚Äã","type":1,"pageTitle":"Tutorial 2: Device calibration","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/calibration#camera-intrinsics-undistortion","content":" Camera calibration enables post-processing of Aria images, such as undistorting images from a fisheye to a linear camera model. Steps:  Use vrs_data_provider to access the camera image and calibration.Create a linear camera model using get_linear_camera_calibration function.Apply distort_by_calibration to distort or undistort the image from the actual Fisheye camera model to linear camera model.  import rerun as rr from projectaria_tools.core import calibration rr.init(&quot;rerun_viz_image_undistortion&quot;) rr.notebook_show() # We already obtained RGB camera calibration as `camera_calib`. # Now, create a linear camera model that is similar to camera_calib linear_camera_model = calibration.get_linear_camera_calibration( image_width=camera_calib.get_image_size()[0], image_height=camera_calib.get_image_size()[1], focal_length=camera_calib.get_focal_lengths()[0], label=&quot;test_linear_camera&quot;, ) rgb_stream_id = vrs_data_provider.get_stream_id_from_label(&quot;camera-rgb&quot;) num_samples = vrs_data_provider.get_num_data(rgb_stream_id) # Plot a few frames from RGB camera, and also plot the undistorted images first_few = min(10, num_samples) for i in range(first_few): # Query RGB images image_data, image_record = vrs_data_provider.get_image_data_by_index( rgb_stream_id, i ) if not image_data.is_valid(): continue # Plot original RGB image timestamp_ns = image_record.capture_timestamp_ns rr.set_time_nanos(&quot;device_time&quot;, timestamp_ns) rr.log(&quot;camera_rgb&quot;, rr.Image(image_data.to_numpy_array())) # Undistort RGB image to a linear camera model undistorted_image = calibration.distort_by_calibration( arraySrc=image_data.to_numpy_array(), dstCalib=linear_camera_model, srcCalib=camera_calib, ) rr.log(&quot;undistorted_camera_rgb&quot;, rr.Image(undistorted_image))   ","version":"Next","tagName":"h2"},{"title":"IMU Intrinsics: Measurement Rectification‚Äã","type":1,"pageTitle":"Tutorial 2: Device calibration","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/calibration#imu-intrinsics-measurement-rectification","content":" IMU intrinsics are represented by an affine model. The raw sensor readout (value_raw) is compensated to obtain the real acceleration or angular velocity (value_compensated):  value_compensated = M^-1 * (value_raw - bias)   M is an upper triangular matrix (no global rotation between IMU body and accelerometer frame).  To simulate sensor readout from real values:  value_raw = M * value_compensated + bias   Note that in the following example, the difference between raw reading and compensated IMU signals are pretty close, therefore the plotting may look similar.  def _set_imu_plot_colors(rerun_plot_label): &quot;&quot;&quot; A helper function to set colors for the IMU plots in rerun &quot;&quot;&quot; rr.log( f&quot;{rerun_plot_label}/accl/x[m/sec2]&quot;, rr.SeriesLine(color=[230, 25, 75], name=&quot;accel/x[m/sec2]&quot;), static=True, ) # Red rr.log( f&quot;{rerun_plot_label}/accl/y[m/sec2]&quot;, rr.SeriesLine(color=[60, 180, 75], name=&quot;accel/y[m/sec2]&quot;), static=True, ) # Green rr.log( f&quot;{rerun_plot_label}/accl/z[m/sec2]&quot;, rr.SeriesLine(color=[0, 130, 200], name=&quot;accel/z[m/sec2]&quot;), static=True, ) # Blue rr.log( f&quot;{rerun_plot_label}/gyro/x[rad/sec2]&quot;, rr.SeriesLine(color=[245, 130, 48], name=&quot;gyro/x[rad/sec2]&quot;), static=True, ) # Orange rr.log( f&quot;{rerun_plot_label}/gyro/y[rad/sec2]&quot;, rr.SeriesLine(color=[145, 30, 180], name=&quot;gyro/y[rad/sec2]&quot;), static=True, ) # Purple rr.log( f&quot;{rerun_plot_label}/gyro/z[rad/sec2]&quot;, rr.SeriesLine(color=[70, 240, 240], name=&quot;gyro/z[rad/sec2]&quot;), static=True, ) # Cyan def _plot_imu_signals(accel_data, gyro_data, rerun_plot_label): &quot;&quot;&quot; This is a helper function to plot IMU signals in Rerun 1D plot &quot;&quot;&quot; rr.log( f&quot;{rerun_plot_label}/accl/x[m/sec2]&quot;, rr.Scalar(accel_data[0]), ) rr.log( f&quot;{rerun_plot_label}/accl/y[m/sec2]&quot;, rr.Scalar(accel_data[1]), ) rr.log( f&quot;{rerun_plot_label}/accl/z[m/sec2]&quot;, rr.Scalar(accel_data[2]), ) rr.log( f&quot;{rerun_plot_label}/gyro/x[rad/sec2]&quot;, rr.Scalar(gyro_data[0]), ) rr.log( f&quot;{rerun_plot_label}/gyro/y[rad/sec2]&quot;, rr.Scalar(gyro_data[1]), ) rr.log( f&quot;{rerun_plot_label}/gyro/z[rad/sec2]&quot;, rr.Scalar(gyro_data[2]), ) rr.init(&quot;rerun_viz_imu_rectification&quot;) rr.notebook_show() imu_label = &quot;imu-right&quot; imu_calib = device_calib.get_imu_calib(imu_label) imu_stream_id = vrs_data_provider.get_stream_id_from_label(imu_label) if imu_calib is None or imu_stream_id is None: raise RuntimeError( &quot;imu-right calibration or stream data does not exist! Please use a VRS that contains valid IMU calibration and data for this tutorial. &quot; ) num_samples = vrs_data_provider.get_num_data(imu_stream_id) first_few = min(5000, num_samples) # Set same colors for both plots _set_imu_plot_colors(&quot;imu_right&quot;) _set_imu_plot_colors(&quot;imu_right_compensated&quot;) for i in range(0, first_few, 50): # Query IMU data imu_data = vrs_data_provider.get_imu_data_by_index(imu_stream_id, i) # Plot raw IMU readings rr.set_time_nanos(&quot;device_time&quot;, imu_data.capture_timestamp_ns) # Get compensated imu data compensated_accel = imu_calib.raw_to_rectified_accel(imu_data.accel_msec2) compensated_gyro = imu_calib.raw_to_rectified_gyro(imu_data.gyro_radsec) # print one sample content if i == 0: print( f&quot;IMU compensation: raw accel {imu_data.accel_msec2} , compensated accel {compensated_accel}&quot; ) print( f&quot;IMU compensation: raw gyro {imu_data.gyro_radsec} , compensated gyro {compensated_gyro}&quot; ) # Plot raw IMU readings _plot_imu_signals(imu_data.accel_msec2, imu_data.gyro_radsec, &quot;imu_right&quot;) # Plot compensated IMU readings in a separate plot _plot_imu_signals(compensated_accel, compensated_gyro, &quot;imu_right_compensated&quot;)   ","version":"Next","tagName":"h3"},{"title":"5. Accessing Sensor Extrinsics‚Äã","type":1,"pageTitle":"Tutorial 2: Device calibration","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/calibration#5-accessing-sensor-extrinsics","content":" The core API to query sensor extrinsics is:  get_transform_device_sensor(label = sensor_label, use_cad_calib = False)   This API returns the extrinsics of the sensor, represented as a Sophus::SE3 (translation + rotation). in the reference coordinate frame of Device.  The Device frame is the reference coordinate system for all sensors.For Aria-Gen2, the &quot;Device&quot; frame is the left front-facing SLAM camera (slam-front-left).All sensor extrinsics are defined relative to this frame.  The optional parameter use_cad_calib controls the &quot;source&quot; of the sensor extrinsics.  use_cad_calib=False (default): this will return the sensor extrinsics from factory calibration, if the sensor's extrinsics is factory-calibrated. This include: CamerasIMUs use_cad_calib=True: this will return the sensor's extrinsics in their designed location in CAD. This is useful for sensors without factory-calibrated extrinsics, including: MagnetometerBarometerMicrophones  from projectaria_tools.utils.rerun_helpers import ( AriaGlassesOutline, ToTransform3D, ToBox3D, ) rr.init(&quot;rerun_viz_sensor_extrinsics&quot;) rr.notebook_show() # Obtain a glass outline for visualization. This outline uses factory calibration extrinsics if possible, uses CAD extrinsics if factory calibration is not available. glass_outline = AriaGlassesOutline(device_calib, use_cad_calib=False) rr.log(&quot;device/glasses_outline&quot;, rr.LineStrips3D([glass_outline]), static=True) # Plot all the sensor locations from either factory calibration (if available) or CAD sensor_labels = device_calib.get_all_labels() camera_labels = device_calib.get_camera_labels() for sensor in sensor_labels: # Query for sensor extrinsics from factory calibration if possible. Fall back to CAD values if unavailable. if (&quot;camera&quot; in sensor) or (&quot;imu&quot; in sensor): T_device_sensor = device_calib.get_transform_device_sensor(label = sensor, get_cad_value = False) else: T_device_sensor = device_calib.get_transform_device_sensor(label = sensor, get_cad_value = True) # Skip if extrinsics cannot be obtained if T_device_sensor is None: print(f&quot;Warning: sensor {sensor} does not have extrinsics from neither factory calibration nor CAD, skipping the plotting.&quot;) continue # Plot sensor labels rr.log(f&quot;device/{sensor}&quot;, ToTransform3D(T_device_sensor), static=True) rr.log( f&quot;device/{sensor}/text&quot;, ToBox3D(sensor, [1e-5, 1e-5, 1e-5]), static=True, ) # For cameras, also plot camera frustrum if sensor in camera_labels: camera_calibration = device_calib.get_camera_calib(sensor) rr.log(f&quot;device/{sensor}_frustum&quot;, ToTransform3D(T_device_sensor), static=True) rr.log( f&quot;device/{sensor}_frustum&quot;, rr.Pinhole( resolution=[ camera_calibration.get_image_size()[0], camera_calibration.get_image_size()[1], ], focal_length=float(camera_calibration.get_focal_lengths()[0]), ), static=True, )  ","version":"Next","tagName":"h3"},{"title":"Tutorial 4: Using On-Device Eye-tracking and Hand-tracking","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/eyetracking-handtracking","content":"","keywords":"","version":"Next"},{"title":"Introduction‚Äã","type":1,"pageTitle":"Tutorial 4: Using On-Device Eye-tracking and Hand-tracking","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/eyetracking-handtracking#introduction","content":" In Aria-Gen2 glasses, one of the key upgrade from Aria-Gen1 is the capability to run Machine Perception (MP) algorithms on the device during streaming / recording. Currently supported on-device MP algorithms include Eye-tracking, Hand-tracking, and VIO. These algorithm results are stored as separate data streams in the VRS file.  This tutorial focuses on demonstration of how to use the Eye-tracking and Hand-tracking results.  What you'll learn:  How to access on-device EyeGaze and HandTracking data from VRS filesUnderstanding the concept of interpolated hand tracking and why interpolation is neededHow to visualize EyeGaze and HandTracking data projected onto 2D camera images using DeviceCalibrationHow to match MP data with camera frames using timestamps  Prerequisites  Complete Tutorial 1 (VrsDataProvider Basics) to understand basic data provider conceptsComplete Tutorial 2 (Device Calibration) to understand how to properly use calibration in Aria data.  from projectaria_tools.core import data_provider # Load VRS file vrs_file_path = &quot;path/to/your/recording.vrs&quot; vrs_data_provider = data_provider.create_vrs_data_provider(vrs_file_path) # Access device calibration device_calib = vrs_data_provider.get_device_calibration()   # Query EyeGaze data streams eyegaze_label = &quot;eyegaze&quot; eyegaze_stream_id = vrs_data_provider.get_stream_id_from_label(eyegaze_label) if eyegaze_stream_id is None: raise RuntimeError( f&quot;{eyegaze_label} data stream does not exist! Please use a VRS that contains valid eyegaze data for this tutorial.&quot; ) # Query HandTracking data streams handtracking_label = &quot;handtracking&quot; handtracking_stream_id = vrs_data_provider.get_stream_id_from_label(handtracking_label) if handtracking_stream_id is None: raise RuntimeError( f&quot;{handtracking_label} data stream does not exist! Please use a VRS that contains valid handtracking data for this tutorial.&quot; )   ","version":"Next","tagName":"h2"},{"title":"On-Device Eye-tracking results‚Äã","type":1,"pageTitle":"Tutorial 4: Using On-Device Eye-tracking and Hand-tracking","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/eyetracking-handtracking#on-device-eye-tracking-results","content":" ","version":"Next","tagName":"h2"},{"title":"EyeGaze Data Structure‚Äã","type":1,"pageTitle":"Tutorial 4: Using On-Device Eye-tracking and Hand-tracking","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/eyetracking-handtracking#eyegaze-data-structure","content":" The EyeGaze data type represents on-device eye tracking results.Importantly, it directly reuses the EyeGaze data structurefrom MPS (Machine Perception Services), providing guaranteed compatibility across VRS and MPS.  Key EyeGaze fields  Field Name\tDescriptionsession_uid\tUnique ID for the eyetracking session tracking_timestamp\tTimestamp of the eye tracking camera frame in device time domain, in us. yaw\tGaze direction in yaw (horizontal) in radians pitch\tGaze direction in pitch (vertical) in radians depth\tEstimated gaze depth distance, in meters combined_gaze_origin_in_cpf\tCombined gaze origin in CPF frame (Gen2 only) spatial_gaze_point_in_cpf\t3D spatial gaze point in CPF frame vergence.[left,right]_entrance_pupil_position_meter\tEntrance pupil positions for each eye vergence.[left,right]_pupil_diameter_meter\tEntrance pupil diameter for each eye vergence.[left,right]_blink\tBlink detection for left and right eyes *_valid\tBoolean flags to indicating if the corresponding data field in EyeGaze is valid  ","version":"Next","tagName":"h3"},{"title":"EyeGaze API Reference‚Äã","type":1,"pageTitle":"Tutorial 4: Using On-Device Eye-tracking and Hand-tracking","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/eyetracking-handtracking#eyegaze-api-reference","content":" In vrs_data_provider, EyeGaze is treated the same way as any other sensor data, and share similar query APIs covered in Tutorial_1_vrs_data_provider_basics:  vrs_data_provider.get_eye_gaze_data_by_index(stream_id, index): Query by index.vrs_data_provider.get_eye_gaze_data_by_time_ns(stream_id, timestamp, time_domain, query_options): Query by timestamp.  from projectaria_tools.core.mps import get_unit_vector_from_yaw_pitch from datetime import timedelta print(&quot;=== EyeGaze Data Sample ===&quot;) num_eyegaze_samples = vrs_data_provider.get_num_data(eyegaze_stream_id) selected_index = min(5, num_eyegaze_samples) print(f&quot;Sample {selected_index}:&quot;) eyegaze_data = vrs_data_provider.get_eye_gaze_data_by_index(eyegaze_stream_id, selected_index) # Eyegaze timestamp is in format of datetime.deltatime in microseconds, convert it to integer eyegaze_timestamp_ns = (eyegaze_data.tracking_timestamp // timedelta(microseconds=1)) * 1000 print(f&quot;\\tTracking timestamp: {eyegaze_timestamp_ns}&quot;) # check if combined gaze is valid, if so, print out the gaze direction print(f&quot;\\tCombined gaze valid: {eyegaze_data.combined_gaze_valid}&quot;) if eyegaze_data.combined_gaze_valid: print(f&quot;\\tYaw: {eyegaze_data.yaw:.3f} rad&quot;) print(f&quot;\\tPitch: {eyegaze_data.pitch:.3f} rad&quot;) print(f&quot;\\tDepth: {eyegaze_data.depth:.3f} m&quot;) # Can also print gaze direction in unit vector gaze_direction_in_unit_vec = get_unit_vector_from_yaw_pitch(eyegaze_data.yaw, eyegaze_data.pitch) print(f&quot;\\tGaze direction in unit vec [xyz]: {gaze_direction_in_unit_vec}&quot;) # Check if spatial gaze point is valid, if so, print out the spatial gaze point print( f&quot;\\tSpatial gaze point valid: {eyegaze_data.spatial_gaze_point_valid}&quot; ) if eyegaze_data.spatial_gaze_point_valid: print( f&quot;\\tSpatial gaze point in CPF: {eyegaze_data.spatial_gaze_point_in_cpf}&quot; )   ","version":"Next","tagName":"h3"},{"title":"EyeGaze visualization in camera images‚Äã","type":1,"pageTitle":"Tutorial 4: Using On-Device Eye-tracking and Hand-tracking","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/eyetracking-handtracking#eyegaze-visualization-in-camera-images","content":" To visualize EyeGaze in camera images, you just need to project eye tracking results into the camera images using the camera's calibration. But please note the coordinate frame difference, entailed below.  EyeGaze Coordinate System - Central Pupil Frame (CPF)  All Eyetracking results in Aria are stored in a reference coordinates system called Central Pupil Frame (CPF), which is approximately the center of user's two eye positions. Note that this CPF frame is DIFFERENT from the Device frame in device calibration, where the latter is essentially the slam-front-left (for Gen2) or camera-slam-left (for Gen1) camera. To transform between CPF and Device, we provide the following API to query their relative pose, and see the following code cell for usage:  device_calibration.get_transform_device_cpf()   ","version":"Next","tagName":"h3"},{"title":"Visualizing Eye-tracking Data‚Äã","type":1,"pageTitle":"Tutorial 4: Using On-Device Eye-tracking and Hand-tracking","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/eyetracking-handtracking#visualizing-eye-tracking-data","content":" import rerun as rr import numpy as np from projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions def visualize_eyegaze_in_camera(camera_label, eyegaze_data, camera_calib, device_calib): &quot;&quot;&quot; Project eye-tracking data onto camera image &quot;&quot;&quot; # Convert gaze direction to 3D vector yaw, pitch = eyegaze_data.yaw, eyegaze_data.pitch gaze_vector_device = np.array([ np.cos(pitch) * np.sin(yaw), np.sin(pitch), np.cos(pitch) * np.cos(yaw) ]) # Transform to camera coordinate system T_device_camera = camera_calib.get_transform_device_camera() gaze_vector_camera = T_device_camera.inverse().rotationMatrix() @ gaze_vector_device # Project to image coordinates (assuming gaze origin at camera center) gaze_distance = 2.0 # meters gaze_point_camera = gaze_vector_camera * gaze_distance gaze_pixel = camera_calib.project(gaze_point_camera) # Visualize gaze point on image if camera_calib.is_valid_projection(gaze_pixel): rr.log( f&quot;{camera_label}/eyegaze&quot;, rr.Points2D( positions=[gaze_pixel], colors=[255, 0, 0], # Red color radii=[5.0] ) ) # Example usage in visualization loop rr.init(&quot;rerun_viz_eyegaze&quot;) rr.notebook_show() if eyegaze_stream_id is not None: rgb_stream_id = vrs_data_provider.get_stream_id_from_label(&quot;camera-rgb&quot;) rgb_camera_calib = device_calib.get_camera_calib(&quot;camera-rgb&quot;) # Visualize first few frames with eye-tracking data for i in range(min(10, num_eyegaze_samples)): eyegaze_data = vrs_data_provider.get_eyegaze_data_by_index(eyegaze_stream_id, i) # Find closest RGB frame eyegaze_timestamp_ns = int(eyegaze_data.tracking_timestamp.total_seconds() * 1e9) rgb_data, rgb_record = vrs_data_provider.get_image_data_by_time_ns( rgb_stream_id, eyegaze_timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST ) if rgb_data.is_valid(): rr.set_time_nanos(&quot;device_time&quot;, rgb_record.capture_timestamp_ns) rr.log(&quot;camera-rgb&quot;, rr.Image(rgb_data.to_numpy_array())) # Overlay eye-tracking data visualize_eyegaze_in_camera(&quot;camera-rgb&quot;, eyegaze_data, rgb_camera_calib, device_calib)   ","version":"Next","tagName":"h3"},{"title":"Accessing Hand-tracking Data‚Äã","type":1,"pageTitle":"Tutorial 4: Using On-Device Eye-tracking and Hand-tracking","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/eyetracking-handtracking#accessing-hand-tracking-data","content":" ","version":"Next","tagName":"h2"},{"title":"Basic Hand-tracking Data Access‚Äã","type":1,"pageTitle":"Tutorial 4: Using On-Device Eye-tracking and Hand-tracking","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/eyetracking-handtracking#basic-hand-tracking-data-access","content":" Hand-tracking provides 3D pose estimation for both hands, including joint positions and hand poses.  # Query HandTracking stream handtracking_stream_id = vrs_data_provider.get_stream_id_from_label(&quot;handtracking&quot;) if handtracking_stream_id is None: print(&quot;This VRS file does not contain on-device hand-tracking data.&quot;) else: print(f&quot;Found hand-tracking stream: {handtracking_stream_id}&quot;) # Get total number of hand-tracking samples num_handtracking_samples = vrs_data_provider.get_num_data(handtracking_stream_id) print(f&quot;Total hand-tracking samples: {num_handtracking_samples}&quot;) # Access hand-tracking data print(&quot;\\nFirst few hand-tracking samples:&quot;) for i in range(min(3, num_handtracking_samples)): hand_pose_data = vrs_data_provider.get_hand_pose_data_by_index(handtracking_stream_id, i) print(f&quot;\\nSample {i}:&quot;) print(f&quot;\\tTimestamp: {hand_pose_data.tracking_timestamp}&quot;) # Check left hand if hand_pose_data.left_hand is not None: print(f&quot;\\tLeft hand detected:&quot;) print(f&quot;\\tConfidence: {hand_pose_data.left_hand.confidence}&quot;) print(f&quot;\\tNumber of landmarks: {len(hand_pose_data.left_hand.landmark_positions_device)}&quot;) else: print(f&quot;\\tLeft hand: Not detected&quot;) # Check right hand if hand_pose_data.right_hand is not None: print(f&quot;\\tRight hand detected:&quot;) print(f&quot;\\tConfidence: {hand_pose_data.right_hand.confidence}&quot;) print(f&quot;\\tNumber of landmarks: {len(hand_pose_data.right_hand.landmark_positions_device)}&quot;) else: print(f&quot;\\tRight hand: Not detected&quot;)   ","version":"Next","tagName":"h3"},{"title":"Hand-tracking Data Structure‚Äã","type":1,"pageTitle":"Tutorial 4: Using On-Device Eye-tracking and Hand-tracking","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/eyetracking-handtracking#hand-tracking-data-structure","content":" Hand-tracking data contains:  Tracking Timestamp: When the hand-tracking measurement was takenLeft/Right Hand Data: Each hand (when detected) includes: Confidence: Detection confidence scoreLandmark Positions: 3D positions of hand joints in device coordinate systemWrist Transform: 6DOF pose of the wristPalm Normal: Normal vector of the palm  ","version":"Next","tagName":"h3"},{"title":"Interpolated Hand-tracking Data‚Äã","type":1,"pageTitle":"Tutorial 4: Using On-Device Eye-tracking and Hand-tracking","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/eyetracking-handtracking#interpolated-hand-tracking-data","content":" Since hand-tracking and camera data may not be perfectly synchronized, Aria provides interpolated hand-tracking data that can be queried at arbitrary timestamps.  from projectaria_tools.core.sensor_data import SensorDataType, TimeDomain, TimeQueryOptions from datetime import timedelta print(&quot;\\n=== Demonstrating query interpolated hand tracking results ===&quot;) # Demonstrate how to query interpolated handtracking results slam_stream_id = vrs_data_provider.get_stream_id_from_label(&quot;slam-front-left&quot;) rgb_stream_id = vrs_data_provider.get_stream_id_from_label(&quot;camera-rgb&quot;) # Retrieve a SLAM frame, use its timestamp as query slam_sample_index = min(10, vrs_data_provider.get_num_data(slam_stream_id) - 1) slam_data_and_record = vrs_data_provider.get_image_data_by_index(slam_stream_id, slam_sample_index) slam_timestamp_ns = slam_data_and_record[1].capture_timestamp_ns # Retrieve the closest RGB frame rgb_data_and_record = vrs_data_provider.get_image_data_by_time_ns( rgb_stream_id, slam_timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST ) rgb_timestamp_ns = rgb_data_and_record[1].capture_timestamp_ns # Retrieve the closest hand tracking data sample raw_ht_data = vrs_data_provider.get_hand_pose_data_by_time_ns( handtracking_stream_id, slam_timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST ) raw_ht_timestamp_ns = (raw_ht_data.tracking_timestamp // timedelta(microseconds=1)) * 1000 # Check if hand tracking aligns with RGB or SLAM data print(f&quot;SLAM timestamp: {slam_timestamp_ns}&quot;) print(f&quot;RGB timestamp: {rgb_timestamp_ns}&quot;) print(f&quot;hand tracking timestamp: {raw_ht_timestamp_ns}&quot;) print(f&quot;hand tracking-SLAM time diff: {abs(raw_ht_timestamp_ns - slam_timestamp_ns) / 1e6:.2f} ms&quot;) print(f&quot;hand tracking- RGB time diff: {abs(raw_ht_timestamp_ns - rgb_timestamp_ns) / 1e6:.2f} ms&quot;) # Now, query interpolated hand tracking data sample using RGB timestamp. interpolated_ht_data = vrs_data_provider.get_interpolated_hand_pose_data( handtracking_stream_id, rgb_timestamp_ns ) # Check that interpolated hand tracking now aligns with RGB data if interpolated_ht_data is not None: interpolated_ht_timestamp_ns = (interpolated_ht_data.tracking_timestamp// timedelta(microseconds=1)) * 1000 print(f&quot;Interpolated hand tracking timestamp: {interpolated_ht_timestamp_ns}&quot;) print(f&quot;Interpolated hand tracking-RGB time diff: {abs(interpolated_ht_timestamp_ns - rgb_timestamp_ns) / 1e6:.2f} ms&quot;) else: print(&quot;Interpolated hand tracking data is None - interpolation failed&quot;)   ","version":"Next","tagName":"h3"},{"title":"Visualizing Hand-tracking Results in Cameras‚Äã","type":1,"pageTitle":"Tutorial 4: Using On-Device Eye-tracking and Hand-tracking","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/eyetracking-handtracking#visualizing-hand-tracking-results-in-cameras","content":" import rerun as rr from projectaria_tools.utils.rerun_helpers import create_hand_skeleton_from_landmarks def plot_single_hand_in_camera(hand_joints_in_device, camera_label, camera_calib, hand_label): &quot;&quot;&quot; A helper function to plot a single hand data in 2D camera view &quot;&quot;&quot; # Setting different marker plot sizes for RGB and SLAM since they have different resolutions plot_ratio = 3.0 if camera_label == &quot;camera-rgb&quot; else 1.0 marker_color = [255,64,0] if hand_label == &quot;left&quot; else [255, 255, 0] # project into camera frame, and also create line segments hand_joints_in_camera = [] for pt_in_device in hand_joints_in_device: pt_in_camera = ( camera_calib.get_transform_device_camera().inverse() @ pt_in_device ) pixel = camera_calib.project(pt_in_camera) hand_joints_in_camera.append(pixel) # Create hand skeleton in 2D image space hand_skeleton = create_hand_skeleton_from_landmarks(hand_joints_in_camera) # Remove &quot;None&quot; markers from hand joints in camera. This is intentionally done AFTER the hand skeleton creation hand_joints_in_camera = list( filter(lambda x: x is not None, hand_joints_in_camera) ) rr.log( f&quot;{camera_label}/{hand_label}/landmarks&quot;, rr.Points2D( positions=hand_joints_in_camera, colors= marker_color, radii= [3.0 * plot_ratio] ), ) rr.log( f&quot;{camera_label}/{hand_label}/skeleton&quot;, rr.LineStrips2D( hand_skeleton, colors=[0, 255, 0], radii= [0.5 * plot_ratio], ), ) def plot_handpose_in_camera(hand_pose, camera_label, camera_calib): &quot;&quot;&quot; A helper function to plot hand tracking results into a camera image &quot;&quot;&quot; # Plot both hands if hand_pose.left_hand is not None: plot_single_hand_in_camera( hand_joints_in_device=hand_pose.left_hand.landmark_positions_device, camera_label=camera_label, camera_calib = camera_calib, hand_label=&quot;left&quot;) if hand_pose.right_hand is not None: plot_single_hand_in_camera( hand_joints_in_device=hand_pose.right_hand.landmark_positions_device, camera_label=camera_label, camera_calib = camera_calib, hand_label=&quot;right&quot;) print(&quot;\\n=== Visualizing on-device hand tracking in camera images ===&quot;) # First, query the RGB camera stream id device_calib = vrs_data_provider.get_device_calibration() rgb_camera_label = &quot;camera-rgb&quot; slam_camera_labels = [&quot;slam-front-left&quot;, &quot;slam-front-right&quot;, &quot;slam-side-left&quot;, &quot;slam-side-right&quot;] rgb_stream_id = vrs_data_provider.get_stream_id_from_label(rgb_camera_label) slam_stream_ids = [vrs_data_provider.get_stream_id_from_label(label) for label in slam_camera_labels] rr.init(&quot;rerun_viz_ht_in_cameras&quot;) rr.notebook_show() # Set up a sensor queue with only RGB images. # Handtracking data will be queried with interpolated API. deliver_options = vrs_data_provider.get_default_deliver_queued_options() deliver_options.deactivate_stream_all() for stream_id in slam_stream_ids + [rgb_stream_id]: deliver_options.activate_stream(stream_id) # Play for only 3 seconds total_length_ns = vrs_data_provider.get_last_time_ns_all_streams(TimeDomain.DEVICE_TIME) - vrs_data_provider.get_first_time_ns_all_streams(TimeDomain.DEVICE_TIME) skip_begin_ns = int(15 * 1e9) # Skip 15 seconds duration_ns = int(3 * 1e9) # 3 seconds skip_end_ns = max(total_length_ns - skip_begin_ns - duration_ns, 0) deliver_options.set_truncate_first_device_time_ns(skip_begin_ns) deliver_options.set_truncate_last_device_time_ns(skip_end_ns) # Plot image data, and overlay hand tracking data for sensor_data in vrs_data_provider.deliver_queued_sensor_data(deliver_options): # -- # Only image data will be obtained. # -- device_time_ns = sensor_data.get_time_ns(TimeDomain.DEVICE_TIME) image_data_and_record = sensor_data.image_data_and_record() stream_id = sensor_data.stream_id() camera_label = vrs_data_provider.get_label_from_stream_id(stream_id) camera_calib = device_calib.get_camera_calib(camera_label) # Visualize the RGB images. rr.set_time_nanos(&quot;device_time&quot;, device_time_ns) rr.log(f&quot;{camera_label}&quot;, rr.Image(image_data_and_record[0].to_numpy_array())) # Query and plot interpolated hand tracking result interpolated_hand_pose = vrs_data_provider.get_interpolated_hand_pose_data(handtracking_stream_id, device_time_ns, TimeDomain.DEVICE_TIME) if interpolated_hand_pose is not None: plot_handpose_in_camera(hand_pose = interpolated_hand_pose, camera_label = camera_label, camera_calib = camera_calib) # Wait for rerun to buffer 1 second of data import time time.sleep(1)   ","version":"Next","tagName":"h3"},{"title":"Understanding Interpolation‚Äã","type":1,"pageTitle":"Tutorial 4: Using On-Device Eye-tracking and Hand-tracking","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/eyetracking-handtracking#understanding-interpolation","content":" Hand-tracking interpolation is crucial for synchronizing hand data with camera frames:  Why Interpolation is Needed: Hand-tracking algorithms may run at different frequencies than cameras, leading to temporal misalignment. Interpolation Algorithm: The system uses linear interpolation for 3D positions and SE3 interpolation for poses. Interpolation Rules: Both hands must be valid in both before/after samples for interpolation to workIf either hand is missing in either sample, the interpolated result for that hand will be NoneSingle-hand interpolation includes: Linear interpolation on 3D hand landmark positionsSE3 interpolation on wrist 3D poseRe-calculated wrist and palm normal vectorsMinimum confidence values  ","version":"Next","tagName":"h2"},{"title":"Summary‚Äã","type":1,"pageTitle":"Tutorial 4: Using On-Device Eye-tracking and Hand-tracking","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/eyetracking-handtracking#summary","content":" This tutorial covered accessing and visualizing on-device eye-tracking and hand-tracking data:  Eye-tracking Data: Access gaze direction information and project onto camera imagesHand-tracking Data: Access 3D hand pose data including joint positions and confidence scoresInterpolated Data: Use interpolated hand-tracking for better temporal alignment with camera dataVisualization: Project MP data onto 2D camera images for analysis and debugging  These on-device MP algorithms provide real-time insights into user behavior and can be combined with other sensor data for comprehensive analysis of user interactions and movements. ","version":"Next","tagName":"h2"},{"title":"Tutorial 7: Loading and Visualizing MPS Output Data","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps","content":"","keywords":"","version":"Next"},{"title":"Introduction‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#introduction","content":" This tutorial demonstrates how to access and visualize Machine Perception Services (MPS) results. MPS provides cloud-based processing of Aria data to generate high-quality 3D reconstruction, SLAM trajectories, and other perception outputs.  What you'll learn:  How to load and access MPS SLAM trajectory data (open loop and closed loop)How to load and visualize MPS semi-dense point clouds and observationsHow to create 3D visualizations of MPS SLAM results  Prerequisites  Complete Tutorial 1 (VrsDataProvider Basics) to understand basic data provider conceptsAccess to MPS processed data (SLAM outputs)  import os from projectaria_tools.core import mps # Set up paths to your MPS data mps_folder_path = &quot;path/to/your/mps/folder/&quot; vrs_file_path = &quot;path/to/your/recording.vrs&quot; # Load VRS data provider for additional context vrs_data_provider = data_provider.create_vrs_data_provider(vrs_file_path)   ","version":"Next","tagName":"h2"},{"title":"MPS SLAM Trajectories‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#mps-slam-trajectories","content":" ","version":"Next","tagName":"h2"},{"title":"Understanding Open Loop vs Closed Loop Trajectories‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#understanding-open-loop-vs-closed-loop-trajectories","content":" MPS SLAM algorithm outputs 2 trajectory files (see wiki page for data type definitions):  Open loop trajectory: High-frequency (1kHz) odometry from visual-inertial odometry (VIO), accurate over short periods but drifts over time and distance.Closed loop trajectory: High-frequency (1kHz) pose from mapping with loop closure corrections, reducing drift but possibly less accurate locally over short spans.  ","version":"Next","tagName":"h3"},{"title":"Loading Closed Loop Trajectory‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#loading-closed-loop-trajectory","content":" from projectaria_tools.core.mps.utils import ( filter_points_from_confidence, get_nearest_pose, ) print(&quot;=== MPS - Closed loop trajectory ===&quot;) # Load MPS closed-loop trajectory data closed_loop_trajectory_file = os.path.join( mps_folder_path, &quot;slam&quot;, &quot;closed_loop_trajectory.csv&quot; ) closed_loop_trajectory = mps.read_closed_loop_trajectory(closed_loop_trajectory_file) # Print out the content of the first sample in closed_loop_trajectory if closed_loop_trajectory: sample = closed_loop_trajectory[0] print(&quot;ClosedLoopTrajectoryPose sample:&quot;) print(f&quot; tracking_timestamp: {int(sample.tracking_timestamp.total_seconds() * 1e6)} us&quot;) print(f&quot; utc_timestamp: {int(sample.utc_timestamp.total_seconds() * 1e6)} us&quot;) print(f&quot; transform_world_device:\\n{sample.transform_world_device}&quot;) print(f&quot; device_linear_velocity_device: {sample.device_linear_velocity_device}&quot;) print(f&quot; angular_velocity_device: {sample.angular_velocity_device}&quot;) print(f&quot; quality_score: {sample.quality_score}&quot;) print(f&quot; gravity_world: {sample.gravity_world}&quot;) print(f&quot; graph_uid: {sample.graph_uid}&quot;) else: print(&quot;closed_loop_trajectory is empty.&quot;)   ","version":"Next","tagName":"h3"},{"title":"Loading Open Loop Trajectory‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#loading-open-loop-trajectory","content":" print(&quot;=== MPS - Open loop trajectory ===&quot;) # Load MPS open-loop trajectory data open_loop_trajectory_file = os.path.join( mps_folder_path, &quot;slam&quot;, &quot;open_loop_trajectory.csv&quot; ) open_loop_trajectory = mps.read_open_loop_trajectory(open_loop_trajectory_file) # Print out the content of the first sample in open_loop_trajectory if open_loop_trajectory: sample = open_loop_trajectory[0] print(&quot;OpenLoopTrajectoryPose sample:&quot;) print(f&quot; tracking_timestamp: {int(sample.tracking_timestamp.total_seconds() * 1e6)} us&quot;) print(f&quot; utc_timestamp: {int(sample.utc_timestamp.total_seconds() * 1e6)} us&quot;) print(f&quot; transform_odometry_device:\\n{sample.transform_odometry_device}&quot;) print(f&quot; device_linear_velocity_odometry: {sample.device_linear_velocity_odometry}&quot;) print(f&quot; angular_velocity_device: {sample.angular_velocity_device}&quot;) print(f&quot; quality_score: {sample.quality_score}&quot;) print(f&quot; gravity_odometry: {sample.gravity_odometry}&quot;) print(f&quot; session_uid: {sample.session_uid}&quot;) else: print(&quot;open_loop_trajectory is empty.&quot;)   ","version":"Next","tagName":"h3"},{"title":"MPS Semi-dense Point Cloud and Observations‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#mps-semi-dense-point-cloud-and-observations","content":" ","version":"Next","tagName":"h2"},{"title":"Understanding Point Cloud Data‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#understanding-point-cloud-data","content":" MPS SLAM algorithm outputs 2 files related to semi-dense point cloud (see wiki page for data type definitions):  semidense_points.csv.gz: Global points in the world coordinate frame.semidense_observations.csv.gz: Point observations for each camera, at each timestamp.  Note that semidense point files are normally large, therefore loading them may take some time.  ","version":"Next","tagName":"h3"},{"title":"Loading Semi-dense Point Cloud‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#loading-semi-dense-point-cloud","content":" print(&quot;=== MPS - Semi-dense Point Cloud ===&quot;) # Load MPS semi-dense point cloud data semidense_points_file = os.path.join( mps_folder_path, &quot;slam&quot;, &quot;semidense_points.csv.gz&quot; ) semidense_points = mps.read_global_point_cloud(semidense_points_file) # Print out the content of the first sample in semidense_points if semidense_points: sample = semidense_points[0] print(&quot;GlobalPointPosition sample:&quot;) print(f&quot; uid: {sample.uid}&quot;) print(f&quot; graph_uid: {sample.graph_uid}&quot;) print(f&quot; position_world: {sample.position_world}&quot;) print(f&quot; inverse_distance_std: {sample.inverse_distance_std}&quot;) print(f&quot; distance_std: {sample.distance_std}&quot;) print(f&quot;Total number of semi-dense points: {len(semidense_points)}&quot;) else: print(&quot;semidense_points is empty.&quot;) # Filter semidense points by inv_dep or depth. # The filter will KEEP points with (inv_dep or depth &lt; threshold) filtered_semidense_points = filter_points_from_confidence(raw_points = semidense_points, threshold_invdep = 1e-3, threshold_dep = 5e-2) print(f&quot;Filtering semidense points from a total of {len(semidense_points)} points down to {len(filtered_semidense_points)}&quot;)   ","version":"Next","tagName":"h3"},{"title":"Loading Point Observations‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#loading-point-observations","content":" print(&quot;=== MPS - Semi-dense Point Observations ===&quot;) # Load MPS semi-dense point observations data semidense_observations_file = os.path.join( mps_folder_path, &quot;slam&quot;, &quot;semidense_observations.csv.gz&quot; ) semidense_observations = mps.read_point_observations(semidense_observations_file) # Print out the content of the first sample in semidense_observations if semidense_observations: sample = semidense_observations[0] print(&quot;PointObservation sample:&quot;) print(f&quot; point_uid: {sample.point_uid}&quot;) print(f&quot; frame_capture_timestamp: {int(sample.frame_capture_timestamp.total_seconds() * 1e6)} us&quot;) print(f&quot; camera_serial: {sample.camera_serial}&quot;) print(f&quot; uv: {sample.uv}&quot;) print(f&quot;Total number of point observations: {len(semidense_observations)}&quot;) else: print(&quot;semidense_observations is empty.&quot;)   ","version":"Next","tagName":"h3"},{"title":"Visualizing MPS SLAM Results‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#visualizing-mps-slam-results","content":" In the following code snippet, we demonstrate how to visualize the MPS SLAM results in a 3D view.  We first prepare a short trajectory segment, then extract the semidense points position, along with timestamp-mapped observations for visualization purpose. Finally we plot everything in Rerun.  ","version":"Next","tagName":"h2"},{"title":"Color Mapping Helper Function‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#color-mapping-helper-function","content":" from collections import defaultdict import numpy as np # A helper coloring function def color_from_zdepth(z_depth_m: float) -&gt; np.ndarray: &quot;&quot;&quot; Map z-depth (meters, along the camera's forward axis) to a bright Viridis-like RGB color. - If z_depth_m &lt;= 0 (point is behind the camera), return black [0, 0, 0]. - Near (0.2 m) -&gt; yellow, Far (5.0 m) -&gt; purple. Returns an array of shape (3,) with dtype=uint8. &quot;&quot;&quot; if not np.isfinite(z_depth_m) or z_depth_m &lt;= 0.0: return np.array([0, 0, 0], dtype=np.uint8) NEAR_METERS, FAR_METERS = 0.2, 5.0 # Normalize to [0,1], then flip so near ‚Üí bright (yellow), far ‚Üí dark (purple) clamped = min(max(float(z_depth_m), NEAR_METERS), FAR_METERS) normalized_position = (clamped - NEAR_METERS) / (FAR_METERS - NEAR_METERS + 1e-12) gradient_position = 1.0 - normalized_position # Viridis-like anchor colors: purple ‚Üí blue ‚Üí teal ‚Üí green ‚Üí yellow color_stops = [ (68, 1, 84), (59, 82, 139), (33, 145, 140), (94, 201, 98), (253, 231, 37), ] # Locate segment and blend between its endpoints segment_count = len(color_stops) - 1 continuous_index = gradient_position * segment_count lower_segment_index = int(continuous_index) if lower_segment_index &gt;= segment_count: red, green, blue = color_stops[-1] else: segment_fraction = continuous_index - lower_segment_index r0, g0, b0 = color_stops[lower_segment_index] r1, g1, b1 = color_stops[lower_segment_index + 1] red = r0 + segment_fraction * (r1 - r0) green = g0 + segment_fraction * (g1 - g0) blue = b0 + segment_fraction * (b1 - b0) return np.array([int(red), int(green), int(blue)], dtype=np.uint8)   ","version":"Next","tagName":"h3"},{"title":"Preparing Data for Visualization‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#preparing-data-for-visualization","content":" print(&quot;=== Preparing MPS SLAM results for visualization ===&quot;) # Check if we have valid SLAM data to visualize if not closed_loop_trajectory or not semidense_points: raise RuntimeError(&quot;Warning: This tutorial requires valid MPS SLAM data to run.&quot;) # -- # Prepare Trajectory data # -- # Select a short segment of trajectory (e.g., first 5000 samples, subsampled by 50) segment_length = min(50000, len(closed_loop_trajectory)) trajectory_segment = closed_loop_trajectory[:segment_length:50] timestamp_to_pose = { pose.tracking_timestamp: pose for pose in trajectory_segment } print(f&quot;Finished preparing a trajectory of length {len(trajectory_segment)}... &quot;) # ----------- # Prepare Semidense point data # ----------- # Filter the semidense point cloud by confidence and limit max point count, and extract the point positions filtered_semidense_point_cloud_data = filter_points_from_confidence(semidense_points) points_positions = np.array( [ point.position_world for point in filtered_semidense_point_cloud_data ] ) print(f&quot;Finished preparing filtered semidense points cloud of {len(filtered_semidense_point_cloud_data)} points... &quot;) # ----------- # Prepare Semidense observation data # ----------- # Based on RGB observations, create a per-timestamp point position list, and color them according to its distance from RGB camera point_uid_to_position = { point.uid: np.array(point.position_world) for point in filtered_semidense_point_cloud_data } # A helper function that creates a easier-to-query mapping to obtain observations according to timestamps slam_1_serial = vrs_data_provider.get_device_calibration().get_camera_calib(&quot;slam-front-left&quot;).get_serial_number() timestamp_to_point_positions = defaultdict(list) # t_ns -&gt; [position, position, ...] timestamp_to_point_colors = defaultdict(list) # t_ns -&gt; [color, color, ...] for obs in semidense_observations: # Only add observations for SLAM_1 camera, and if the timestamp is in the chosen trajectory segment if ( obs.camera_serial == slam_1_serial and obs.frame_capture_timestamp in timestamp_to_pose and obs.point_uid in point_uid_to_position): # Insert point position obs_timestamp = obs.frame_capture_timestamp point_position = point_uid_to_position[obs.point_uid] timestamp_to_point_positions[obs_timestamp].append(point_position) # Insert point color T_world_device = timestamp_to_pose[obs_timestamp].transform_world_device point_in_device = T_world_device.inverse() @ point_position point_z_depth = point_in_device.squeeze()[2] point_color = color_from_zdepth(point_z_depth) timestamp_to_point_colors[obs_timestamp].append(point_color) from itertools import islice print(f&quot;Finished preparing semidense points observations: &quot;) for timestamp, points in islice(timestamp_to_point_positions.items(), 5): print(f&quot;\\t timestamp {int(timestamp.total_seconds() * 1e9)} ns has {len(points)} observed points in slam-front-left view. &quot;) print(f&quot;\\t ...&quot;)   ","version":"Next","tagName":"h3"},{"title":"3D Visualization with Rerun‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#3d-visualization-with-rerun","content":" import rerun as rr import numpy as np from projectaria_tools.utils.rerun_helpers import ( AriaGlassesOutline, ToTransform3D, ToBox3D, ) from projectaria_tools.core.mps.utils import ( filter_points_from_confidence, get_nearest_pose, ) print(&quot;=== Visualizing MPS SLAM Results in 3D ===&quot;) # Initialize Rerun rr.init(&quot;MPS SLAM Visualization&quot;) rr.notebook_show() # Set up the 3D scene rr.log(&quot;world&quot;, rr.ViewCoordinates.RIGHT_HAND_Z_UP, static=True) # Log point cloud rr.log( &quot;world/semidense_points&quot;, rr.Points3D( positions=points_positions, colors=[255, 255, 255, 125], radii=0.001 ), static=True ) # Aria glass outline for visualization purpose device_calib = vrs_data_provider.get_device_calibration() aria_glasses_point_outline = AriaGlassesOutline( device_calib, use_cad_calib=True ) # Plot Closed loop trajectory closed_loop_traj_cached_full = [] observation_points_cached = None observation_colors_cached = None for closed_loop_pose in trajectory_segment: capture_timestamp_ns = int(closed_loop_pose.tracking_timestamp.total_seconds() * 1e9) rr.set_time_nanos(&quot;device_time&quot;, capture_timestamp_ns) T_world_device = closed_loop_pose.transform_world_device # Log device pose as a coordinate frame rr.log( &quot;world/device&quot;, ToTransform3D( T_world_device, axis_length=0.05, ), ) # Plot Aria glass outline rr.log( &quot;world/device/glasses_outline&quot;, rr.LineStrips3D( aria_glasses_point_outline, colors=[150,200,40], radii=5e-3, ), ) # Plot gravity direction vector rr.log( &quot;world/vio_gravity&quot;, rr.Arrows3D( origins=[T_world_device.translation()[0]], vectors=[ closed_loop_pose.gravity_world * 1e-2 ], # length converted from 9.8 meter -&gt; 10 cm colors=[101,67,33], radii=5e-3, ), static=False, ) # Update cached results for observations. Cache is needed because observation has a much lower freq than high-freq trajectory. if closed_loop_pose.tracking_timestamp in timestamp_to_point_positions.keys(): observation_points_cached = timestamp_to_point_positions[closed_loop_pose.tracking_timestamp] observation_colors_cached = timestamp_to_point_colors[closed_loop_pose.tracking_timestamp] if observation_points_cached is not None: rr.log( &quot;world/semidense_observations&quot;, rr.Points3D( positions = observation_points_cached, colors = observation_colors_cached, radii=0.01 ), static = False ) # Plot the entire VIO trajectory that are cached so far closed_loop_traj_cached_full.append(T_world_device.translation()[0]) rr.log( &quot;world/vio_trajectory&quot;, rr.LineStrips3D( closed_loop_traj_cached_full, colors=[173, 216, 255], radii=5e-3, ), static=False, )   ","version":"Next","tagName":"h3"},{"title":"Understanding MPS Data Structures‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#understanding-mps-data-structures","content":" ","version":"Next","tagName":"h2"},{"title":"Trajectory Data Types‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#trajectory-data-types","content":" ClosedLoopTrajectoryPose‚Äã  tracking_timestamp: Device timestamp when pose was computedutc_timestamp: UTC timestamptransform_world_device: 6DOF pose in world coordinate framedevice_linear_velocity_device: Linear velocity in device frameangular_velocity_device: Angular velocity in device framequality_score: Pose estimation quality (higher = better)gravity_world: Gravity vector in world framegraph_uid: Unique identifier for the pose graph  OpenLoopTrajectoryPose‚Äã  tracking_timestamp: Device timestamp when pose was computedutc_timestamp: UTC timestamptransform_odometry_device: 6DOF pose in odometry coordinate framedevice_linear_velocity_odometry: Linear velocity in odometry frameangular_velocity_device: Angular velocity in device framequality_score: Pose estimation quality (higher = better)gravity_odometry: Gravity vector in odometry framesession_uid: Unique identifier for the session  ","version":"Next","tagName":"h3"},{"title":"Point Cloud Data Types‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#point-cloud-data-types","content":" GlobalPointPosition‚Äã  uid: Unique identifier for the 3D pointgraph_uid: Identifier linking point to pose graphposition_world: 3D position in world coordinate frameinverse_distance_std: Inverse distance standard deviation (quality metric)distance_std: Distance standard deviation (quality metric)  PointObservation‚Äã  point_uid: Links observation to 3D pointframe_capture_timestamp: When the observation was capturedcamera_serial: Serial number of the observing camerauv: 2D pixel coordinates of the observation  ","version":"Next","tagName":"h3"},{"title":"MPS vs On-Device Comparisons‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#mps-vs-on-device-comparisons","content":" ","version":"Next","tagName":"h2"},{"title":"Key Differences‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#key-differences","content":" Aspect\tOn-Device (VIO/SLAM)\tMPS SLAMProcessing\tReal-time during recording\tCloud-based post-processing Accuracy\tGood for real-time use\tHigher accuracy with global optimization Frequency\t20Hz (VIO), 800Hz (high-freq)\t1kHz (both open/closed loop) Drift\tAccumulates over time\tMinimized with loop closure Point Cloud\tNot available\tDense semi-dense reconstructions Coordinate Frame\tOdometry frame\tGlobal world frame  ","version":"Next","tagName":"h3"},{"title":"Use Cases‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#use-cases","content":" On-Device Data: Real-time applications, live feedback, immediate processingMPS Data: High-quality reconstruction, research analysis, detailed mapping  ","version":"Next","tagName":"h3"},{"title":"Summary‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#summary","content":" This tutorial covered the essential aspects of working with MPS data:  Trajectory Access: Loading both open loop and closed loop trajectoriesPoint Cloud Data: Accessing semi-dense 3D reconstructions and observationsData Filtering: Using confidence thresholds to improve point cloud quality3D Visualization: Creating comprehensive visualizations with trajectories and point cloudsData Structures: Understanding the comprehensive MPS data formats  MPS provides high-quality, globally consistent 3D reconstructions that are ideal for:  Research Applications: Detailed spatial analysis and mapping3D Reconstruction: High-fidelity environmental modelingMotion Analysis: Accurate trajectory analysis without driftMulti-modal Studies: Combining precise 3D data with sensor informationBenchmarking: Comparing against ground truth for algorithm development ","version":"Next","tagName":"h2"},{"title":"Tutorial 3: Sequential Accessing Multi-sensor Data","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/queue","content":"","keywords":"","version":"Next"},{"title":"Introduction‚Äã","type":1,"pageTitle":"Tutorial 3: Sequential Accessing Multi-sensor Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/queue#introduction","content":" This tutorial shows how to use the unified queued API in vrs_data_provider to efficiently stream multi-sensor data from Aria VRS files.  We will learn how to use the unified SensorData interface, access time-ordered sensor data queues, and customize stream control and time windowing for efficient processing.  In this tutorial, we will learn:  Use the basic queued API to iterate through all sensor data.Explore the unified SensorData interfaceCustomize stream selection and time windowing in this queued API.Apply frame rate subsampling for efficient processing  Prerequisites  Complete Tutorial 1 (VrsDataProvider Basics) to understand basic data provider concepts  from projectaria_tools.core import data_provider # Load local VRS file vrs_file_path = &quot;path/to/your/recording.vrs&quot; vrs_data_provider = data_provider.create_vrs_data_provider(vrs_file_path)   ","version":"Next","tagName":"h2"},{"title":"Basic Sequential Data Access API‚Äã","type":1,"pageTitle":"Tutorial 3: Sequential Accessing Multi-sensor Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/queue#basic-sequential-data-access-api","content":" The deliver_queued_sensor_data() method in vrs_data_provider provides a unified way to iterate through all sensor data in timestamp order. This is the primary API for sequential access to multi-sensor data.  Key features of the queued API:  Returns data from ALL streams in the VRS fileOrders data by device timestamp (chronological order)Customizable via stream selection, sub-sampling each stream, etc.The returned data can be further converted to each sensor type via a unified interface.  Here is a simple example to query the first K data samples from the VRS, and inspect each data sample's properties:  print(&quot;\\n=== Basic Sequential Data Access ===&quot;) print(&quot;Processing all sensor data in timestamp order...&quot;) # Variables to store how many data samples has arrived for each sensor stream data_count = 0 per_stream_data_counts = {} total_num_samples = 5000 # Call deliver queued sensor data API to obtain a &quot;streamed&quot; data, in sorted timestamp order # The iterator would return a unified SensorData instance print(f&quot;Start inspecting the first {total_num_samples} data samples in the VRS&quot;) for sensor_data in vrs_data_provider.deliver_queued_sensor_data(): # Which stream does this sensor data belong to stream_id = sensor_data.stream_id() stream_label = vrs_data_provider.get_label_from_stream_id(stream_id) # Aggregate data count for this stream data_count += 1 if stream_label not in per_stream_data_counts: per_stream_data_counts[stream_label] = 0 per_stream_data_counts[stream_label] += 1 # Limit output for demonstration if data_count &gt;= total_num_samples: print(&quot;Stopping after 5000 samples for demonstration...&quot;) break # Print data counts for each sensor stream print(f&quot;\\nTotal processed: {data_count} sensor data samples&quot;) print(&quot;Data count per stream:&quot;) for stream_label, count in per_stream_data_counts.items(): print(f&quot;\\t{stream_label}: {count}&quot;)   ","version":"Next","tagName":"h2"},{"title":"Understanding the Unified SensorData Interface‚Äã","type":1,"pageTitle":"Tutorial 3: Sequential Accessing Multi-sensor Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/queue#understanding-the-unified-sensordata-interface","content":" The queued API returns data using a unified SensorData interface. This allows you to handle different types of sensor data (images, IMU, audio, etc.) in a consistent way, regardless of the sensor type.  Each SensorData object provides:  Stream ID and stream label for identificationSensor data type (IMAGE, IMU, AUDIO, etc.)Timestamps in different time domainsAccess to the actual sensor data (images, IMU, audio, etc.)  from projectaria_tools.core.sensor_data import SensorDataType, TimeDomain, TimeQueryOptions print(&quot;\\n=== Exploring the SensorData Interface ===&quot;) # Get a few samples to examine their properties data_count = 0 for sensor_data in vrs_data_provider.deliver_queued_sensor_data(): if data_count &gt;= 5: break # Inspect where this sensor data come from, and what is its data type stream_id = sensor_data.stream_id() stream_label = vrs_data_provider.get_label_from_stream_id(stream_id) data_type = sensor_data.sensor_data_type() # Inspect the device timestamp of this sensor data device_time = sensor_data.get_time_ns(TimeDomain.DEVICE_TIME) print(f&quot;\\nSample {data_count + 1}:&quot;) print(f&quot; Stream: {stream_label} (Stream ID: {stream_id})&quot;) print(f&quot; Type: {data_type}&quot;) print(f&quot; Device Time: {device_time/1e9:.6f}s&quot;) # Map sensor data to its specific type, and inspect its actual data content. # Here we use image and IMU as an example if data_type == SensorDataType.IMAGE: image_data = sensor_data.image_data_and_record()[0] print(f&quot; Image size: {image_data.get_width()} x {image_data.get_height()}&quot;) print(f&quot; Pixel format: {image_data.get_pixel_format()}&quot;) elif data_type == SensorDataType.IMU: imu_data = sensor_data.imu_data() accel = imu_data.accel_msec2 gyro = imu_data.gyro_radsec print(f&quot; IMU Accel: [{accel[0]:.3f}, {accel[1]:.3f}, {accel[2]:.3f}] m/s¬≤&quot;) print(f&quot; IMU Gyro: [{gyro[0]:.3f}, {gyro[1]:.3f}, {gyro[2]:.3f}] rad/s&quot;) data_count += 1   ","version":"Next","tagName":"h2"},{"title":"Customizing Data Access with DeliverQueuedOptions‚Äã","type":1,"pageTitle":"Tutorial 3: Sequential Accessing Multi-sensor Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/queue#customizing-data-access-with-deliverqueuedoptions","content":" The real power of the queued API comes from customization options. The DeliverQueuedOptions class allows you to:  Apply time windowing - Process only specific time ranges.Select specific streams - Choose which sensors to include.Subsample data - Reduce frame rates for specific streams.  These all provide flexible ways to control the sensor queue, to focus on specific time periods or sensor modalities, or customize data rates for different analysis needs.  import rerun as rr print(&quot;\\n=== Customizing Data Access with DeliverQueuedOptions ===&quot;) customized_deliver_options = vrs_data_provider.get_default_deliver_queued_options() # ----------------- # 1. Stream selection feature - only select RGB, 1 SLAM camera, and 1 ET camera data. # ----------------- rgb_to_select = vrs_data_provider.get_stream_id_from_label(&quot;camera-rgb&quot;) slam_to_select = vrs_data_provider.get_stream_id_from_label(&quot;slam-front-right&quot;) et_to_select = vrs_data_provider.get_stream_id_from_label(&quot;camera-et-right&quot;) # First deactivate all streams, then just add back selected streams customized_deliver_options.deactivate_stream_all() for selected_stream_id in [rgb_to_select,slam_to_select,et_to_select]: customized_deliver_options.activate_stream(selected_stream_id) # ----------------- # 2. Time windowing feature - Skip first 2 seconds, and play for 3 seconds, if possible # ----------------- total_length_ns = vrs_data_provider.get_last_time_ns_all_streams(TimeDomain.DEVICE_TIME) - vrs_data_provider.get_first_time_ns_all_streams(TimeDomain.DEVICE_TIME) skip_begin_ns = int(2 * 1e9) # 2 seconds duration_ns = int(3 * 1e9) # 3 seconds skip_end_ns = max(total_length_ns - skip_begin_ns - duration_ns, 0) customized_deliver_options.set_truncate_first_device_time_ns(skip_begin_ns) customized_deliver_options.set_truncate_last_device_time_ns(skip_end_ns) # ----------------- # 3. Per-stream sub-sampling feature - subsample slam camera at rate of 3 # ----------------- slam_subsample_rate = 3 customized_deliver_options.set_subsample_rate(stream_id = slam_to_select, rate = slam_subsample_rate) # ----------------- # 4. Deliver customized data queue, and visualize # ----------------- print(f&quot;Start visualizing customized sensor data queue&quot;) rr.init(&quot;rerun_viz_customized_sensor_data_queue&quot;) rr.notebook_show() for sensor_data in vrs_data_provider.deliver_queued_sensor_data(customized_deliver_options): stream_id = sensor_data.stream_id() stream_label = vrs_data_provider.get_label_from_stream_id(stream_id) device_time_ns = sensor_data.get_time_ns(TimeDomain.DEVICE_TIME) image_data_and_record = sensor_data.image_data_and_record() # Visualize rr.set_time_nanos(&quot;device_time&quot;, device_time_ns) rr.log(stream_label, rr.Image(image_data_and_record[0].to_numpy_array()))  ","version":"Next","tagName":"h2"},{"title":"Get Started","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/projectariatools/start","content":"","keywords":"","version":"Next"},{"title":"Installing projectaria-tools‚Äã","type":1,"pageTitle":"Get Started","url":"/projectaria_tools/gen2/research-tools/projectariatools/start#installing","content":" Select your preferences We‚Äôll generate the right install command Operating System Linux &amp; macOSWindows Package pipbuild from source (C++ &amp; Python) Channel StableDev Dev channel is unavailable when using pip. Install CommandCopy rm -rf $HOME/projectaria_tools_python_env python3.12 -m venv $HOME/projectaria_tools_python_env source $HOME/projectaria_tools_python_env/bin/activate python3.12 -m pip install projectaria-tools'[all]'==2.0.0rc1   Use the above table to get the exact command to install projectaria-tools library.  We strongly recommend install projectaria-tools via pip packages within a Python virtual environment.stable represents the most currently tested and supported version of PyTorch. This should be suitable for many users; develop is pushed continuously to the main branch on Github repo.For users who wants to build the library from source code (C++ or Python), please refer to instructions in the advanced installation page.  ","version":"Next","tagName":"h2"},{"title":"Supported Platforms‚Äã","type":1,"pageTitle":"Get Started","url":"/projectaria_tools/gen2/research-tools/projectariatools/start#supported-platforms","content":" OS / Platform\tOS / Distro Details\tAria Gen2 Support (projectaria-tools ‚â•2.0)\tSupported Python VersionsLinux (x64)\tFedora 40/41; Ubuntu 20.04 LTS (focal) / 22.04 LTS (jammy)\t‚úÖ Supported\t3.8 ‚Äì 3.12 macOS (Apple Silicon / ARM64)\tmacOS 14+ (Sonoma or newer) on M1/M2/M3\t‚úÖ Supported\t3.10 ‚Äì 3.12 macOS (Intel)\tmacOS 13+ (Ventura or newer)\t‚úÖ Supported\t3.8 ‚Äì 3.12 Windows (x64)\tMSVC 2019/2022\tüöß Planned\t3.10 ‚Äì 3.12  ","version":"Next","tagName":"h2"},{"title":"Python Notebook Tutorials‚Äã","type":1,"pageTitle":"Get Started","url":"/projectaria_tools/gen2/research-tools/projectariatools/start#python-notebook-tutorials","content":" Proceed to the following Python notebook tutorials to learn how to use the Python APIs to consume Aria data:  Tutorial 1: VrsDataProvider Basics: how to perform basic operations in loading and access data in an Aria VRS file.Tutorial 2: Device Calibration: how to work with device calibration in Aria VRS.Tutorial 3: Queued Sensor Data: how to use the unified queued API to efficiently ‚Äústream‚Äù multi-sensor data.Tutorial 4: On-Device Eye Tracking and Hand Tracking: how to work with on-device-generated EyeGaze and Hand-tracking signals from Aria Gen2 glasses.Tutorial 5: On-Device VIO: how to work with on-device-generated VIO data from Aria Gen2 glasses.Tutorial 6: Timestamp Mapping in Single- and Multi-Device Recordings: understanding timestamp mapping in Aria data, and how to use timestamp mapping in multi-device recording.Tutorial 7: Machine Perception Services (MPS) Data Loading: how to load and visualize output data from Aria MP. ","version":"Next","tagName":"h2"},{"title":"C++ Visualization Tools","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/cppviz","content":"","keywords":"","version":"Next"},{"title":"Prerequisites‚Äã","type":1,"pageTitle":"C++ Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/cppviz#prerequisites","content":" To use the C++ visualization tools, you need to build them from source using CMake. These tools are not included in the standard Python package installation.  Required build step: Follow the &quot;build using CMake from source code&quot; instructions in the Advanced Installation guide to compile the C++ visualization tools.  The C++ tools require the Pangolin library for visualization. If Pangolin is not found during compilation, the visualization tools will not be built.  ","version":"Next","tagName":"h2"},{"title":"aria_viewer‚Äã","type":1,"pageTitle":"C++ Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/cppviz#aria_viewer","content":" The aria_viewer is a C++ binary that provides real-time visualization of Aria VRS (Video Recording and Sensor) data using the Pangolin framework. It offers native performance for visualizing multi-modal sensor data from Aria devices.     ","version":"Next","tagName":"h2"},{"title":"Basic Usage‚Äã","type":1,"pageTitle":"C++ Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/cppviz#basic-usage","content":" aria_viewer --vrs path/to/your/file.vrs   ","version":"Next","tagName":"h3"},{"title":"Command Line Options‚Äã","type":1,"pageTitle":"C++ Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/cppviz#command-line-options","content":" Parameter\tType\tRequired\tDescription--vrs\tstring\tYes\tPath to the VRS file you want to visualize  ","version":"Next","tagName":"h3"},{"title":"What You'll See‚Äã","type":1,"pageTitle":"C++ Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/cppviz#what-youll-see","content":" The C++ viewer provides an interactive visualization environment with multiple components:  RGB &amp; SLAM Camera Streams: Real-time display of RGB and SLAM camera images with overlaid eye gaze and hand tracking results1D Sensor Data: Time series plots of: IMU signals (accelerometer and gyroscope data)Audio signals (microphone waveforms)Barometer signals (pressure and temperature readings) 3D Visualization: Interactive 3D scene showing: VIO trajectory (Visual-Inertial Odometry path)Eye gaze vectors in 3D spaceHand tracking results and posesDevice pose and orientation  ","version":"Next","tagName":"h3"},{"title":"Interactive Controls‚Äã","type":1,"pageTitle":"C++ Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/cppviz#interactive-controls","content":" The viewer includes real-time controls for:  Play/Pause: Start and stop data playbackSpeed Control: Adjust playback speed using a sliderTimestamp Navigation: Jump to specific timestamps by dragging the timestamp sliderStream Toggles: Enable/disable individual data streams for focused analysis3D Camera Controls: Navigate and explore the 3D visualization  ","version":"Next","tagName":"h3"},{"title":"Known Issues and Workarounds‚Äã","type":1,"pageTitle":"C++ Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/cppviz#known-issues-and-workarounds","content":" High-frequency sequence playback issue: There is a known issue when playing high-frequency Aria-Gen2 data at real speed, where camera views may update asynchronously. This is due to CPU-based image decoding speed limitations.  Workarounds:  Use Python viewer: Fall back to the aria_rerun_viewer for smooth high-frequency playbackReduce playback speed: Use the speed control slider to play at slower speedsRandom access: Dragging the timestamp slider for random access always works correctly  The development team is actively working on a solution for this issue. ","version":"Next","tagName":"h3"},{"title":"Export Gen2 On-device Machine Perception data to CSV","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/exportcsv","content":"","keywords":"","version":"Next"},{"title":"Basic Usage‚Äã","type":1,"pageTitle":"Export Gen2 On-device Machine Perception data to CSV","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/exportcsv#basic-usage","content":" run_gen2_mp_csv_exporter --vrs-path path/to/your/gen2_file.vrs \\ --output-folder ./exported_mp_data/ \\ --vio-high-freq-subsample-rate 80   ","version":"Next","tagName":"h2"},{"title":"Command Line Options‚Äã","type":1,"pageTitle":"Export Gen2 On-device Machine Perception data to CSV","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/exportcsv#command-line-options","content":" Parameter\tType\tRequired\tDescription--vrs-path\tstring\tYes\tPath to the Gen2 VRS file containing on-device MP data --output-folder\tstring\tNo\tFolder to output MP CSV files. Default: current directory --vio-high-freq-subsample-rate\tint\tNo\tSubsample rate for VIO high frequency data. Default: 1 (no subsampling)  ","version":"Next","tagName":"h2"},{"title":"Data Types Exported‚Äã","type":1,"pageTitle":"Export Gen2 On-device Machine Perception data to CSV","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/exportcsv#data-types-exported","content":" After running the tool, your output folder will contain the following files, with csv / JSONL formats that are compatible with MPS data formats:  output_folder/ ‚îú‚îÄ‚îÄ slam/ ‚îÇ ‚îú‚îÄ‚îÄ open_loop_trajectory.csv # VIO high-frequency poses ‚îÇ ‚îî‚îÄ‚îÄ online_calibration.jsonl # Online calibration updates ‚îú‚îÄ‚îÄ eye_gaze/ ‚îÇ ‚îî‚îÄ‚îÄ generalized_eye_gaze.csv # Eye gaze data ‚îî‚îÄ‚îÄ hand_tracking/ ‚îî‚îÄ‚îÄ hand_tracking_results.csv # Hand tracking results   Data Type\tSource VRS Data Stream\tOutput File\tContent\tNotesVIO High-Frequency Trajectory\tvio_high_frequency\t${OUTPUT_FOLDER}/slam/open_loop_trajectory.csv\tHigh-frequency device trajectory poses from Visual-Inertial Odometry\tCompatible with MPS open-loop trajectory format. Subsampling configurable via --vio-high-freq-subsample-rate parameter Eye Gaze Data\teyegaze\t${OUTPUT_FOLDER}/eye_gaze/generalized_eye_gaze.csv\tEye gaze directions and fixation points\tCompatible with MPS eye gaze format Hand Tracking Data\thandtracking\t${OUTPUT_FOLDER}/hand_tracking/hand_tracking_results.csv\tHand pose estimates, joint positions, and hand landmarks\tCompatible with MPS hand tracking format Online Calibration Data\tvio\t${OUTPUT_FOLDER}/slam/online_calibration.jsonl\tOnline SLAM camera and IMU calibration updates computed on-device\tCompatible with MPS online calibration format  ","version":"Next","tagName":"h2"},{"title":"Important Notes‚Äã","type":1,"pageTitle":"Export Gen2 On-device Machine Perception data to CSV","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/exportcsv#important-notes","content":" Gen2 specific: This tool only works with Gen2 Aria VRS files that contain on-device MP dataStream availability: Not all Gen2 recordings may contain all MP data streamsData consistency: The tool preserves original timestamps and coordinate systems from the on-device processingCalibration updates: Online calibration data represents dynamic updates computed during recording, which may differ from factory calibration ","version":"Next","tagName":"h2"},{"title":"VRS to MP4 Converter","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/vrstomp4","content":"","keywords":"","version":"Next"},{"title":"Python Visualization Tools","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/pythonviz","content":"","keywords":"","version":"Next"},{"title":"aria_rerun_viewer‚Äã","type":1,"pageTitle":"Python Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/pythonviz#aria_rerun_viewer","content":" The aria_rerun_viewer is a Python tool that visualizes Aria VRS (Video Recording and Sensor) files using the Rerun visualization framework. It supports both Aria Gen1 and Gen2 devices and can display multiple sensor streams including cameras, IMU, audio, eye gaze, hand tracking, and more.     ","version":"Next","tagName":"h2"},{"title":"Basic Usage‚Äã","type":1,"pageTitle":"Python Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/pythonviz#basic-usage","content":" aria_rerun_viewer --vrs path/to/your/file.vrs   ","version":"Next","tagName":"h3"},{"title":"Command Line Options‚Äã","type":1,"pageTitle":"Python Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/pythonviz#command-line-options","content":" Parameter\tType\tRequired\tDescription--vrs\tstring\tYes\tPath to the VRS file you want to visualize --skip-begin-sec\tfloat\tNo\tNumber of seconds to skip at the beginning of the VRS file --skip-end-sec\tfloat\tNo\tNumber of seconds to skip at the end of the VRS file --enabled-streams\tstring list\tNo\tEnable specific streams by their labels (space-separated). Available streams include: camera-rgb, slam-front-left, slam-front-right, slam-side-left, slam-side-right, camera-et-left, camera-et-right, imu-left, imu-right, mic, baro0, mag0, gps, handtracking, eyegaze, vio, vio_high_frequency. Default: all available streams --subsample-rates\tstring list\tNo\tSpecify subsampling rates for streams in the format stream=rate (space-separated pairs). Example: camera-rgb=2 eyegaze=5. Default: vio_high_frequency=10  ","version":"Next","tagName":"h3"},{"title":"Examples‚Äã","type":1,"pageTitle":"Python Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/pythonviz#examples","content":" Basic Visualization‚Äã  aria_rerun_viewer --vrs recording.vrs   Visualize Only RGB Camera and Eye Gaze‚Äã  aria_rerun_viewer \\ --vrs recording.vrs \\ --enabled-streams camera-rgb eyegaze   Skip Beginning and End of Recording‚Äã  aria_rerun_viewer \\ --vrs recording.vrs \\ --skip-begin-sec 10 \\ --skip-end-sec 5   Apply Custom Subsampling‚Äã  aria_rerun_viewer \\ --vrs recording.vrs \\ --subsample-rates camera-rgb=3 vio_high_frequency=20   Complex Example with Multiple Options‚Äã  aria_rerun_viewer \\ --vrs recording.vrs \\ --enabled-streams camera-rgb slam-front-left slam-front-right eyegaze handtracking \\ --subsample-rates camera-rgb=2 handtracking=5 \\ --skip-begin-sec 30 \\ --skip-end-sec 10   ","version":"Next","tagName":"h3"},{"title":"What You'll See‚Äã","type":1,"pageTitle":"Python Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/pythonviz#what-youll-see","content":" The viewer displays data in an interactive 3D environment using Rerun:  RGB &amp; SLAM Camera stream: RGB and SLAM camera images, with overlaid eye gaze and hand tracking results.1D Sensor Data: IMU, audio, magnetometer, and barometer data plotted as 1D time series.3D World View: 3D visualization of the VIO trajectory, eye gaze, and hand tracking results.Device calibration: 3D representation of the sensor locations on Aria device.  ","version":"Next","tagName":"h3"},{"title":"Important Notes‚Äã","type":1,"pageTitle":"Python Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/pythonviz#important-notes","content":" VIO High Frequency Subsampling: The vio_high_frequency stream runs at 800Hz by default, which is automatically subsampled to 80Hz (subsample rate of 10) to improve visualization performance. You can adjust this using --subsample-rates vio_high_frequency=&lt;rate&gt;. Image Decoding Performance: Image decoding is currently performed on CPU on Linux, so plotting speed might be slow depending on CPU load. To see smooth visualization, wait until Rerun caches some data then click play again, or use subsample options like --subsample-rates camera-rgb=2 to reduce the frame rate.  ","version":"Next","tagName":"h3"},{"title":"viewer_mps‚Äã","type":1,"pageTitle":"Python Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/pythonviz#viewer_mps","content":" The viewer_mps tool visualizes Aria data along with Machine Perception Services (MPS) outputs like SLAM trajectories, point clouds, eye gaze, and hand tracking results.     ","version":"Next","tagName":"h2"},{"title":"Basic Usage‚Äã","type":1,"pageTitle":"Python Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/pythonviz#basic-usage-1","content":" viewer_mps --vrs path/to/recording.vrs   ","version":"Next","tagName":"h3"},{"title":"Command Line Options‚Äã","type":1,"pageTitle":"Python Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/pythonviz#command-line-options-1","content":" Input Files‚Äã  --vrs: Path to VRS file--trajectory: Path(s) to MPS trajectory files (supports multiple files)--points: Path(s) to MPS global point cloud files (supports multiple files)--eyegaze: Path to MPS eye gaze file--hands: Path to MPS wrist and palm poses file--hands_all: Path to MPS full hand tracking results file--mps_folder: Path to MPS folder (overrides default &lt;vrs_file&gt;/mps location)  Visualization Options‚Äã  --no_rectify_image: Show raw fisheye RGB images without undistortion--web: Run viewer in web browser instead of desktop app  ","version":"Next","tagName":"h3"},{"title":"Examples‚Äã","type":1,"pageTitle":"Python Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/pythonviz#examples-1","content":" Auto-detect MPS Data‚Äã  # Automatically finds MPS data in &lt;vrs_file&gt;/mps folder viewer_mps --vrs recording.vrs   Specify Individual MPS Files‚Äã  viewer_mps \\ --vrs recording.vrs \\ --trajectory trajectory/closed_loop_trajectory.csv \\ --points global_points/global_points.csv.gz \\ --eyegaze eye_gaze/general_eye_gaze.csv   Web Browser Mode‚Äã  viewer_mps \\ --vrs recording.vrs \\ --web   Multiple Trajectories and Point Clouds‚Äã  viewer_mps \\ --trajectory trajectory1.csv trajectory2.csv \\ --points points1.csv points2.csv   ","version":"Next","tagName":"h3"},{"title":"What You'll See‚Äã","type":1,"pageTitle":"Python Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/pythonviz#what-youll-see-1","content":" The MPS viewer provides:  3D Scene: SLAM trajectory, point clouds, and device poses in 3D spaceCamera Views: RGB camera feeds with overlaid eye gaze and hand tracking projectionsHand Tracking: 3D hand landmarks, skeleton connections, and wrist/palm posesEye Gaze: 3D gaze vectors and their projections onto camera images  This tool is particularly useful for validating MPS processing results and understanding the spatial relationships between different data modalities. ","version":"Next","tagName":"h3"},{"title":"Tutorial 6: Device Time Alignment in Aria Gen2","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/time-sync","content":"","keywords":"","version":"Next"},{"title":"Introduction‚Äã","type":1,"pageTitle":"Tutorial 6: Device Time Alignment in Aria Gen2","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/time-sync#introduction","content":" In Project Aria glasses, one of the key features is that it provides multi-sensor data that are temporally aligned to a shared, device-time domain for each single device, and also provide multi-device Time Alignment using SubGHz signals (Aria Gen2), TICSync (Aria Gen1), or TimeCode signals (Aria Gen1). In this tutorial, we will demonstrate how to use such temporal aligned data from Aria Gen2 recordings.  What you'll learn:  How to access temporally aligned sensor data on a single VRS recording.How to access temporally aligned sensor data across multiple recordings using SubGHz signals.  Prerequisites  Complete Tutorial 1 (VrsDataProvider Basics) to understand basic data provider conceptsComplete Tutorial 3 (Sequential Access multi-sensor data) to understand how to create a queue of sensor data from VRS file.  from projectaria_tools.core import data_provider # Load local VRS file vrs_file_path = &quot;path/to/your/recording.vrs&quot; vrs_data_provider = data_provider.create_vrs_data_provider(vrs_file_path)   ","version":"Next","tagName":"h2"},{"title":"Single-Device Timestamp alignment‚Äã","type":1,"pageTitle":"Tutorial 6: Device Time Alignment in Aria Gen2","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/time-sync#single-device-timestamp-alignment","content":" In projectaria_tools, every timestamp is linked to a specific TimeDomain, which represents the time reference or clock used to generate that timestamp. Timestamps from different TimeDomains are not directly comparable‚Äîonly timestamps within the same TimeDomain are consistent and can be accurately compared or aligned.  ","version":"Next","tagName":"h2"},{"title":"Supported Time Domains‚Äã","type":1,"pageTitle":"Tutorial 6: Device Time Alignment in Aria Gen2","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/time-sync#supported-time-domains","content":" Important: Use DEVICE_TIME for single-device Aria data analysis  The following table shows all supported time domains in projectaria_tools. For single-device Aria data analysis, use DEVICE_TIME for accurate temporal alignment between sensors.  Time Domain\tDescription\tUsageDEVICE_TIME (Recommended)\tCapture time in device's time domain. Accurate and reliable. All sensors on the same Aria device share the same device time domain.\tUse this for single-device Aria data analysis RECORD_TIME\tTimestamps stored in the index of VRS files. For Aria glasses, these are equal to device timestamp converted to double-precision floating point.\tFast access, but use DEVICE_TIME for accuracy HOST_TIME\tTimestamps when sensor data is saved to the device (not when captured).\tShould not be needed for any purpose --- Time Domains for Multi-device time alignment --- SUBGHZ\tMulti-device time alignment option for Aria Gen2\tSee next part in this tutorial UTC\tMulti-device time alignment option\tSee next part in this tutorial TIME_CODE\tMulti-device time alignment option for Aria Gen1\tSee Gen1 multi-device tutorial TIC_SYNC\tMulti-device time alignment option for Aria Gen1\tSee Gen1 multi-device tutorial  ","version":"Next","tagName":"h3"},{"title":"Data API to query by timestamp‚Äã","type":1,"pageTitle":"Tutorial 6: Device Time Alignment in Aria Gen2","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/time-sync#data-api-to-query-by-timestamp","content":" The VRS data provider offers powerful timestamp-based data access through the get_&lt;SENSOR&gt;_data_by_time_ns() API family. This is the recommended approach for temporal alignment across sensors and precise timestamp-based data retrieval.  For any sensor type, you can query data by timestamp using the get_&lt;SENSOR&gt;_data_by_time_ns() function, where &lt;SENSOR&gt; can be replaced by any sensor data type available in Aria VRS. See the VrsDataProvider.h for a complete list of supported sensor types.  TimeQueryOptions  This TimeQueryOptions parameter controls how the system finds data when your query timestamp doesn't exactly match a recorded timestamp:  Option\tBehavior\tUse CaseBEFORE\tReturns the last valid data with timestamp ‚â§ query_time\tDefault and most common - Get the most recent data before or at the query time AFTER\tReturns the first valid data with timestamp ‚â• query_time\tGet the next available data after or at the query time CLOSEST\tReturns data with smallest `\ttimestamp - query_time  ","version":"Next","tagName":"h3"},{"title":"Boundary Behavior‚Äã","type":1,"pageTitle":"Tutorial 6: Device Time Alignment in Aria Gen2","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/time-sync#boundary-behavior","content":" The API handles edge cases automatically:  Query Condition\tBEFORE\tAFTER\tCLOSESTquery_time &lt; first_timestamp\tReturns invalid data\tReturns first data\tReturns first data first_timestamp ‚â§ query_time ‚â§ last_timestamp\tReturns data with timestamp ‚â§ query_time\tReturns data with timestamp ‚â• query_time\tReturns temporally closest data query_time &gt; last_timestamp\tReturns last data\tReturns invalid data\tReturns last data  ","version":"Next","tagName":"h3"},{"title":"Single VRS Timestamp-Based Query Example‚Äã","type":1,"pageTitle":"Tutorial 6: Device Time Alignment in Aria Gen2","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/time-sync#single-vrs-timestamp-based-query-example","content":" from projectaria_tools.core.sensor_data import SensorDataType, TimeDomain, TimeQueryOptions print(&quot;=== Single VRS timestamp based query ===&quot;) # Select RGB stream ID rgb_stream_id = vrs_data_provider.get_stream_id_from_label(&quot;camera-rgb&quot;) # Get a timestamp within the recording (3 seconds after start) start_timestamp_ns = vrs_data_provider.get_first_time_ns(rgb_stream_id, TimeDomain.DEVICE_TIME) selected_timestamp_ns = start_timestamp_ns + int(3e9) # Fetch the RGB frame that is CLOSEST to this selected timestamp_ns closest_rgb_data_and_record = vrs_data_provider.get_image_data_by_time_ns( stream_id = rgb_stream_id, time_ns = selected_timestamp_ns, time_domain = TimeDomain.DEVICE_TIME, time_query_options = TimeQueryOptions.CLOSEST ) closest_timestamp_ns = closest_rgb_data_and_record[1].capture_timestamp_ns closest_frame_number = closest_rgb_data_and_record[1].frame_number print(f&quot; The closest RGB frame to query timestamp {selected_timestamp_ns} is the {closest_frame_number}-th frame, with capture timestamp of {closest_timestamp_ns}&quot;) # Fetch the frame BEFORE this frame prev_rgb_data_and_record = vrs_data_provider.get_image_data_by_time_ns( stream_id = rgb_stream_id, time_ns = closest_timestamp_ns - 1, time_domain = TimeDomain.DEVICE_TIME, time_query_options = TimeQueryOptions.BEFORE ) prev_timestamp_ns = prev_rgb_data_and_record[1].capture_timestamp_ns prev_frame_number = prev_rgb_data_and_record[1].frame_number print(f&quot; The previous RGB frame is the {prev_frame_number}-th frame, with capture timestamp of {prev_timestamp_ns}&quot;) # Fetch the frame AFTER this frame next_rgb_data_and_record = vrs_data_provider.get_image_data_by_time_ns( stream_id = rgb_stream_id, time_ns = closest_timestamp_ns + 1, time_domain = TimeDomain.DEVICE_TIME, time_query_options = TimeQueryOptions.AFTER ) next_timestamp_ns = next_rgb_data_and_record[1].capture_timestamp_ns next_frame_number = next_rgb_data_and_record[1].frame_number print(f&quot; The next RGB frame is the {next_frame_number}-th frame, with capture timestamp of {next_timestamp_ns}&quot;)   ","version":"Next","tagName":"h3"},{"title":"Visualizing Synchronized Multi-sensor Data‚Äã","type":1,"pageTitle":"Tutorial 6: Device Time Alignment in Aria Gen2","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/time-sync#visualizing-synchronized-multi-sensor-data","content":" In this additional example, we demonstrate how to query and visualize &quot;groups&quot; of RGB + SLAM images approximately at the same timestamp.  import rerun as rr print(&quot;=== Single VRS timestamp-based query visualization examples ===&quot;) rr.init(&quot;rerun_viz_single_vrs_timestamp_based_query&quot;) rr.notebook_show() # Select RGB and SLAM stream IDs to visualize all_labels = vrs_data_provider.get_device_calibration().get_camera_labels() slam_labels = [label for label in all_labels if &quot;slam&quot; in label ] slam_stream_ids = [vrs_data_provider.get_stream_id_from_label(label) for label in slam_labels] rgb_stream_id = vrs_data_provider.get_stream_id_from_label(&quot;camera-rgb&quot;) # Starting from +3 seconds into the recording, and at 5Hz frequency target_period_ns = int(2e8) start_timestamp_ns = vrs_data_provider.get_first_time_ns(rgb_stream_id, TimeDomain.DEVICE_TIME) + int(3e9) # Plot 20 samples current_timestamp_ns = start_timestamp_ns for frame_i in range(20): # Query and plot RGB image rgb_image_data, rgb_image_record = vrs_data_provider.get_image_data_by_time_ns( stream_id = rgb_stream_id, time_ns = current_timestamp_ns, time_domain = TimeDomain.DEVICE_TIME, time_query_options = TimeQueryOptions.CLOSEST) rr.set_time_nanos(&quot;device_time&quot;, rgb_image_record.capture_timestamp_ns) rr.log(&quot;rgb_image&quot;, rr.Image(rgb_image_data.to_numpy_array())) # Query and plot SLAM images for slam_i in range(len(slam_labels)): single_slam_label = slam_labels[slam_i] single_slam_stream_id = slam_stream_ids[slam_i] slam_image_data, slam_image_record = vrs_data_provider.get_image_data_by_time_ns( stream_id = single_slam_stream_id, time_ns = current_timestamp_ns, time_domain = TimeDomain.DEVICE_TIME, time_query_options = TimeQueryOptions.CLOSEST) rr.set_time_nanos(&quot;device_time&quot;, slam_image_record.capture_timestamp_ns) rr.log(single_slam_label, rr.Image(slam_image_data.to_numpy_array())) # Increment query timestamp current_timestamp_ns += target_period_ns   ","version":"Next","tagName":"h3"},{"title":"Multi-Device Timestamp alignment‚Äã","type":1,"pageTitle":"Tutorial 6: Device Time Alignment in Aria Gen2","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/time-sync#multi-device-timestamp-alignment","content":" While recording, multiple Aria-Gen2 glasses can enable a feature that allows their timestamps to be mapped across devices using SubGHz signals. Please refer to the multi-device recording wiki page from ARK (TODO: add link) to learn how to record with this feature.  Basically, one pair of glasses acts as the host device, that actively broadcasts SubGHz signals to a specified channel; all other glasses act as client devices, that receives the SubGHz signals, and record a new Time Domain Mapping data streams in their VRS file. It is essentially a timestamp hash mapping from host DEVICE_TIME -&gt; client DEVICE_TIME. Therefore this mapping data stream only exists in client VRS, but not host VRS.  In projectaria_tools, we provide 2 types of APIs to easily perform timestamp-based query across multi-device recordings:  Converter APIs provides direct convert functions that maps timestamps between any 2 TimeDomain.Query APIs that allows users to specifies time_domain = TimeDomain.SUBGHZ in a client VRS, to query &quot;from timestamp of the host&quot;.  The following code shows examples of using each type of API. Note that in the visualization example, the host and client windows will play intermittently. This is expected and correct, because the host and client devices' RGB cameras are NOT trigger aligned by nature.  ","version":"Next","tagName":"h2"},{"title":"Timestamp Converter APIs‚Äã","type":1,"pageTitle":"Tutorial 6: Device Time Alignment in Aria Gen2","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/time-sync#timestamp-converter-apis","content":" import rerun as rr from projectaria_tools.core.sensor_data import ( SensorData, ImageData, TimeDomain, TimeQueryOptions, TimeSyncMode, ) # Create data providers for both host and client recordings host_recording = &quot;path/to/host.vrs&quot; host_data_provider = data_provider.create_vrs_data_provider(host_recording) client_recording = &quot;path/to/client.vrs&quot; client_data_provider = data_provider.create_vrs_data_provider(client_recording) print(&quot;======= Multi-VRS time mapping example: Timestamp converter APIs ======&quot;) # Because host and client recordings may start at different times, # we manually pick a timestamp in the middle of the host recording. # Note that for host, we always use DEVICE_TIME domain. selected_timestamp_host = (host_data_provider.get_first_time_ns_all_streams(time_domain = TimeDomain.DEVICE_TIME) + host_data_provider.get_last_time_ns_all_streams(time_domain = TimeDomain.DEVICE_TIME)) // 2 # Convert from host time to client time selected_timestamp_client = client_data_provider.convert_from_synctime_to_device_time_ns(selected_timestamp_host, TimeSyncMode.SUBGHZ) # Convert from client time back to host time. Note that there could be some small numerical differences compared selected_timestamp_host_roundtrip = client_data_provider.convert_from_device_time_to_synctime_ns(selected_timestamp_client, TimeSyncMode.SUBGHZ) print(f&quot; Selected host timestamp is {selected_timestamp_host}; &quot;) print(f&quot; Converted to client timestamp is {selected_timestamp_client}; &quot;) print(f&quot; Then roundtrip convert back to host:{selected_timestamp_host_roundtrip}, &quot; f&quot; And delta value from original host timestamp is {selected_timestamp_host_roundtrip - selected_timestamp_host}. This is mainly due to numerical errors. &quot;)   ","version":"Next","tagName":"h3"},{"title":"Multi-Device Query APIs‚Äã","type":1,"pageTitle":"Tutorial 6: Device Time Alignment in Aria Gen2","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/time-sync#multi-device-query-apis","content":" print(&quot;======= Multi-VRS time mapping example: Query APIs ======&quot;) rr.init(&quot;rerun_viz_multi_vrs_time_mapping&quot;) # Set up sensor queue options in host VRS, only turn on RGB stream host_deliver_options = host_data_provider.get_default_deliver_queued_options() host_deliver_options.deactivate_stream_all() rgb_stream_id = host_data_provider.get_stream_id_from_label(&quot;camera-rgb&quot;) host_deliver_options.activate_stream(rgb_stream_id) # Select only a segment to plot host_vrs_start_timestamp = host_data_provider.get_first_time_ns_all_streams(time_domain = TimeDomain.DEVICE_TIME) host_segment_start = host_vrs_start_timestamp + int(20e9) # 20 seconds after start host_segment_duration = int(5e9) host_segment_end = host_segment_start + host_segment_duration host_vrs_end_timestamp = host_data_provider.get_last_time_ns_all_streams(time_domain = TimeDomain.DEVICE_TIME) host_deliver_options.set_truncate_first_device_time_ns(host_segment_start - host_vrs_start_timestamp) host_deliver_options.set_truncate_last_device_time_ns(host_vrs_end_timestamp - host_segment_end) # Plot RGB image data from both host and client for sensor_data in host_data_provider.deliver_queued_sensor_data(host_deliver_options): # --------- # Plotting in host. # Everything is done in DEVICE_TIME domain. # --------- host_image_data, host_image_record = sensor_data.image_data_and_record() # Set timestamps directly from host image record host_timestamp_ns = host_image_record.capture_timestamp_ns rr.set_time_nanos(&quot;device_time&quot;, host_timestamp_ns) rr.log(&quot;rgb_image_in_host&quot;, rr.Image(host_image_data.to_numpy_array())) # --------- # Plotting in client. # All the query APIs are done in SUBGHZ domain. # --------- # Query the closest RGB image from client VRS client_image_data, client_image_record = client_data_provider.get_image_data_by_time_ns( stream_id = rgb_stream_id, time_ns = host_timestamp_ns, time_domain = TimeDomain.SUBGHZ, time_query_options = TimeQueryOptions.CLOSEST) # Still need to convert client's device time back to host's time, # because we want to log this image data on host's timeline in Rerun client_timestamp_ns = client_image_record.capture_timestamp_ns converted_client_timestamp_ns = client_data_provider.convert_from_device_time_to_synctime_ns(client_timestamp_ns, TimeSyncMode.SUBGHZ) rr.set_time_nanos(&quot;device_time&quot;, converted_client_timestamp_ns) # Plot client image rr.log(&quot;rgb_image_in_client&quot;, rr.Image(client_image_data.to_numpy_array())) rr.notebook_show()  ","version":"Next","tagName":"h3"},{"title":"Basic Usage‚Äã","type":1,"pageTitle":"VRS to MP4 Converter","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/vrstomp4#basic-usage","content":" vrs_to_mp4 --vrs input.vrs --output_video output.mp4   ","version":"Next","tagName":"h2"},{"title":"Command Line Options‚Äã","type":1,"pageTitle":"VRS to MP4 Converter","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/vrstomp4#command-line-options","content":" Parameter\tType\tRequired\tDescription--vrs\tstring\tYes\tPath to the VRS file to be converted to a video --output_video\tstring\tYes\tPath to the MP4 video file you want to create --stream_id\tstring\tNo\tStream ID to convert to video. Options: 214-1 (RGB), 1201-1, 1201-2, 1201-3, 1201-4 (SLAM cameras). Default: 214-1 --log_folder\tstring\tNo\tFolder to store logs: mp4_to_vrs_time_map.csv, vrs_to_mp4_log.json, audio files --downsample\tinteger\tNo\tDownsampling factor for VRS images (must be ‚â•1). Default: 1 --audio_channels\tinteger list\tNo\tAudio channel indices to include in the MP4. Default: [0, 2] (the 2 mics on the lower frame of the glasses)  ","version":"Next","tagName":"h2"},{"title":"Stream IDs Explained‚Äã","type":1,"pageTitle":"VRS to MP4 Converter","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/vrstomp4#stream-ids-explained","content":" 214-1: RGB camera stream (default choice)1201-1, 1201-2, 1201-3, 1201-4: SLAM camera streams (grayscale)  The RGB stream (214-1) is typically what you want for creating viewable videos, while SLAM streams are useful for computer vision applications.  ","version":"Next","tagName":"h3"},{"title":"Examples‚Äã","type":1,"pageTitle":"VRS to MP4 Converter","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/vrstomp4#examples","content":" ","version":"Next","tagName":"h2"},{"title":"Basic RGB Video Conversion‚Äã","type":1,"pageTitle":"VRS to MP4 Converter","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/vrstomp4#basic-rgb-video-conversion","content":" vrs_to_mp4 --vrs recording.vrs --output_video output.mp4   ","version":"Next","tagName":"h3"},{"title":"Select Specific Audio Channels‚Äã","type":1,"pageTitle":"VRS to MP4 Converter","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/vrstomp4#select-specific-audio-channels","content":" vrs_to_mp4 \\ --vrs recording.vrs \\ --output_video custom_audio.mp4 \\ --audio_channels 0 1 3  ","version":"Next","tagName":"h3"},{"title":"Technical Specifications","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs","content":"Technical Specifications","keywords":"","version":"Next"},{"title":"Placeholder","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/aria-studio/feature1","content":"Placeholder Detailed feature.","keywords":"","version":"Next"},{"title":"Application Programming Interface","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/client-sdk/api","content":"Application Programming Interface API","keywords":"","version":"Next"},{"title":"Tutorial 5: On-Device VIO data streams","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/vio","content":"","keywords":"","version":"Next"},{"title":"Introduction‚Äã","type":1,"pageTitle":"Tutorial 5: On-Device VIO data streams","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/vio#introduction","content":" In Aria-Gen2 glasses, one of the key upgrade from Aria-Gen1 is the capability to run Machine Perception (MP) algorithms on the device during streaming / recording. Currently supported on-device MP algorithms include Eye-tracking, Hand-tracking, and VIO. These algorithm results are stored as separate data streams in the VRS file.  VIO (Visual Inertial Odometry) combines camera images and IMU (Inertial Measurement Unit) data to estimate device pose and motion in real-time. VIO tracks the device's position, orientation, and velocity by performing visual tracking, IMU integration, sensor fusion, etc, making it the foundation for spatial tracking and understanding.  In Aria-Gen2 devices, the VIO algorithm are run on device to produce 2 types of tracking results as part of the VRS file: VIO and VIO High Frequency. This tutorial focuses on demonstration of how to use the on-device VIO and VIO_high_frequency results.  What you'll learn:  How to access on-device VIO and VIO_high_frequency data from VRS filesHow to visualize 3D trajectory from on-device VIO data.  Prerequisites  Complete Tutorial 1 (VrsDataProvider Basics) to understand basic data provider conceptsComplete Tutorial 2 (Device Calibration) to understand how to properly use calibration in Aria data.  from projectaria_tools.core import data_provider # Load local VRS file vrs_file_path = &quot;path/to/your/recording.vrs&quot; vrs_data_provider = data_provider.create_vrs_data_provider(vrs_file_path) # Query VIO data streams vio_label = &quot;vio&quot; vio_stream_id = vrs_data_provider.get_stream_id_from_label(vio_label) if vio_stream_id is None: raise RuntimeError( f&quot;{vio_label} data stream does not exist! Please use a VRS that contains valid VIO data for this tutorial.&quot; ) # Query VIO_high_frequency data streams vio_high_freq_label = &quot;vio_high_frequency&quot; vio_high_freq_stream_id = vrs_data_provider.get_stream_id_from_label(vio_high_freq_label) if vio_high_freq_stream_id is None: raise RuntimeError( f&quot;{vio_high_freq_label} data stream does not exist! Please use a VRS that contains valid VIO high frequency data for this tutorial.&quot; )   ","version":"Next","tagName":"h2"},{"title":"On-Device VIO Data Stream‚Äã","type":1,"pageTitle":"Tutorial 5: On-Device VIO data streams","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/vio#on-device-vio-data-stream","content":" ","version":"Next","tagName":"h2"},{"title":"Data Type: FrontendOutput‚Äã","type":1,"pageTitle":"Tutorial 5: On-Device VIO data streams","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/vio#data-type-frontendoutput","content":" This a new data type introduced to store the results from the VIO system, containing the following fields:  Field Name\tDescriptionfrontend_session_uid\tSession identifier (resets on VIO restart) frame_id\tFrame set identifier capture_timestamp_ns\tCenter capture time in nanoseconds unix_timestamp_ns\tUnix timestamp in nanoseconds status\tVIO status (VALID/INVALID) pose_quality\tPose quality (GOOD/BAD/UNKNOWN) visual_tracking_quality\tVisual-only tracking quality online_calib\tOnline calibration estimates for SLAM cameras and IMUs gravity_in_odometry\tGravity vector in odometry frame transform_odometry_bodyimu\tBody IMU's pose in odometry reference frame transform_bodyimu_device\tTransform from body IMU to device frame linear_velocity_in_odometry\tLinear velocity in odometry frame in m/s angular_velocity_in_bodyimu\tAngular velocity in body IMU frame in rad/s  Here, body IMU is the IMU that is picked as the reference for motion tracking. For Aria-Gen2' on-device VIO algorithm, this is often imu-left.  Important Note: Always check status == VioStatus.VALID andpose_quality == TrackingQuality.GOOD for VIO data validity!  ","version":"Next","tagName":"h3"},{"title":"Data Access API‚Äã","type":1,"pageTitle":"Tutorial 5: On-Device VIO data streams","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/vio#data-access-api","content":" from projectaria_tools.core.sensor_data import VioStatus, TrackingQuality print(&quot;=== VIO Data Sample ===&quot;) # Find the first valid VIO data sample num_vio_samples = vrs_data_provider.get_num_data(vio_stream_id) first_valid_index = None for idx in range(num_vio_samples): vio_data = vrs_data_provider.get_vio_data_by_index(vio_stream_id, idx) if ( vio_data.status == VioStatus.VALID and vio_data.pose_quality == TrackingQuality.GOOD ): first_valid_index = idx break if first_valid_index is not None: print(&quot;=&quot; * 50) print(f&quot;First VALID VIO Data Sample (Index: {first_valid_index})&quot;) print(&quot;=&quot; * 50) # Session Information print(f&quot;Session UID: {vio_data.frontend_session_uid}&quot;) print(f&quot;Frame ID: {vio_data.frame_id}&quot;) # Timestamps print(f&quot;Capture Time: {vio_data.capture_timestamp_ns} ns&quot;) print(f&quot;Unix Time: {vio_data.unix_timestamp_ns} ns&quot;) # Quality Status print(f&quot;Status: {vio_data.status}&quot;) print(f&quot;Pose Quality: {vio_data.pose_quality}&quot;) print(f&quot;Visual Quality: {vio_data.visual_tracking_quality}&quot;) # Transforms print(f&quot;Transform Odometry ‚Üí Body IMU:\\n{vio_data.transform_odometry_bodyimu.to_matrix()}&quot;) print(f&quot;Transform Body IMU ‚Üí Device:\\n{vio_data.transform_bodyimu_device.to_matrix()}&quot;) # Motion print(f&quot;Linear Velocity: {vio_data.linear_velocity_in_odometry}&quot;) print(f&quot;Angular Velocity: {vio_data.angular_velocity_in_bodyimu}&quot;) print(f&quot;Gravity Vector: {vio_data.gravity_in_odometry}&quot;) else: print(&quot;‚ö†Ô∏è No valid VIO sample found&quot;)   ","version":"Next","tagName":"h3"},{"title":"On-Device VIO High Frequency Data Stream‚Äã","type":1,"pageTitle":"Tutorial 5: On-Device VIO data streams","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/vio#on-device-vio-high-frequency-data-stream","content":" VIO High Frequency results are generated directly from the on-device VIO results by performing IMU integration between VIO poses, hence provides a much higher data rate at approximately 800Hz.  ","version":"Next","tagName":"h2"},{"title":"Data Type: OpenLoopTrajectoryPose‚Äã","type":1,"pageTitle":"Tutorial 5: On-Device VIO data streams","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/vio#data-type-openlooptrajectorypose","content":" The VioHighFrequency stream re-uses the OpenLoopTrajectoryPose data structure defined in MPS.  Field Name\tDescriptiontracking_timestamp\tTimestamp in device time domain, in microseconds transform_odometry_device\tTransformation from device to odometry coordinate frame, represented as a SE3 instance. device_linear_velocity_odometry\tTranslational velocity of device in odometry frame, in m/s angular_velocity_device\tAngular velocity of device in device frame, in rad/s quality_score\tQuality of pose estimation (higher = better) gravity_odometry\tEarth gravity vector in odometry frame session_uid\tUnique identifier for VIO tracking session  Important Note: Due to the high frequency nature of this data (~800Hz), consider subsampling for visualization to maintain performance.  print(&quot;=== VIO High-Frequency Data Sample ===&quot;) # Find the first VIO high_frequency data sample with high quality value num_vio_high_freq_samples = vrs_data_provider.get_num_data(vio_high_freq_stream_id) first_valid_index = None for idx in range(num_vio_samples): vio_high_freq_data = vrs_data_provider.get_vio_high_freq_data_by_index(vio_high_freq_stream_id, idx) if ( vio_high_freq_data.quality_score &gt; 0.5 ): first_valid_index = idx break if first_valid_index is not None: print(&quot;=&quot; * 50) print(f&quot;First VIO High Freq Data Sample with good quality score (Index: {first_valid_index})&quot;) print(&quot;=&quot; * 50) # Timestamps, convert timedelta to nanoseconds capture_timestamp_ns = int(vio_high_freq_data.tracking_timestamp.total_seconds() * 1e9) # Session Information print(f&quot;Session UID: {vio_high_freq_data.session_uid}&quot;) # Timestamps print(f&quot;Tracking Time: {capture_timestamp_ns} ns&quot;) # Quality print(f&quot;Quality Score: {vio_high_freq_data.quality_score:.3f}&quot;) # Transform print(f&quot;Transform Odometry ‚Üí Device:\\n{vio_high_freq_data.transform_odometry_device.to_matrix()}&quot;) # Motion print(f&quot;Linear Velocity: {vio_high_freq_data.device_linear_velocity_odometry}&quot;) print(f&quot;Angular Velocity: {vio_high_freq_data.angular_velocity_device}&quot;) print(f&quot;Gravity Vector: {vio_high_freq_data.gravity_odometry}&quot;)   ","version":"Next","tagName":"h3"},{"title":"Visualizing On-Device VIO trajectory‚Äã","type":1,"pageTitle":"Tutorial 5: On-Device VIO data streams","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/vio#visualizing-on-device-vio-trajectory","content":" The following code snippets demonstrate how to visualize a VIO trajectory, along with glass frame + hand tracking results, in a 3D view.  def plot_single_hand_3d( hand_joints_in_device, hand_label ): &quot;&quot;&quot; A helper function to plot single hand data in 3D view &quot;&quot;&quot; marker_color = [255,64,0] if hand_label == &quot;left&quot; else [255, 255, 0] hand_skeleton_3d = create_hand_skeleton_from_landmarks(hand_joints_in_device) rr.log( f&quot;world/device/handtracking/{hand_label}/landmarks&quot;, rr.Points3D( positions=hand_joints_in_device, colors= marker_color, radii=5e-3, ), ) rr.log( f&quot;world/device/handtracking/{hand_label}/hand_skeleton&quot;, rr.LineStrips3D( hand_skeleton_3d, colors=[0, 255, 0], radii=3e-3, ), ) def plot_hand_pose_data_3d(hand_pose_data): &quot;&quot;&quot; A helper function to plot hand pose data in 3D world view &quot;&quot;&quot; # Clear the canvas (only if hand_tracking_label exists for this device version) rr.log( f&quot;world/device/handtracking&quot;, rr.Clear.recursive(), ) # Plot both hands if hand_pose_data.left_hand is not None: plot_single_hand_3d( hand_joints_in_device=hand_pose_data.left_hand.landmark_positions_device, hand_label=&quot;left&quot;, ) if hand_pose_data.right_hand is not None: plot_single_hand_3d( hand_joints_in_device=hand_pose_data.right_hand.landmark_positions_device, hand_label=&quot;right&quot;, )   ","version":"Next","tagName":"h2"},{"title":"3D Trajectory Visualization‚Äã","type":1,"pageTitle":"Tutorial 5: On-Device VIO data streams","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/vio#3d-trajectory-visualization","content":" import rerun as rr from projectaria_tools.core.sensor_data import SensorDataType, TimeDomain, TimeQueryOptions from projectaria_tools.utils.rerun_helpers import ( create_hand_skeleton_from_landmarks, AriaGlassesOutline, ToTransform3D ) print(&quot;\\n=== Visualizing on-device VIO trajectory + HandTracking in 3D view ===&quot;) rr.init(&quot;rerun_viz_vio_trajectory&quot;) rr.notebook_show() device_calib = vrs_data_provider.get_device_calibration() handtracking_stream_id = vrs_data_provider.get_stream_id_from_label(&quot;handtracking&quot;) # Set up a data queue deliver_options = vrs_data_provider.get_default_deliver_queued_options() deliver_options.deactivate_stream_all() deliver_options.activate_stream(vio_stream_id) # Play for only 3 seconds total_length_ns = vrs_data_provider.get_last_time_ns_all_streams(TimeDomain.DEVICE_TIME) - vrs_data_provider.get_first_time_ns_all_streams(TimeDomain.DEVICE_TIME) skip_begin_ns = int(15 * 1e9) # Skip 15 seconds duration_ns = int(3 * 1e9) # 3 seconds skip_end_ns = max(total_length_ns - skip_begin_ns - duration_ns, 0) deliver_options.set_truncate_first_device_time_ns(skip_begin_ns) deliver_options.set_truncate_last_device_time_ns(skip_end_ns) # Plot VIO trajectory in 3D view. # Need to keep a cache to store already-loaded trajectory vio_traj_cached_full = [] for sensor_data in vrs_data_provider.deliver_queued_sensor_data(deliver_options): # Convert sensor data to VIO data vio_data = sensor_data.vio_data() # Check VIO data validity, only plot for valid data if ( vio_data.status != VioStatus.VALID or vio_data.pose_quality != TrackingQuality.GOOD): print(f&quot;VIO data is invalid for timestamp {sensor_data.get_time_ns(TimeDomain.DEVICE_TIME)}&quot;) continue # Set timestamp rr.set_time_nanos(&quot;device_time&quot;, vio_data.capture_timestamp_ns) # Set and plot the Device pose for the current timestamp, as a RGB axis T_World_Device = ( vio_data.transform_odometry_bodyimu @ vio_data.transform_bodyimu_device ) rr.log( &quot;world/device&quot;, ToTransform3D( T_World_Device, axis_length=0.05, ), ) # Also plot Aria glass outline for visualization aria_glasses_point_outline = AriaGlassesOutline( device_calib, use_cad_calib=True ) rr.log( &quot;world/device/glasses_outline&quot;, rr.LineStrips3D( aria_glasses_point_outline, colors=[200,200,200], radii=5e-4, ), ) # Plot gravity direction vector rr.log( &quot;world/vio_gravity&quot;, rr.Arrows3D( origins=[T_World_Device.translation()[0]], vectors=[ vio_data.gravity_in_odometry * 1e-2 ], # length converted from 9.8 meter -&gt; 10 cm colors=[101,67,33], radii=1.5e-3, ), static=False, ) # Plot VIO trajectory that are cached so far vio_traj_cached_full.append(T_World_Device.translation()[0]) rr.log( &quot;world/vio_trajectory&quot;, rr.LineStrips3D( vio_traj_cached_full, colors=[173, 216, 255], radii=1.5e-3, ), static=False, ) # For visualization purpose, also plot the hand tracking results interpolated_hand_pose = vrs_data_provider.get_interpolated_hand_pose_data(handtracking_stream_id, vio_data.capture_timestamp_ns, TimeDomain.DEVICE_TIME) if interpolated_hand_pose is not None: plot_hand_pose_data_3d(hand_pose_data = interpolated_hand_pose)  ","version":"Next","tagName":"h3"},{"title":"Placeholder","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/companion-app/feature1","content":"Placeholder Detailed feature.","keywords":"","version":"Next"},{"title":"Command Line Interface","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli","content":"Command Line Interface CLI","keywords":"","version":"Next"},{"title":"3D Coordinate Frame Conventions","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/coordinate/3d-coor","content":"3D Coordinate Frame Conventions 3D Coordinate Frame Conventions","keywords":"","version":"Next"},{"title":"Hardware Specification","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/device/hardware","content":"Hardware Specification Hardware Specification","keywords":"","version":"Next"},{"title":"Project Aria Glasses 2D Image Coordinate System Conventions","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/coordinate/2d-coor","content":"Project Aria Glasses 2D Image Coordinate System Conventions For any provided camera intrinsic calibration value, we use the convention that the color value of a pixel with integer coordinates (u,v)(u,v)(u,v) is the average color of the square spanning from (u‚àí0.5,v‚àí0.5)(u-0.5,v-0.5)(u‚àí0.5,v‚àí0.5) to (u+0.5,v+0.5)(u+0.5,v+0.5)(u+0.5,v+0.5) in continuous coordinates. This is visualized in the Figure 1, and has the following important consequences: Checking in bound: A pixel (u,v)(u,v)(u,v) is considered to be in bound if ‚àí0.5‚â§u&lt;W‚àí0.5-0.5\\leq u&lt;W-0.5‚àí0.5‚â§u&lt;W‚àí0.5 and ‚àí0.5‚â§v&lt;H‚àí0.5-0.5\\leq v&lt;H-0.5‚àí0.5‚â§v&lt;H‚àí0.5.Interpolation: In bilinear interpolation, a point (u,v) can be interpolated of all four neighboring integer-valued pixel coordinates are in-bound. That requires 0‚â§u‚â§W‚àí10 \\leq u \\leq W-10‚â§u‚â§W‚àí1 and 0‚â§v‚â§H‚àí10 \\leq v \\leq H-10‚â§v‚â§H‚àí1.Image down-sampling: When downsampling images by a factor of sss, every s√óss \\times ss√ós pixel are squeezed into a single pixel. For example, the intensity at pixel s√óss \\times ss√ós in the scaled image accounts for all the photons collected in the area [‚àí0.5,s‚àí0.5]√ó[‚àí0.5,s‚àí0.5][-0.5,s-0.5]\\times[-0.5,s-0.5][‚àí0.5,s‚àí0.5]√ó[‚àí0.5,s‚àí0.5] (i.e. column 000 to s‚àí1s - 1s‚àí1, and row 000 to s‚àí1s-1s‚àí1 in the discrete coordinate) in the original image. In order to keep this assumption valid, the re-scaled point pscaledp_\\text{scaled}pscaled‚Äã not only needs to scale from the corresponding point in the original image poriginalp_\\text{original}poriginal‚Äã but also accounts for the (0.5,0.5)(0.5,0.5)(0.5,0.5) translation accordingly by pscaled=s(poriginal+0.5)‚àí0.5p_\\text{scaled} =s (p_\\text{original}+0.5)-0.5pscaled‚Äã=s(poriginal‚Äã+0.5)‚àí0.5 Figure 1: 2D Image Coordinate System Conventions","keywords":"","version":"Next"},{"title":"Device Calibration","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/device/calibration","content":"Device Calibration Device Calibration","keywords":"","version":"Next"},{"title":"Project Aria Gen 2 Profiles","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/device/profile","content":"Project Aria Gen 2 Profiles Project Aria Gen 2 glasses offer a range of profiles, allowing users to select the optimal configuration of sensors and settings for their research or application needs. Each profile defines a specific combination of sensor streams, data rates, resolutions, and encoding formats. These profiles are designed to balance data quality, device performance, and operational stability. Profiles are divided into two categories: Pre-defined profiles: Thoroughly tested, stable, and fully supported. Recommended for most users.Custom profiles: User-defined configurations for advanced or experimental use. These may not be stable and are not recommended for general use. The table below details the specifications of the currently supported pre-defined profiles. New profiles may be added as the platform evolves. Feature / Sensor\tprofile8\tprofile9\tprofile10\tmp_streaming_demoDescription\tGeneral purpose recording profile\tGeneral purpose streaming profile\tRecording profile with RGB 3MP 30Hz\tMachine perception streaming demo (streaming only) IMUs (Hz)\t800\t800\t800\t800 Magnetometer (Hz)\t100\t100\t100\t‚ùå Barometer (Hz)\t50\t50\t50\t‚ùå Audio Sample Rate (Hz)\t16K\t16K\t16K\t16K Audio Encoding Format\tOPUS\tOPUS\tOPUS\tOPUS Audio Bitrate\t256000\t256000\t256000\t256000 GPS (Hz)\t1\t‚ùå\t1\t‚ùå BLE Period (ms)\t30000\t‚ùå\t30000\t‚ùå BLE Duration (ms)\t2000\t‚ùå\t2000\t‚ùå WiFi Period (ms)\t30000\t‚ùå\t30000\t‚ùå WiFi Scan Mode\tACTIVE\t‚ùå\tACTIVE\t‚ùå SLAM Cameras (Hz)\t30\t10\t30\t10 SLAM Encoding\tH265 (CQP QP=22)\tH265 (CBR 900kbps)\tH265 (CQP QP=22)\tH265 (CQP QP=30) RGB Camera (Hz)\t10\t5\t30\t10 RGB Resolution\t2560x1920\t2560x1920\t2016x1512\t2016x1512 RGB Encoding\tH265 (CQP QP=22)\tH265 (CBR 8Mbps), blur filter\tH265 (CQP QP=22)\tH265 (CBR 1.2Mbps), blur filter ET Cameras (Hz)\t5\t‚ùå\t5\t5 ET Resolution\t200x200\t‚ùå\t200x200\t200x200 ET Encoding\tH265 (CQP QP=22)\t‚ùå\tH265 (CQP QP=22)\tH265 (CQP QP=30) ET (Hz)\t30\t30\t30\t30 HT (Hz)\t30\t30\t30\t10 VIO (Hz)\t10\t10\t10\t10 Device Info Period (ms)\t30000\t10000\t30000\t10000 PPG (Hz)\t128\t128\t128\t128 Temperature (Hz)\t1\t1\t1\t1 ALS (Hz)\t9.434\t9.434\t10\t‚ùå ALS Exposure (us)\t3200\t3200\t3200\t‚ùå","keywords":"","version":"Next"},{"title":"CAD File Downloads","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/device/cad","content":"","keywords":"","version":"Next"},{"title":"Glasses CAD:‚Äã","type":1,"pageTitle":"CAD File Downloads","url":"/projectaria_tools/gen2/technical-specs/device/cad#glasses-cad","content":" \tFrame Width\tTemple Arm Length\tNosepad\tCAD file1\tNarrow\tShort\tHigh\tOpen Hinge, Closed Hinge 2\tNarrow\tShort\tLow\tOpen Hinge, Closed Hinge 3\tNarrow\tLong\tHigh\tOpen Hinge, Closed Hinge 4\tNarrow\tLong\tLow\tOpen Hinge, Closed Hinge 5\tWide\tShort\tHigh\tOpen Hinge, Closed Hinge 6\tWide\tShort\tLow\tOpen Hinge, Closed Hinge 7\tWide\tLong\tHigh\tOpen Hinge, Closed Hinge 8\tWide\tLong\tLow\tOpen Hinge, Closed Hinge  ","version":"Next","tagName":"h2"},{"title":"Glasses Mount Reference Design:‚Äã","type":1,"pageTitle":"CAD File Downloads","url":"/projectaria_tools/gen2/technical-specs/device/cad#glasses-mount-reference-design","content":" The Aria Gen 2 glasses can be mounted using the following design files as a reference. The two reference CAD files differ by frame width. Additionally, the designs can accommodate either an open or close hinge application. In these files, they are designed to be compatible with a standard camera mount, ¬º‚Äù-20 insert (McMaster PN: 93365A160). A 16mm diamter o-ring is used to retain the glasses to the mount (McMaster PN: 1174N407).  Frame Width\tArms\tCAD fileNarrow\tUnfolded\tNEBULA TRIPOD MOUNT - NARROW - UNFOLDED.zip Narrow\tFolded\tNEBULA TRIPOD MOUNT - NARROW - FOLDED.zip Wide\tUnfolded\tNEBULA TRIPOD MOUNT - WIDE - UNFOLDED.zip Wide\tFolded\tNEBULA TRIPOD MOUNT - WIDE - FOLDED.zip ","version":"Next","tagName":"h2"},{"title":"Error Code","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/mps/error-code","content":"Error Code Error Code.","keywords":"","version":"Next"},{"title":"Hand Tracking","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/mps/hand-tracking","content":"Hand Tracking Hand Tracking - landmark definition, csv content.","keywords":"","version":"Next"},{"title":"Useful VRS Tools","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/vrs/useful-tools","content":"Useful VRS Tools The most intuitive way to access Aria VRS files is via the tools (and APIs) shipped in projectaria-tools library, which includes tools to visualize Aria VRS, convert to MP4, and export to other data format.Users can inspect the data quality of Aria VRS file using VrsHealthCheck, which is a tool that we specially tailored for Aria VRS recordings.Users can also choose to use the native tool provided by the VRS library to perform some simple actions on the VRS file, including inspecting basic data information, extracting to images or audio files, etc. But please note that the VRS library needs to be built with H.265 decoding support in order to properly handle Aria Gen2 VRS files.","keywords":"","version":"Next"},{"title":"SLAM","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/mps/slam","content":"SLAM SLAM - file structure, explain common terminology, open_loop_trajectory.csv, closed_loop_trajectory.csv, semidense_points.csv.gz, semidense_observations.csv.gz, online_calibration.jsonl, multi-slam folder structure","keywords":"","version":"Next"},{"title":"Project Aria VRS Data Format","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/vrs/data-format","content":"","keywords":"","version":"Next"},{"title":"Aria data streams‚Äã","type":1,"pageTitle":"Project Aria VRS Data Format","url":"/projectaria_tools/gen2/technical-specs/vrs/data-format#aria-data-streams","content":" In VRS, data is organized by streams, each storing data measured by a specific sensor, or calculated from an on-device machine perception algorithm.  The VRS streams are uniquely identified by their StreamId, each consisting of a RecordableTypeId to categorize the type of the stream data, and an InstanceId for identifying the specific instance of the sensor. For example, the first SLAM camera is identified with StreamId = 1201-1, where 1201 is the numerical ID for SLAM camera data type, and -1 identifies the first of all the SLAM cameras.  For convenience, we also provide a short label for each stream within our library projectaria-tools, which provides a human-readable way to access the data. See this page to learn more.  ","version":"Next","tagName":"h2"},{"title":"Aria sensor data and configuration‚Äã","type":1,"pageTitle":"Project Aria VRS Data Format","url":"/projectaria_tools/gen2/technical-specs/vrs/data-format#aria-sensor-data-and-configuration","content":" Sensor data includes:  Sensor readoutTimestampsAcquisition parameters (exposure and gain settings)Conditions (e.g. temperature) during data collection  Most sensor data of a single stream and at a specific timestamp is stored as a single piece, except for image and audio.  ","version":"Next","tagName":"h2"},{"title":"How data is stored for image recordings‚Äã","type":1,"pageTitle":"Project Aria VRS Data Format","url":"/projectaria_tools/gen2/technical-specs/vrs/data-format#how-data-is-stored-for-image-recordings","content":" Each camera stores a single image frame at a time.The image frame contains two parts, the image itself and the image record. The image record stores timestamps, frame id, and acquisition parameters, such as exposure and gain. This avoids having to read image data to get the information in the record.  ","version":"Next","tagName":"h3"},{"title":"How data is stored for audio recordings‚Äã","type":1,"pageTitle":"Project Aria VRS Data Format","url":"/projectaria_tools/gen2/technical-specs/vrs/data-format#how-data-is-stored-for-audio-recordings","content":" The audio data is grouped into data chunks of 4096 audio samples from all microphones.Each chunk contains two parts, the data part for the audio signal, and the report part for the timestamps of each audio signal.  ","version":"Next","tagName":"h3"},{"title":"Sensor configuration blob‚Äã","type":1,"pageTitle":"Project Aria VRS Data Format","url":"/projectaria_tools/gen2/technical-specs/vrs/data-format#sensor-configuration-blob","content":" The sensor configuration blob stores the static information of a stream. Common sensor configuration stores information, such as sensor model, sensor serial (if available) as well as frame rate. Stream-specific information, such as image resolution, is also stored in configurations.  Go to this python tutorial to learn how to access and use the sensor data using Python data utilities. Go to the source code for the detailed implementation of sensor data and configurations. ","version":"Next","tagName":"h3"},{"title":"VRS Stream ID to Label Mapping in Aria Data","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/vrs/streamid-label-mapper","content":"VRS Stream ID to Label Mapping in Aria Data In Aria VRS files, each data stream is uniquely identified by a VRS StreamId. This page provides the mapping information between all the VRS StreamIds, and the corresponding readable sensor labels used in device calibration and projectaria_tools APIs. Table 1: StreamID to label mapping for Aria Gen2 Sensor Stream ID Recordable Type ID label ET camera left 211-1 EyeCameraRecordableClass camera-et-left ET camera right 211-2 EyeCameraRecordableClass camera-et-right RGB camera 214-1 RgbCameraRecordableClass camera-rgb All microphones 231-1 StereoAudioRecordableClass mic Temperature 246-1 TemperatureRecordableClass temperature Barometer 247-1 BarometerRecordableClass baro0 Photoplethysmography sensor 248-1 PpgRecordableClass ppg GPS (sensor) 281-2 GpsRecordableClass gps GPS (from Companion App) 281-1 GpsRecordableClass gps-app Wi-Fi 282-1 WifiBeaconRecordableClass wps Bluetooth 283-1 BluetoothBeaconRecordableClass\tbluetooth Ambient light sensor 500-1 AlsRecordableClass als SLAM camera front left 1201-1 SlamCameraData slam-front-left SLAM camera front right\t1201-2 SlamCameraData slam-front-right SLAM camera side left 1201-3 SlamCameraData slam-side-left SLAM camera side right 1201-4 SlamCameraData slam-side-right IMU left 1202-1 SlamImuData imu-left IMU right 1202-2 SlamImuData imu-right Magnetometer 1203-1 SlamMagnetometerData mag0 Eye gaze (on-device MP) 373-1 EyeGazeRecordableClass eyegaze Hand tracking (on-device MP) 371-* (dynamic) PoseRecordableClass handtracking VIO (on-device MP) 371-* (dynamic) PoseRecordableClass vio VIO high frequency (on-device MP)\t371-* (dynamic) PoseRecordableClass vio_high_frequency Dynamic Stream IDs The handtracking, vio, and vio_high_frequency streams have dynamic stream IDs that are determined at runtime by querying the VRS file. Case 1: If handtracking stream exists VRS Stream ID\tSensor Label371-1\thandtracking 371-2\tvio 371-3\tvio_high_frequency Case 2: If handtracking stream does not exist VRS Stream ID\tSensor Label371-1\tvio 371-2\tvio_high_frequency These streams are identified by their RecordableTypeId (PoseRecordableClass) and their specific flavor strings: device/oatmeal/hand, device/oatmeal/vio, and device/oatmeal/vio_high_frequency.","keywords":"","version":"Next"}]